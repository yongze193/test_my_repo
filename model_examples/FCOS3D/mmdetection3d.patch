diff --git a/configs/_base_/datasets/nus-mono3d.py b/configs/_base_/datasets/nus-mono3d.py
index 9a1d2265..bd522476 100644
--- a/configs/_base_/datasets/nus-mono3d.py
+++ b/configs/_base_/datasets/nus-mono3d.py
@@ -54,6 +54,7 @@ train_dataloader = dict(
     batch_size=2,
     num_workers=2,
     persistent_workers=True,
+    pin_memory=True,
     sampler=dict(type='DefaultSampler', shuffle=True),
     dataset=dict(
         type=dataset_type,
@@ -82,6 +83,7 @@ val_dataloader = dict(
     num_workers=2,
     persistent_workers=True,
     drop_last=False,
+    pin_memory=True,
     sampler=dict(type='DefaultSampler', shuffle=False),
     dataset=dict(
         type=dataset_type,
diff --git a/configs/_base_/default_runtime.py b/configs/_base_/default_runtime.py
index 9249ab99..b2e9ec71 100644
--- a/configs/_base_/default_runtime.py
+++ b/configs/_base_/default_runtime.py
@@ -11,7 +11,7 @@ default_hooks = dict(
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl', timeout=7200000),
 )
 
 log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)
diff --git a/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py b/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py
index baec55f9..85d97889 100644
--- a/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py
+++ b/configs/benchmark/hv_PartA2_secfpn_4x8_cyclic_80e_pcdet_kitti-3d-3class.py
@@ -371,7 +371,7 @@ custom_hooks = [
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 vis_backends = [dict(type='LocalVisBackend')]
diff --git a/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py b/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py
index dc59480e..b63905c4 100644
--- a/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py
+++ b/configs/benchmark/hv_pointpillars_secfpn_3x8_100e_det3d_kitti-3d-car.py
@@ -234,7 +234,7 @@ custom_hooks = [
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 vis_backends = [dict(type='LocalVisBackend')]
diff --git a/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py b/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
index 01dc8b5c..5a0ca4a3 100644
--- a/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
+++ b/configs/benchmark/hv_pointpillars_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
@@ -277,7 +277,7 @@ custom_hooks = [
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 vis_backends = [dict(type='LocalVisBackend')]
diff --git a/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py b/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
index f8dd0d24..9075d3b9 100644
--- a/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
+++ b/configs/benchmark/hv_second_secfpn_4x8_80e_pcdet_kitti-3d-3class.py
@@ -267,7 +267,7 @@ custom_hooks = [
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 vis_backends = [dict(type='LocalVisBackend')]
diff --git a/configs/mvfcos3d/multiview-fcos3d_r101-dcn_8xb2_waymoD5-3d-3class.py b/configs/mvfcos3d/multiview-fcos3d_r101-dcn_8xb2_waymoD5-3d-3class.py
index b75a6db5..b644b7e8 100644
--- a/configs/mvfcos3d/multiview-fcos3d_r101-dcn_8xb2_waymoD5-3d-3class.py
+++ b/configs/mvfcos3d/multiview-fcos3d_r101-dcn_8xb2_waymoD5-3d-3class.py
@@ -40,7 +40,7 @@ default_scope = 'mmdet3d'
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 log_level = 'INFO'
diff --git a/mmdet3d/__init__.py b/mmdet3d/__init__.py
index 17c17773..e7b5408c 100644
--- a/mmdet3d/__init__.py
+++ b/mmdet3d/__init__.py
@@ -18,11 +18,6 @@ mmdet_minimum_version = '3.0.0rc5'
 mmdet_maximum_version = '3.4.0'
 mmdet_version = digit_version(mmdet.__version__)
 
-assert (mmcv_version >= digit_version(mmcv_minimum_version)
-        and mmcv_version < digit_version(mmcv_maximum_version)), \
-    f'MMCV=={mmcv.__version__} is used but incompatible. ' \
-    f'Please install mmcv>={mmcv_minimum_version}, <{mmcv_maximum_version}.'
-
 assert (mmengine_version >= digit_version(mmengine_minimum_version)
         and mmengine_version < digit_version(mmengine_maximum_version)), \
     f'MMEngine=={mmengine.__version__} is used but incompatible. ' \
diff --git a/mmdet3d/configs/_base_/default_runtime.py b/mmdet3d/configs/_base_/default_runtime.py
index c13d0e11..32687766 100644
--- a/mmdet3d/configs/_base_/default_runtime.py
+++ b/mmdet3d/configs/_base_/default_runtime.py
@@ -21,7 +21,7 @@ default_hooks = dict(
 env_cfg = dict(
     cudnn_benchmark=False,
     mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
-    dist_cfg=dict(backend='nccl'),
+    dist_cfg=dict(backend='hccl'),
 )
 
 log_processor = dict(type=LogProcessor, window_size=50, by_epoch=True)
diff --git a/tools/dist_test.sh b/tools/dist_test.sh
index dea131b4..39bf6ae9 100755
--- a/tools/dist_test.sh
+++ b/tools/dist_test.sh
@@ -9,7 +9,8 @@ PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+# python -m torch.distributed.launch \
+torchrun \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
diff --git a/tools/dist_train.sh b/tools/dist_train.sh
index 3fca7641..bcd052f1 100755
--- a/tools/dist_train.sh
+++ b/tools/dist_train.sh
@@ -8,7 +8,8 @@ PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+# python -m torch.distributed.launch \
+torchrun \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
diff --git a/tools/dist_train_performance.sh b/tools/dist_train_performance.sh
new file mode 100644
index 00000000..616ca8c3
--- /dev/null
+++ b/tools/dist_train_performance.sh
@@ -0,0 +1,20 @@
+#!/usr/bin/env bash
+
+CONFIG=$1
+GPUS=$2
+NNODES=${NNODES:-1}
+NODE_RANK=${NODE_RANK:-0}
+PORT=${PORT:-29500}
+MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
+
+PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
+# python -m torch.distributed.launch \
+torchrun \
+    --nnodes=$NNODES \
+    --node_rank=$NODE_RANK \
+    --master_addr=$MASTER_ADDR \
+    --nproc_per_node=$GPUS \
+    --master_port=$PORT \
+    $(dirname "$0")/train_performance.py \
+    $CONFIG \
+    --launcher pytorch ${@:3}
diff --git a/tools/test.py b/tools/test.py
index 4afc2559..a3500eb7 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -9,6 +9,12 @@ from mmengine.runner import Runner
 
 from mmdet3d.utils import replace_ceph_backend
 
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+torch.npu.config.allow_internal_format = False
+torch_npu.npu.set_compile_mode(jit_compile=False)
+
 
 # TODO: support fuse_conv_bn and format_only
 def parse_args():
diff --git a/tools/train.py b/tools/train.py
index 6b9c3b08..22e9475e 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -11,6 +11,15 @@ from mmengine.runner import Runner
 
 from mmdet3d.utils import replace_ceph_backend
 
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+from mx_driving.patcher.patcher import PatcherBuilder, Patch
+from mx_driving.patcher.mmcv import dc, mdc
+
+torch.npu.config.allow_internal_format = False
+torch_npu.npu.set_compile_mode(jit_compile=False)
+
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a 3D detector')
@@ -142,4 +151,6 @@ def main():
 
 
 if __name__ == '__main__':
-    main()
+    pb = PatcherBuilder().add_module_patch("mmcv.ops", Patch(dc), Patch(mdc))
+    with pb.build():
+        main()
diff --git a/tools/train_performance.py b/tools/train_performance.py
new file mode 100644
index 00000000..b0f8caf3
--- /dev/null
+++ b/tools/train_performance.py
@@ -0,0 +1,156 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+import argparse
+import logging
+import os
+import os.path as osp
+
+from mx_driving.patcher.patcher import PatcherBuilder, Patch
+from mx_driving.patcher.mmcv import dc, mdc
+
+from mmengine.config import Config, DictAction
+from mmengine.logging import print_log
+from mmengine.registry import RUNNERS
+from mmengine.runner import Runner
+
+from mmdet3d.utils import replace_ceph_backend
+
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+torch.npu.config.allow_internal_format = False
+torch_npu.npu.set_compile_mode(jit_compile=False)
+
+
+def parse_args():
+    parser = argparse.ArgumentParser(description='Train a 3D detector')
+    parser.add_argument('config', help='train config file path')
+    parser.add_argument('--work-dir', help='the dir to save logs and models')
+    parser.add_argument(
+        '--amp',
+        action='store_true',
+        default=False,
+        help='enable automatic-mixed-precision training')
+    parser.add_argument(
+        '--sync_bn',
+        choices=['none', 'torch', 'mmcv'],
+        default='none',
+        help='convert all BatchNorm layers in the model to SyncBatchNorm '
+        '(SyncBN) or mmcv.ops.sync_bn.SyncBatchNorm (MMSyncBN) layers.')
+    parser.add_argument(
+        '--auto-scale-lr',
+        action='store_true',
+        help='enable automatically scaling LR.')
+    parser.add_argument(
+        '--resume',
+        nargs='?',
+        type=str,
+        const='auto',
+        help='If specify checkpoint path, resume from it, while if not '
+        'specify, try to auto resume from the latest checkpoint '
+        'in the work directory.')
+    parser.add_argument(
+        '--ceph', action='store_true', help='Use ceph as data storage backend')
+    parser.add_argument(
+        '--cfg-options',
+        nargs='+',
+        action=DictAction,
+        help='override some settings in the used config, the key-value pair '
+        'in xxx=yyy format will be merged into config file. If the value to '
+        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
+        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
+        'Note that the quotation marks are necessary and that no white space '
+        'is allowed.')
+    parser.add_argument(
+        '--launcher',
+        choices=['none', 'pytorch', 'slurm', 'mpi'],
+        default='none',
+        help='job launcher')
+    # When using PyTorch version >= 2.0.0, the `torch.distributed.launch`
+    # will pass the `--local-rank` parameter to `tools/train.py` instead
+    # of `--local_rank`.
+    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
+    args = parser.parse_args()
+    if 'LOCAL_RANK' not in os.environ:
+        os.environ['LOCAL_RANK'] = str(args.local_rank)
+    return args
+
+
+def main():
+    args = parse_args()
+
+    # load config
+    cfg = Config.fromfile(args.config)
+
+    # TODO: We will unify the ceph support approach with other OpenMMLab repos
+    if args.ceph:
+        cfg = replace_ceph_backend(cfg)
+
+    cfg.launcher = args.launcher
+    if args.cfg_options is not None:
+        cfg.merge_from_dict(args.cfg_options)
+
+    # work_dir is determined in this priority: CLI > segment in file > filename
+    if args.work_dir is not None:
+        # update configs according to CLI args if args.work_dir is not None
+        cfg.work_dir = args.work_dir
+    elif cfg.get('work_dir', None) is None:
+        # use config filename as default work_dir if cfg.work_dir is None
+        cfg.work_dir = osp.join('./work_dirs',
+                                osp.splitext(osp.basename(args.config))[0])
+
+    # enable automatic-mixed-precision training
+    if args.amp is True:
+        optim_wrapper = cfg.optim_wrapper.type
+        if optim_wrapper == 'AmpOptimWrapper':
+            print_log(
+                'AMP training is already enabled in your config.',
+                logger='current',
+                level=logging.WARNING)
+        else:
+            assert optim_wrapper == 'OptimWrapper', (
+                '`--amp` is only supported when the optimizer wrapper type is '
+                f'`OptimWrapper` but got {optim_wrapper}.')
+            cfg.optim_wrapper.type = 'AmpOptimWrapper'
+            cfg.optim_wrapper.loss_scale = 'dynamic'
+
+    # convert BatchNorm layers
+    if args.sync_bn != 'none':
+        cfg.sync_bn = args.sync_bn
+
+    # enable automatically scaling LR
+    if args.auto_scale_lr:
+        if 'auto_scale_lr' in cfg and \
+                'enable' in cfg.auto_scale_lr and \
+                'base_batch_size' in cfg.auto_scale_lr:
+            cfg.auto_scale_lr.enable = True
+        else:
+            raise RuntimeError('Can not find "auto_scale_lr" or '
+                               '"auto_scale_lr.enable" or '
+                               '"auto_scale_lr.base_batch_size" in your'
+                               ' configuration file.')
+
+    # resume is determined in this priority: resume from > auto_resume
+    if args.resume == 'auto':
+        cfg.resume = True
+        cfg.load_from = None
+    elif args.resume is not None:
+        cfg.resume = True
+        cfg.load_from = args.resume
+
+    # build the runner from config
+    if 'runner_type' not in cfg:
+        # build the default runner
+        runner = Runner.from_cfg(cfg)
+    else:
+        # build customized runner from the registry
+        # if 'runner_type' is set in the cfg
+        runner = RUNNERS.build(cfg)
+
+    # start training
+    runner.train()
+
+
+if __name__ == '__main__':
+    pb = PatcherBuilder().add_module_patch("mmcv.ops", Patch(dc), Patch(mdc)).brake_at(1000)
+    with pb.build():
+        main()
