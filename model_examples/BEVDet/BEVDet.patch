diff --git a/configs/_base_/default_runtime.py b/configs/_base_/default_runtime.py
index 5fc198b..576738c 100644
--- a/configs/_base_/default_runtime.py
+++ b/configs/_base_/default_runtime.py
@@ -10,7 +10,7 @@ log_config = dict(
         dict(type='TensorboardLoggerHook')
     ])
 # yapf:enable
-dist_params = dict(backend='nccl')
+dist_params = dict(backend='hccl')
 log_level = 'INFO'
 work_dir = None
 load_from = None
diff --git a/configs/bevdet/bevdet-r50-4d-depth-cbgs.py b/configs/bevdet/bevdet-r50-4d-depth-cbgs.py
index 5aa3ee8..4cadb8b 100644
--- a/configs/bevdet/bevdet-r50-4d-depth-cbgs.py
+++ b/configs/bevdet/bevdet-r50-4d-depth-cbgs.py
@@ -301,7 +301,7 @@ for key in ['val', 'test']:
 data['train']['dataset'].update(share_data_config)
 
 # Optimizer
-optimizer = dict(type='AdamW', lr=2e-4, weight_decay=1e-2)
+optimizer = dict(type='NpuFusedAdamW', lr=2e-4, weight_decay=1e-2)
 optimizer_config = dict(grad_clip=dict(max_norm=5, norm_type=2))
 lr_config = dict(
     policy='step',
diff --git a/configs/bevdet/bevdet-r50.py b/configs/bevdet/bevdet-r50.py
index ea3bf8b..31293ed 100644
--- a/configs/bevdet/bevdet-r50.py
+++ b/configs/bevdet/bevdet-r50.py
@@ -234,7 +234,7 @@ test_data_config = dict(
 
 data = dict(
     samples_per_gpu=8,
-    workers_per_gpu=4,
+    workers_per_gpu=8,
     train=dict(
         data_root=data_root,
         ann_file=data_root + 'bevdetv3-nuscenes_infos_train.pkl',
@@ -252,7 +252,7 @@ for key in ['train', 'val', 'test']:
     data[key].update(share_data_config)
 
 # Optimizer
-optimizer = dict(type='AdamW', lr=2e-4, weight_decay=1e-07)
+optimizer = dict(type='NpuFusedAdamW', lr=2e-4, weight_decay=1e-07)
 optimizer_config = dict(grad_clip=dict(max_norm=5, norm_type=2))
 lr_config = dict(
     policy='step',
diff --git a/mmdet3d/__init__.py b/mmdet3d/__init__.py
index 643c39c..0cf030f 100644
--- a/mmdet3d/__init__.py
+++ b/mmdet3d/__init__.py
@@ -2,7 +2,6 @@
 import mmcv
 
 import mmdet
-import mmseg
 from .version import __version__, short_version
 
 
@@ -19,7 +18,7 @@ def digit_version(version_str):
 
 
 mmcv_minimum_version = '1.5.2'
-mmcv_maximum_version = '1.7.0'
+mmcv_maximum_version = '1.7.2'
 mmcv_version = digit_version(mmcv.__version__)
 
 
@@ -39,11 +38,5 @@ assert (mmdet_version >= digit_version(mmdet_minimum_version)
 
 mmseg_minimum_version = '0.20.0'
 mmseg_maximum_version = '1.0.0'
-mmseg_version = digit_version(mmseg.__version__)
-assert (mmseg_version >= digit_version(mmseg_minimum_version)
-        and mmseg_version <= digit_version(mmseg_maximum_version)), \
-    f'MMSEG=={mmseg.__version__} is used but incompatible. ' \
-    f'Please install mmseg>={mmseg_minimum_version}, ' \
-    f'<={mmseg_maximum_version}.'
 
 __all__ = ['__version__', 'short_version']
diff --git a/mmdet3d/apis/train.py b/mmdet3d/apis/train.py
index 4d97026..be10ecd 100644
--- a/mmdet3d/apis/train.py
+++ b/mmdet3d/apis/train.py
@@ -4,7 +4,7 @@ import warnings
 
 import numpy as np
 import torch
-from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
                          Fp16OptimizerHook, OptimizerHook, build_optimizer,
                          build_runner, get_dist_info)
@@ -103,13 +103,13 @@ def train_segmentor(model,
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
     else:
-        model = MMDataParallel(
+        model = NPUDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
     # build runner
@@ -223,13 +223,13 @@ def train_detector(model,
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
     else:
-        model = MMDataParallel(
+        model = NPUDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
     # build runner
diff --git a/mmdet3d/datasets/pipelines/loading.py b/mmdet3d/datasets/pipelines/loading.py
index b9357ff..3f23888 100644
--- a/mmdet3d/datasets/pipelines/loading.py
+++ b/mmdet3d/datasets/pipelines/loading.py
@@ -1163,7 +1163,7 @@ class LoadAnnotations(object):
 
     def __call__(self, results):
         gt_boxes, gt_labels = results['ann_infos']
-        gt_boxes, gt_labels = torch.Tensor(gt_boxes), torch.tensor(gt_labels)
+        gt_boxes, gt_labels = torch.Tensor(np.array(gt_boxes)), torch.tensor(np.array(gt_labels))
         if len(gt_boxes) == 0:
             gt_boxes = torch.zeros(0, 9)
         results['gt_bboxes_3d'] = \
diff --git a/mmdet3d/datasets/pipelines/transforms_3d.py b/mmdet3d/datasets/pipelines/transforms_3d.py
index a960dd3..a52a1d8 100644
--- a/mmdet3d/datasets/pipelines/transforms_3d.py
+++ b/mmdet3d/datasets/pipelines/transforms_3d.py
@@ -519,7 +519,7 @@ class ObjectSample(object):
                 input_dict['img'] = sampled_dict['img']
         gt_bboxes_ignore = np.ones_like(gt_labels_3d)
         gt_bboxes_ignore[num_exist:] = 0
-        gt_bboxes_ignore = gt_bboxes_ignore.astype(np.bool)
+        gt_bboxes_ignore = gt_bboxes_ignore.astype(np.bool_)
         input_dict['gt_bboxes_ignore'] = gt_bboxes_ignore
         input_dict['gt_bboxes_3d'] = gt_bboxes_3d
         input_dict['gt_labels_3d'] = gt_labels_3d.astype(np.int64)
@@ -924,14 +924,14 @@ class ObjectRangeFilter(object):
 
         if 'gt_bboxes_ignore' in input_dict:
             gt_bboxes_ignore = input_dict['gt_bboxes_ignore']
-            gt_bboxes_ignore = gt_bboxes_ignore[mask.numpy().astype(np.bool)]
+            gt_bboxes_ignore = gt_bboxes_ignore[mask.numpy().astype(np.bool_)]
             input_dict['gt_bboxes_ignore'] = gt_bboxes_ignore
         gt_bboxes_3d = gt_bboxes_3d[mask]
         # mask is a torch tensor but gt_labels_3d is still numpy array
         # using mask to index gt_labels_3d will cause bug when
         # len(gt_labels_3d) == 1, where mask=1 will be interpreted
         # as gt_labels_3d[1] and cause out of index error
-        gt_labels_3d = gt_labels_3d[mask.numpy().astype(np.bool)]
+        gt_labels_3d = gt_labels_3d[mask.cpu().numpy().astype(np.bool_)]
 
         # limit rad to [-pi, pi]
         gt_bboxes_3d.limit_yaw(offset=0.5, period=2 * np.pi)
diff --git a/mmdet3d/models/dense_heads/centerpoint_head.py b/mmdet3d/models/dense_heads/centerpoint_head.py
index f6a58ab..d6faf27 100644
--- a/mmdet3d/models/dense_heads/centerpoint_head.py
+++ b/mmdet3d/models/dense_heads/centerpoint_head.py
@@ -14,6 +14,8 @@ from mmdet3d.models.utils import clip_sigmoid
 from mmdet.core import build_bbox_coder, multi_apply, reduce_mean
 from ..builder import HEADS, build_loss
 
+from mx_driving import npu_gaussian
+
 
 @HEADS.register_module()
 class SeparateHead(BaseModule):
@@ -101,7 +103,7 @@ class SeparateHead(BaseModule):
         Returns:
             dict[str: torch.Tensor]: contains the following keys:
 
-                -reg （torch.Tensor): 2D regression value with the
+                -reg (torch.Tensor): 2D regression value with the
                     shape of [B, 2, H, W].
                 -height (torch.Tensor): Height value with the
                     shape of [B, 1, H, W].
@@ -217,7 +219,7 @@ class DCNSeparateHead(BaseModule):
         Returns:
             dict[str: torch.Tensor]: contains the following keys:
 
-                -reg （torch.Tensor): 2D regression value with the
+                -reg (torch.Tensor): 2D regression value with the
                     shape of [B, 2, H, W].
                 -height (torch.Tensor): Height value with the
                     shape of [B, 1, H, W].
@@ -506,82 +508,37 @@ class CenterHead(BaseModule):
 
             num_objs = min(task_boxes[idx].shape[0], max_objs)
 
+            temp_classes = task_classes[idx]
+            temp_boxes = task_boxes[idx]
+            center_int, radius, mask, ind, anno_box = npu_gaussian(temp_boxes,
+                                                                   self.train_cfg['out_size_factor'],
+                                                                   self.train_cfg['gaussian_overlap'],
+                                                                   self.train_cfg['min_radius'],
+                                                                   voxel_size[0],
+                                                                   voxel_size[1],
+                                                                   pc_range[0],
+                                                                   pc_range[1],
+                                                                   feature_map_size[0],
+                                                                   feature_map_size[1],
+                                                                   self.norm_bbox,
+                                                                   self.with_velocity)
             for k in range(num_objs):
-                cls_id = task_classes[idx][k] - 1
+                cls_id = temp_classes[k] - 1
 
-                width = task_boxes[idx][k][3]
-                length = task_boxes[idx][k][4]
+                width = temp_boxes[k][3]
+                length = temp_boxes[k][4]
                 width = width / voxel_size[0] / self.train_cfg[
                     'out_size_factor']
                 length = length / voxel_size[1] / self.train_cfg[
                     'out_size_factor']
 
                 if width > 0 and length > 0:
-                    radius = gaussian_radius(
-                        (length, width),
-                        min_overlap=self.train_cfg['gaussian_overlap'])
-                    radius = max(self.train_cfg['min_radius'], int(radius))
-
-                    # be really careful for the coordinate system of
-                    # your box annotation.
-                    x, y, z = task_boxes[idx][k][0], task_boxes[idx][k][
-                        1], task_boxes[idx][k][2]
-
-                    coor_x = (
-                        x - pc_range[0]
-                    ) / voxel_size[0] / self.train_cfg['out_size_factor']
-                    coor_y = (
-                        y - pc_range[1]
-                    ) / voxel_size[1] / self.train_cfg['out_size_factor']
-
-                    center = torch.tensor([coor_x, coor_y],
-                                          dtype=torch.float32,
-                                          device=device)
-                    center_int = center.to(torch.int32)
-
-                    # throw out not in range objects to avoid out of array
-                    # area when creating the heatmap
-                    if not (0 <= center_int[0] < feature_map_size[0]
-                            and 0 <= center_int[1] < feature_map_size[1]):
-                        continue
-
-                    draw_gaussian(heatmap[cls_id], center_int, radius)
-
-                    new_idx = k
-                    x, y = center_int[0], center_int[1]
-
-                    assert (y * feature_map_size[0] + x <
-                            feature_map_size[0] * feature_map_size[1])
-
-                    ind[new_idx] = y * feature_map_size[0] + x
-                    mask[new_idx] = 1
-                    # TODO: support other outdoor dataset
-                    rot = task_boxes[idx][k][6]
-                    box_dim = task_boxes[idx][k][3:6]
-                    if self.norm_bbox:
-                        box_dim = box_dim.log()
-                    if self.with_velocity:
-                        vx, vy = task_boxes[idx][k][7:]
-                        anno_box[new_idx] = torch.cat([
-                            center - torch.tensor([x, y], device=device),
-                            z.unsqueeze(0), box_dim,
-                            torch.sin(rot).unsqueeze(0),
-                            torch.cos(rot).unsqueeze(0),
-                            vx.unsqueeze(0),
-                            vy.unsqueeze(0)
-                        ])
-                    else:
-                        anno_box[new_idx] = torch.cat([
-                            center - torch.tensor([x, y], device=device),
-                            z.unsqueeze(0), box_dim,
-                            torch.sin(rot).unsqueeze(0),
-                            torch.cos(rot).unsqueeze(0)
-                        ])
-
+                    draw_gaussian(heatmap[cls_id], center_int[k], radius[k].item())
             heatmaps.append(heatmap)
             anno_boxes.append(anno_box)
             masks.append(mask)
             inds.append(ind)
+
         return heatmaps, anno_boxes, inds, masks
 
     def loss(self, gt_bboxes_3d, gt_labels_3d, preds_dicts, **kwargs):
diff --git a/mmdet3d/models/detectors/__init__.py b/mmdet3d/models/detectors/__init__.py
index afc800c..3bf7046 100644
--- a/mmdet3d/models/detectors/__init__.py
+++ b/mmdet3d/models/detectors/__init__.py
@@ -1,9 +1,8 @@
 # Copyright (c) OpenMMLab. All rights reserved.
 from .base import Base3DDetector
-from .bevdet import BEVDepth4D, BEVDet, BEVDet4D, BEVDetTRT, BEVStereo4D
+from .bevdet import BEVDepth4D, BEVDet, BEVDet4D, BEVStereo4D
 from .bevdet_occ import BEVStereo4DOCC
 from .centerpoint import CenterPoint
-from .dal import DAL
 from .dynamic_voxelnet import DynamicVoxelNet
 from .fcos_mono3d import FCOSMono3D
 from .groupfree3dnet import GroupFree3DNet
@@ -28,5 +27,5 @@ __all__ = [
     'CenterPoint', 'SSD3DNet', 'ImVoteNet', 'SingleStageMono3DDetector',
     'FCOSMono3D', 'ImVoxelNet', 'GroupFree3DNet', 'PointRCNN', 'SMOKEMono3D',
     'MinkSingleStage3DDetector', 'SASSD', 'BEVDet', 'BEVDet4D', 'BEVDepth4D',
-    'BEVDetTRT', 'BEVStereo4D', 'BEVStereo4DOCC'
+    'BEVStereo4D', 'BEVStereo4DOCC'
 ]
diff --git a/mmdet3d/models/detectors/bevdet.py b/mmdet3d/models/detectors/bevdet.py
index ad1154e..13c68e1 100644
--- a/mmdet3d/models/detectors/bevdet.py
+++ b/mmdet3d/models/detectors/bevdet.py
@@ -3,7 +3,6 @@ import torch
 import torch.nn.functional as F
 from mmcv.runner import force_fp32
 
-from mmdet3d.ops.bev_pool_v2.bev_pool import TRTBEVPoolv2
 from mmdet.models import DETECTORS
 from .. import builder
 from .centerpoint import CenterPoint
@@ -215,58 +214,6 @@ class BEVDet(CenterPoint):
         return outs
 
 
-@DETECTORS.register_module()
-class BEVDetTRT(BEVDet):
-
-    def result_serialize(self, outs):
-        outs_ = []
-        for out in outs:
-            for key in ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']:
-                outs_.append(out[0][key])
-        return outs_
-
-    def result_deserialize(self, outs):
-        outs_ = []
-        keys = ['reg', 'height', 'dim', 'rot', 'vel', 'heatmap']
-        for head_id in range(len(outs) // 6):
-            outs_head = [dict()]
-            for kid, key in enumerate(keys):
-                outs_head[0][key] = outs[head_id * 6 + kid]
-            outs_.append(outs_head)
-        return outs_
-
-    def forward(
-        self,
-        img,
-        ranks_depth,
-        ranks_feat,
-        ranks_bev,
-        interval_starts,
-        interval_lengths,
-    ):
-        x = self.img_backbone(img)
-        x = self.img_neck(x)
-        x = self.img_view_transformer.depth_net(x)
-        depth = x[:, :self.img_view_transformer.D].softmax(dim=1)
-        tran_feat = x[:, self.img_view_transformer.D:(
-            self.img_view_transformer.D +
-            self.img_view_transformer.out_channels)]
-        tran_feat = tran_feat.permute(0, 2, 3, 1)
-        x = TRTBEVPoolv2.apply(depth.contiguous(), tran_feat.contiguous(),
-                               ranks_depth, ranks_feat, ranks_bev,
-                               interval_starts, interval_lengths)
-        x = x.permute(0, 3, 1, 2).contiguous()
-        bev_feat = self.bev_encoder(x)
-        outs = self.pts_bbox_head([bev_feat])
-        outs = self.result_serialize(outs)
-        return outs
-
-    def get_bev_pool_input(self, input):
-        input = self.prepare_inputs(input)
-        coor = self.img_view_transformer.get_lidar_coor(*input[1:7])
-        return self.img_view_transformer.voxel_pooling_prepare_v2(coor)
-
-
 @DETECTORS.register_module()
 class BEVDet4D(BEVDet):
     r"""BEVDet4D paradigm for multi-camera 3D object detection.
@@ -630,7 +577,7 @@ class BEVStereo4D(BEVDepth4D):
             [x, sensor2keyego, ego2global, intrin, post_rot, post_tran, bda,
              mlp_input], metas)
         if self.pre_process:
-            bev_feat = self.pre_process_net(bev_feat)[0]
+            bev_feat = self.pre_process_net(bev_feat.half())[0]
         return bev_feat, depth, stereo_feat
 
     def extract_img_feat(self,
diff --git a/mmdet3d/models/detectors/bevdet_occ.py b/mmdet3d/models/detectors/bevdet_occ.py
index 37efdb6..52acada 100644
--- a/mmdet3d/models/detectors/bevdet_occ.py
+++ b/mmdet3d/models/detectors/bevdet_occ.py
@@ -29,7 +29,7 @@ class BEVStereo4DOCC(BEVStereo4D):
                         kernel_size=3,
                         stride=1,
                         padding=1,
-                        bias=True,
+                        bias=False,
                         conv_cfg=dict(type='Conv3d'))
         self.use_predicter =use_predicter
         if use_predicter:
@@ -124,7 +124,7 @@ class BEVStereo4DOCC(BEVStereo4D):
         loss_depth = self.img_view_transformer.get_depth_loss(gt_depth, depth)
         losses['loss_depth'] = loss_depth
 
-        occ_pred = self.final_conv(img_feats[0]).permute(0, 4, 3, 2, 1) # bncdhw->bnwhdc
+        occ_pred = self.final_conv(img_feats[0].half()).permute(0, 4, 3, 2, 1) # bncdhw->bnwhdc
         if self.use_predicter:
             occ_pred = self.predicter(occ_pred)
         voxel_semantics = kwargs['voxel_semantics']
diff --git a/mmdet3d/models/necks/view_transformer.py b/mmdet3d/models/necks/view_transformer.py
index ec03722..cc786a5 100644
--- a/mmdet3d/models/necks/view_transformer.py
+++ b/mmdet3d/models/necks/view_transformer.py
@@ -7,12 +7,10 @@ from mmcv.runner import BaseModule, force_fp32
 from torch.cuda.amp.autocast_mode import autocast
 from torch.utils.checkpoint import checkpoint
 
-from mmdet3d.ops.bev_pool_v2.bev_pool import bev_pool_v2
+from mx_driving.point import bev_pool_v3
 from mmdet.models.backbones.resnet import BasicBlock
 from ..builder import NECKS
 
-from torch.utils.checkpoint import checkpoint
-
 
 @NECKS.register_module()
 class LSSViewTransformer(BaseModule):
@@ -165,19 +163,19 @@ class LSSViewTransformer(BaseModule):
         # post-transformation
         # B x N x D x H x W x 3
         points = self.frustum.to(sensor2ego) - post_trans.view(B, N, 1, 1, 1, 3)
-        points = torch.inverse(post_rots).view(B, N, 1, 1, 1, 3, 3)\
-            .matmul(points.unsqueeze(-1))
+        B, N, D, H, W, _ = points.shape
+        points = points.view(B, N, D*H*W, 3, 1)
+        points = torch.inverse(post_rots).view(B, N, 1, 3, 3).matmul(points)
 
         # cam_to_ego
-        points = torch.cat(
-            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)
+        points = torch.cat((points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 3)
         combine = sensor2ego[:,:,:3,:3].matmul(torch.inverse(cam2imgs))
-        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points).squeeze(-1)
-        points += sensor2ego[:,:,:3, 3].view(B, N, 1, 1, 1, 3)
-        points = bda[:, :3, :3].view(B, 1, 1, 1, 1, 3, 3).matmul(
+        points = combine.view(B, N, 1, 3, 3).matmul(points).squeeze(-1)
+        points += sensor2ego[:,:,:3, 3].view(B, N, 1, 3)
+        points = bda[:, :3, :3].view(B, 1, 1, 3, 3).matmul(
             points.unsqueeze(-1)).squeeze(-1)
-        points += bda[:, :3, 3].view(B, 1, 1, 1, 1, 3)
-        return points
+        points += bda[:, :3, 3].view(B, 1, 1, 3)
+        return points.view(B, N, D, H, W, 3)
 
     def init_acceleration_v2(self, coor):
         """Pre-compute the necessary information in acceleration including the
@@ -190,20 +188,14 @@ class LSSViewTransformer(BaseModule):
                 (B, N_cams, D, H, W, C).
         """
 
-        ranks_bev, ranks_depth, ranks_feat, \
-            interval_starts, interval_lengths = \
-            self.voxel_pooling_prepare_v2(coor)
+        ranks_bev, ranks_depth, ranks_feat = self.voxel_pooling_prepare_v2(coor)
 
         self.ranks_bev = ranks_bev.int().contiguous()
         self.ranks_feat = ranks_feat.int().contiguous()
         self.ranks_depth = ranks_depth.int().contiguous()
-        self.interval_starts = interval_starts.int().contiguous()
-        self.interval_lengths = interval_lengths.int().contiguous()
 
     def voxel_pooling_v2(self, coor, depth, feat):
-        ranks_bev, ranks_depth, ranks_feat, \
-            interval_starts, interval_lengths = \
-            self.voxel_pooling_prepare_v2(coor)
+        ranks_bev, ranks_depth, ranks_feat = self.voxel_pooling_prepare_v2(coor)
         if ranks_feat is None:
             print('warning ---> no points within the predefined '
                   'bev receptive field')
@@ -219,9 +211,8 @@ class LSSViewTransformer(BaseModule):
         bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),
                           int(self.grid_size[1]), int(self.grid_size[0]),
                           feat.shape[-1])  # (B, Z, Y, X, C)
-        bev_feat = bev_pool_v2(depth, feat, ranks_depth, ranks_feat, ranks_bev,
-                               bev_feat_shape, interval_starts,
-                               interval_lengths)
+        bev_feat = bev_pool_v3(depth, feat, ranks_depth, ranks_feat,
+                               ranks_bev, bev_feat_shape)
         # collapse Z
         if self.collapse_z:
             bev_feat = torch.cat(bev_feat.unbind(dim=2), 1)
@@ -270,22 +261,8 @@ class LSSViewTransformer(BaseModule):
             self.grid_size[2] * self.grid_size[1] * self.grid_size[0])
         ranks_bev += coor[:, 2] * (self.grid_size[1] * self.grid_size[0])
         ranks_bev += coor[:, 1] * self.grid_size[0] + coor[:, 0]
-        order = ranks_bev.argsort()
-        ranks_bev, ranks_depth, ranks_feat = \
-            ranks_bev[order], ranks_depth[order], ranks_feat[order]
-
-        kept = torch.ones(
-            ranks_bev.shape[0], device=ranks_bev.device, dtype=torch.bool)
-        kept[1:] = ranks_bev[1:] != ranks_bev[:-1]
-        interval_starts = torch.where(kept)[0].int()
-        if len(interval_starts) == 0:
-            return None, None, None, None, None
-        interval_lengths = torch.zeros_like(interval_starts)
-        interval_lengths[:-1] = interval_starts[1:] - interval_starts[:-1]
-        interval_lengths[-1] = ranks_bev.shape[0] - interval_starts[-1]
         return ranks_bev.int().contiguous(), ranks_depth.int().contiguous(
-        ), ranks_feat.int().contiguous(), interval_starts.int().contiguous(
-        ), interval_lengths.int().contiguous()
+        ), ranks_feat.int().contiguous()
 
     def pre_compute(self, input):
         if self.initial_flag:
@@ -304,10 +281,9 @@ class LSSViewTransformer(BaseModule):
             bev_feat_shape = (depth.shape[0], int(self.grid_size[2]),
                               int(self.grid_size[1]), int(self.grid_size[0]),
                               feat.shape[-1])  # (B, Z, Y, X, C)
-            bev_feat = bev_pool_v2(depth, feat, self.ranks_depth,
+            bev_feat = bev_pool_v3(depth, feat, self.ranks_depth,
                                    self.ranks_feat, self.ranks_bev,
-                                   bev_feat_shape, self.interval_starts,
-                                   self.interval_lengths)
+                                   bev_feat_shape)
 
             bev_feat = bev_feat.squeeze(2)
         else:
@@ -585,25 +561,30 @@ class DepthNet(nn.Module):
     def gen_grid(self, metas, B, N, D, H, W, hi, wi):
         frustum = metas['frustum']
         points = frustum - metas['post_trans'].view(B, N, 1, 1, 1, 3)
-        points = torch.inverse(metas['post_rots']).view(B, N, 1, 1, 1, 3, 3) \
+        ori_shape = points.shape
+        points = points.view(B, N, -1, 3)
+        points = torch.inverse(metas['post_rots']).view(B, N, 1, 3, 3) \
             .matmul(points.unsqueeze(-1))
         points = torch.cat(
-            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 5)
+            (points[..., :2, :] * points[..., 2:3, :], points[..., 2:3, :]), 3)
 
         rots = metas['k2s_sensor'][:, :, :3, :3].contiguous()
         trans = metas['k2s_sensor'][:, :, :3, 3].contiguous()
         combine = rots.matmul(torch.inverse(metas['intrins']))
 
-        points = combine.view(B, N, 1, 1, 1, 3, 3).matmul(points)
-        points += trans.view(B, N, 1, 1, 1, 3, 1)
-        neg_mask = points[..., 2, 0] < 1e-3
-        points = metas['intrins'].view(B, N, 1, 1, 1, 3, 3).matmul(points)
+        points = combine.view(B, N, 1, 3, 3).matmul(points)
+        points += trans.view(B, N, 1, 3, 1)
+        neg_mask = (points.view(ori_shape))[..., 2, 0] < 1e-3
+        points = metas['intrins'].view(B, N, 1, 3, 3).matmul(points)
         points = points[..., :2, :] / points[..., 2:3, :]
 
-        points = metas['post_rots'][...,:2,:2].view(B, N, 1, 1, 1, 2, 2).matmul(
+        points = metas['post_rots'][...,:2,:2].view(B, N, 1, 2, 2).matmul(
             points).squeeze(-1)
-        points += metas['post_trans'][...,:2].view(B, N, 1, 1, 1, 2)
+        points += metas['post_trans'][...,:2].view(B, N, 1, 2)
 
+        new_shape = list(ori_shape)
+        new_shape[-1] = 2
+        points = points.view(new_shape)
         px = points[..., 0] / (wi - 1.0) * 2.0 - 1.0
         py = points[..., 1] / (hi - 1.0) * 2.0 - 1.0
         px[neg_mask] = -2
diff --git a/mmdet3d/ops/paconv/paconv.py b/mmdet3d/ops/paconv/paconv.py
index bda8bfe..9015b9e 100644
--- a/mmdet3d/ops/paconv/paconv.py
+++ b/mmdet3d/ops/paconv/paconv.py
@@ -97,8 +97,6 @@ class ScoreNet(nn.Module):
             scores = F.softmax(scores / self.temp_factor, dim=1)
         elif self.score_norm == 'sigmoid':
             scores = torch.sigmoid(scores / self.temp_factor)
-        else:  # 'identity'
-            scores = scores
 
         scores = scores.permute(0, 2, 3, 1)  # (B, N, K, M)
 
diff --git a/tests/test_utils/test_box3d.py b/tests/test_utils/test_box3d.py
index 69d8b31..5149884 100644
--- a/tests/test_utils/test_box3d.py
+++ b/tests/test_utils/test_box3d.py
@@ -1197,7 +1197,7 @@ def test_depth_boxes3d():
     # Test init with numpy array
     np_boxes = np.array(
         [[1.4856, 2.5299, -0.5570, 0.9385, 2.1404, 0.8954, 3.0601],
-         [2.3262, 3.3065, --0.44255, 0.8234, 0.5325, 1.0099, 2.9971]],
+         [2.3262, 3.3065, -0.44255, 0.8234, 0.5325, 1.0099, 2.9971]],
         dtype=np.float32)
     boxes_1 = DepthInstance3DBoxes(np_boxes)
     assert torch.allclose(boxes_1.tensor, torch.from_numpy(np_boxes))
diff --git a/tools/deployment/mmdet3d_handler.py b/tools/deployment/mmdet3d_handler.py
index 8b526cd..dd5699f 100644
--- a/tools/deployment/mmdet3d_handler.py
+++ b/tools/deployment/mmdet3d_handler.py
@@ -30,10 +30,9 @@ class MMdet3dHandler(BaseHandler):
                 pertaining to the model artifacts parameters.
         """
         properties = context.system_properties
-        self.map_location = 'cuda' if torch.cuda.is_available() else 'cpu'
+        self.map_location = 'npu'
         self.device = torch.device(self.map_location + ':' +
-                                   str(properties.get('gpu_id')) if torch.cuda.
-                                   is_available() else self.map_location)
+                                   str(properties.get('gpu_id')))
         self.manifest = context.manifest
 
         model_dir = properties.get('model_dir')
diff --git a/tools/dist_test.sh b/tools/dist_test.sh
index dea131b..6dafb49 100755
--- a/tools/dist_test.sh
+++ b/tools/dist_test.sh
@@ -9,7 +9,7 @@ PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+python -m torch.distributed.run \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
diff --git a/tools/dist_train.sh b/tools/dist_train.sh
index aa71bf4..c45b84d 100755
--- a/tools/dist_train.sh
+++ b/tools/dist_train.sh
@@ -8,7 +8,7 @@ PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+OMP_NUM_THREADS=16 MKL_NUM_THREADS=16 python -m torch.distributed.run \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
diff --git a/tools/test.py b/tools/test.py
index c669247..904623e 100644
--- a/tools/test.py
+++ b/tools/test.py
@@ -7,7 +7,7 @@ import mmcv
 import torch
 from mmcv import Config, DictAction
 from mmcv.cnn import fuse_conv_bn
-from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
                          wrap_fp16_model)
 
@@ -18,6 +18,9 @@ from mmdet3d.models import build_model
 from mmdet.apis import multi_gpu_test, set_random_seed
 from mmdet.datasets import replace_ImageToTensor
 
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 if mmdet.__version__ > '2.23.0':
     # If mmdet version > 2.23.0, setup_multi_processes would be imported and
     # used from mmdet instead of mmdet3d.
@@ -116,6 +119,7 @@ def parse_args():
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
+    parser.add_argument('--local-rank', type=int, default=0)
     parser.add_argument('--local_rank', type=int, default=0)
     args = parser.parse_args()
     if 'LOCAL_RANK' not in os.environ:
@@ -235,10 +239,10 @@ def main():
         model.PALETTE = dataset.PALETTE
 
     if not distributed:
-        model = MMDataParallel(model, device_ids=cfg.gpu_ids)
+        model = NPUDataParallel(model.npu(), device_ids=cfg.gpu_ids)
         outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
     else:
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False)
diff --git a/tools/train.py b/tools/train.py
index 72a1579..21ae95e 100644
--- a/tools/train.py
+++ b/tools/train.py
@@ -22,6 +22,9 @@ from mmdet3d.utils import collect_env, get_root_logger
 from mmdet.apis import set_random_seed
 from mmseg import __version__ as mmseg_version
 
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 try:
     # If mmdet version > 2.20.0, setup_multi_processes would be imported and
     # used from mmdet instead of mmdet3d.
@@ -93,6 +96,7 @@ def parse_args():
         choices=['none', 'pytorch', 'slurm', 'mpi'],
         default='none',
         help='job launcher')
+    parser.add_argument('--local-rank', type=int, default=0)
     parser.add_argument('--local_rank', type=int, default=0)
     parser.add_argument(
         '--autoscale-lr',
