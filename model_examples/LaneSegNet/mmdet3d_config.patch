diff --git a/mmdet3d/__init__.py b/mmdet3d/__init__.py
index 643c39c9..001b2247 100644
--- a/mmdet3d/__init__.py
+++ b/mmdet3d/__init__.py
@@ -19,7 +19,7 @@ def digit_version(version_str):
 
 
 mmcv_minimum_version = '1.5.2'
-mmcv_maximum_version = '1.7.0'
+mmcv_maximum_version = '1.7.2'
 mmcv_version = digit_version(mmcv.__version__)
 
 
diff --git a/mmdet3d/apis/train.py b/mmdet3d/apis/train.py
index 4d970264..ac6e8090 100644
--- a/mmdet3d/apis/train.py
+++ b/mmdet3d/apis/train.py
@@ -10,7 +10,8 @@ from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
                          build_runner, get_dist_info)
 from mmcv.utils import build_from_cfg
 from torch import distributed as dist
-
+import torch_npu
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmdet3d.datasets import build_dataset
 from mmdet3d.utils import find_latest_checkpoint
 from mmdet.core import DistEvalHook as MMDET_DistEvalHook
@@ -97,7 +98,6 @@ def train_segmentor(model,
             seed=cfg.seed,
             drop_last=True) for ds in dataset
     ]
-
     # put model on gpus
     if distributed:
         find_unused_parameters = cfg.get('find_unused_parameters', False)
@@ -223,7 +223,7 @@ def train_detector(model,
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
diff --git a/requirements/runtime.txt b/requirements/runtime.txt
index 643cb0cd..885e3129 100644
--- a/requirements/runtime.txt
+++ b/requirements/runtime.txt
@@ -1,6 +1,6 @@
 lyft_dataset_sdk
 networkx>=2.2,<2.3
-numba==0.53.0
+#numba==0.53.0
 numpy
 nuscenes-devkit
 plyfile
