diff --git a/mmdet/__init__.py b/mmdet/__init__.py
index 1f8ee169..d9e6ba13 100644
--- a/mmdet/__init__.py
+++ b/mmdet/__init__.py
@@ -1,4 +1,5 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import mmcv
 
 from .version import __version__, short_version
@@ -17,7 +18,7 @@ def digit_version(version_str):
 
 
 mmcv_minimum_version = '1.3.17'
-mmcv_maximum_version = '1.6.0'
+mmcv_maximum_version = '1.7.2'
 mmcv_version = digit_version(mmcv.__version__)
 
 
diff --git a/mmdet/models/backbones/resnet.py b/mmdet/models/backbones/resnet.py
index 1eaaae67..9d224e54 100644
--- a/mmdet/models/backbones/resnet.py
+++ b/mmdet/models/backbones/resnet.py
@@ -1,4 +1,5 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import warnings
 
 import torch.nn as nn
@@ -6,6 +7,9 @@ import torch.utils.checkpoint as cp
 from mmcv.cnn import build_conv_layer, build_norm_layer, build_plugin_layer
 from mmcv.runner import BaseModule
 from torch.nn.modules.batchnorm import _BatchNorm
+import mx_driving
+import torch
+import torch_npu
 
 from ..builder import BACKBONES
 from ..utils import ResLayer
@@ -288,7 +292,7 @@ class Bottleneck(BaseModule):
             if self.downsample is not None:
                 identity = self.downsample(x)
 
-            out += identity
+            out = mx_driving.npu_add_relu(out, identity)
 
             return out
 
@@ -297,8 +301,6 @@ class Bottleneck(BaseModule):
         else:
             out = _inner_forward(x)
 
-        out = self.relu(out)
-
         return out
 
 
@@ -608,7 +610,6 @@ class ResNet(BaseModule):
                 self.norm_cfg, stem_channels, postfix=1)
             self.add_module(self.norm1_name, norm1)
             self.relu = nn.ReLU(inplace=True)
-        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
 
     def _freeze_stages(self):
         if self.frozen_stages >= 0:
@@ -636,7 +637,7 @@ class ResNet(BaseModule):
             x = self.conv1(x)
             x = self.norm1(x)
             x = self.relu(x)
-        x = self.maxpool(x)
+        x = mx_driving.npu_max_pool2d(x, 3, 2, 1)
         outs = []
         for i, layer_name in enumerate(self.res_layers):
             res_layer = getattr(self, layer_name)
