diff --git a/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py b/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
index 58a49f0..6c5e8e1 100644
--- a/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
+++ b/projects/configs/PanoOcc/Panoptic/PanoOcc_base_4f.py
@@ -307,7 +307,7 @@ data = dict(
 )
 
 optimizer = dict(
-    type='AdamW',
+    type='NpuFusedAdamW',
     lr=4e-4,
     paramwise_cfg=dict(
         custom_keys={
diff --git a/projects/configs/_base_/default_runtime.py b/projects/configs/_base_/default_runtime.py
index 4e85b69..cd301c6 100644
--- a/projects/configs/_base_/default_runtime.py
+++ b/projects/configs/_base_/default_runtime.py
@@ -10,7 +10,7 @@ log_config = dict(
         dict(type='TensorboardLoggerHook')
     ])
 # yapf:enable
-dist_params = dict(backend='nccl')
+dist_params = dict(backend='hccl')
 log_level = 'INFO'
 work_dir = None
 load_from = None
diff --git a/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py b/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
index e57bd22..fc4600b 100644
--- a/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
+++ b/projects/mmdet3d_plugin/bevformer/apis/mmdet_train.py
@@ -9,7 +9,7 @@ import warnings
 import numpy as np
 import torch
 import torch.distributed as dist
-from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.runner import (HOOKS, DistSamplerSeedHook, EpochBasedRunner,
                          Fp16OptimizerHook, OptimizerHook, build_optimizer,
                          build_runner, get_dist_info)
@@ -64,6 +64,7 @@ def custom_train_detector(model,
             seed=cfg.seed,
             shuffler_sampler=cfg.data.shuffler_sampler,  # dict(type='DistributedGroupSampler'),
             nonshuffler_sampler=cfg.data.nonshuffler_sampler,  # dict(type='DistributedSampler'),
+            pin_memory=True
         ) for ds in dataset
     ]
 
@@ -72,22 +73,22 @@ def custom_train_detector(model,
         find_unused_parameters = cfg.get('find_unused_parameters', False)
         # Sets the `find_unused_parameters` parameter in
         # torch.nn.parallel.DistributedDataParallel
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False,
             find_unused_parameters=find_unused_parameters)
         if eval_model is not None:
-            eval_model = MMDistributedDataParallel(
+            eval_model = NPUDistributedDataParallel(
                 eval_model.cuda(),
                 device_ids=[torch.cuda.current_device()],
                 broadcast_buffers=False,
                 find_unused_parameters=find_unused_parameters)
     else:
-        model = MMDataParallel(
+        model = NPUDataParallel(
             model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
         if eval_model is not None:
-            eval_model = MMDataParallel(
+            eval_model = NPUDataParallel(
                 eval_model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)
 
 
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py b/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
index a831372..8d9815a 100644
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/dense_heads/__init__.py
@@ -1,3 +1,2 @@
 from .pano_occ_head import PanoOccHead
-from .panoseg_occ_head import PanoSegOccHead
-from .panoseg_occ_sparse_head import SparseOccupancyHead
\ No newline at end of file
+from .panoseg_occ_head import PanoSegOccHead
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py b/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
index cc986f0..aadce74 100644
--- a/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
+++ b/projects/mmdet3d_plugin/bevformer/dense_heads/panoseg_occ_head.py
@@ -18,10 +18,42 @@ import numpy as np
 import mmcv
 import cv2 as cv
 from projects.mmdet3d_plugin.models.utils.visual import save_tensor
-from mmdet3d.ops import scatter_v2
-import torch_scatter
+import mx_driving
+import mx_driving._C
 from mmdet.models.builder import build_loss
 
+
+def custom_unique_n3(coors, return_inverse, return_counts, dim):
+    # assert dim == 0
+
+    voxels = mx_driving._C.point_to_voxel(coors, [], [], "ZYX")
+    cnt, unq_voxels, unq_ind, argsort_ind, _ = mx_driving._C.unique_voxel(voxels)
+    unq_coors = mx_driving._C.voxel_to_point(unq_voxels, [], [], "ZYX")
+    
+    if return_inverse:
+        sorted_ind = torch.argsort(argsort_ind.to(torch.float32), dim=dim).to(torch.long)
+        is_unq = torch.zeros(coors.size(0)).to(coors.device)
+        is_unq[unq_ind] = 1
+        unq_inv_sorted = is_unq.cumsum(dim) - 1
+        unq_inv = torch.gather(unq_inv_sorted, dim, sorted_ind)
+        unq_inv = unq_inv.to(torch.int64)
+
+    if return_counts:
+        unq_ind_nxt = torch.ones_like(unq_ind) * coors.size(0)
+        unq_ind_nxt[:-1] = unq_ind[1:]
+        unq_cnt = unq_ind_nxt - unq_ind
+        unq_cnt = unq_cnt.to(torch.int64)
+
+    if return_inverse and return_counts:
+        return unq_coors, unq_inv, unq_cnt
+    elif return_inverse:
+        return unq_coors, unq_inv
+    elif return_counts:
+        return unq_coors, unq_cnt
+    else:
+        return unq_coors
+
+
 @HEADS.register_module()
 class PanoSegOccHead(DETRHead):
     def __init__(self,
@@ -35,22 +67,22 @@ class PanoSegOccHead(DETRHead):
                  bev_h=30,
                  bev_w=30,
                  bev_z=5,
-                 voxel_lidar = [0.05, 0.05, 0.05],
-                 voxel_det = [2.048,2.048,1],
+                 voxel_lidar=[0.05, 0.05, 0.05],
+                 voxel_det=[2.048,2.048,1],
                  loss_occupancy=dict(
                     type='FocalLoss',
                     use_sigmoid=True,
                     gamma=2.0,
                     alpha=0.25,
                     loss_weight=5.0),
-                loss_occupancy_aux = None,
+                loss_occupancy_aux=None,
                 loss_occupancy_det=dict(
                     type='FocalLoss',
                     use_sigmoid=True,
                     gamma=2.0,
                     alpha=0.25,
                     loss_weight=5.0),
-                bg_weight = 0.02,
+                bg_weight=0.02,
                  **kwargs):
 
         self.bev_h = bev_h
@@ -88,6 +120,13 @@ class PanoSegOccHead(DETRHead):
         if loss_occupancy_aux is not None:
             self.lidar_seg_aux_loss = build_loss(loss_occupancy_aux)
 
+        self.pc_range = nn.Parameter(torch.tensor(
+            self.pc_range, requires_grad=False), requires_grad=False)
+        self.voxel_lidar = nn.Parameter(torch.tensor(
+            self.voxel_lidar, requires_grad=False), requires_grad=False)
+        self.voxel_det = nn.Parameter(torch.tensor(
+            self.voxel_det, requires_grad=False), requires_grad=False)
+
     def _init_layers(self):
         """Initialize classification branch and regression branch of head."""
         cls_branch = []
@@ -159,7 +198,7 @@ class PanoSegOccHead(DETRHead):
         object_query_embeds = self.query_embedding.weight.to(dtype)
         bev_queries = self.bev_embedding.weight.to(dtype)
 
-        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w, self.bev_z),device=bev_queries.device).to(dtype)
+        bev_mask = torch.zeros((bs, self.bev_h, self.bev_w, self.bev_z), device=bev_queries.device, dtype=dtype)
         bev_pos = self.positional_encoding(bev_mask).to(dtype)
 
         if only_bev:
@@ -180,21 +219,21 @@ class PanoSegOccHead(DETRHead):
             )
             bev_feat, bev_embed, hs, init_reference, inter_references, occupancy, occupancy_det = outputs
             return bev_feat, bev_embed
-        else:
-            outputs = self.transformer(
-                mlvl_feats,
-                bev_queries,
-                object_query_embeds,
-                self.bev_h,
-                self.bev_w,
-                self.bev_z,
-                grid_length=(self.real_h / self.bev_h,
-                             self.real_w / self.bev_w),
-                bev_pos=bev_pos,
-                reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
-                cls_branches=self.cls_branches if self.as_two_stage else None,
-                img_metas=img_metas,
-                prev_bev=prev_bev
+
+        outputs = self.transformer(
+            mlvl_feats,
+            bev_queries,
+            object_query_embeds,
+            self.bev_h,
+            self.bev_w,
+            self.bev_z,
+            grid_length=(self.real_h / self.bev_h,
+                            self.real_w / self.bev_w),
+            bev_pos=bev_pos,
+            reg_branches=self.reg_branches if self.with_box_refine else None,  # noqa:E501
+            cls_branches=self.cls_branches if self.as_two_stage else None,
+            img_metas=img_metas,
+            prev_bev=prev_bev
         )
 
         bev_feat, bev_embed, hs, init_reference, inter_references, occupancy, occupancy_det = outputs
@@ -211,7 +250,7 @@ class PanoSegOccHead(DETRHead):
             tmp = self.reg_branches[lvl](hs[lvl])
 
             # TODO: check the shape of reference
-            assert reference.shape[-1] == 3
+            # assert reference.shape[-1] == 3
             tmp[..., 0:2] += reference[..., 0:2]
             tmp[..., 0:2] = tmp[..., 0:2].sigmoid()
             tmp[..., 4:5] += reference[..., 2:3]
@@ -279,7 +318,7 @@ class PanoSegOccHead(DETRHead):
         gt_c = gt_bboxes.shape[-1]
 
         assign_result = self.assigner.assign(bbox_pred, cls_score, gt_bboxes,
-                                             gt_labels, gt_bboxes_ignore)
+                                             gt_labels)
 
         sampling_result = self.sampler.sample(assign_result, bbox_pred,
                                               gt_bboxes)
@@ -338,17 +377,11 @@ class PanoSegOccHead(DETRHead):
                 - num_total_neg (int): Number of negative samples in all \
                     images.
         """
-        assert gt_bboxes_ignore_list is None, \
-            'Only supports for gt_bboxes_ignore setting to None.'
-        num_imgs = len(cls_scores_list)
-        gt_bboxes_ignore_list = [
-            gt_bboxes_ignore_list for _ in range(num_imgs)
-        ]
 
         (labels_list, label_weights_list, bbox_targets_list,
          bbox_weights_list, pos_inds_list, neg_inds_list) = multi_apply(
             self._get_target_single, cls_scores_list, bbox_preds_list,
-            gt_labels_list, gt_bboxes_list, gt_bboxes_ignore_list)
+            gt_labels_list, gt_bboxes_list)
         num_total_pos = sum((inds.numel() for inds in pos_inds_list))
         num_total_neg = sum((inds.numel() for inds in neg_inds_list))
         return (labels_list, label_weights_list, bbox_targets_list,
@@ -382,8 +415,7 @@ class PanoSegOccHead(DETRHead):
         cls_scores_list = [cls_scores[i] for i in range(num_imgs)]
         bbox_preds_list = [bbox_preds[i] for i in range(num_imgs)]
         cls_reg_targets = self.get_targets(cls_scores_list, bbox_preds_list,
-                                           gt_bboxes_list, gt_labels_list,
-                                           gt_bboxes_ignore_list)
+                                           gt_bboxes_list, gt_labels_list)
         (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,
          num_total_pos, num_total_neg) = cls_reg_targets
         labels = torch.cat(labels_list, 0)
@@ -394,11 +426,9 @@ class PanoSegOccHead(DETRHead):
         # classification loss
         cls_scores = cls_scores.reshape(-1, self.cls_out_channels)
         # construct weighted avg_factor to match with the official DETR repo
-        cls_avg_factor = num_total_pos * 1.0 + \
-            num_total_neg * self.bg_cls_weight
+        cls_avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_cls_weight
         if self.sync_cls_avg_factor:
-            cls_avg_factor = reduce_mean(
-                cls_scores.new_tensor([cls_avg_factor]))
+            cls_avg_factor = reduce_mean(cls_scores.new_tensor([cls_avg_factor]))
 
         cls_avg_factor = max(cls_avg_factor, 1)
 
@@ -417,7 +447,7 @@ class PanoSegOccHead(DETRHead):
         bbox_weights = bbox_weights * self.code_weights
 
         loss_bbox = self.loss_bbox(
-            bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan,:10], bbox_weights[isnotnan, :10],
+            bbox_preds[isnotnan, :10], normalized_bbox_targets[isnotnan, :10], bbox_weights[isnotnan, :10],
             avg_factor=num_total_pos)
         if digit_version(TORCH_VERSION) >= digit_version('1.8'):
             loss_cls = torch.nan_to_num(loss_cls)
@@ -426,13 +456,13 @@ class PanoSegOccHead(DETRHead):
     
     def get_occupancy_det_label(self,voxel_coors_det, voxel_label_det, occupancy_det_label):
 
-        voxel_coors_det[:,1] = voxel_coors_det[:,1].clip(min=0,max=self.bev_z-1)
-        voxel_coors_det[:,2] = voxel_coors_det[:,2].clip(min=0,max=self.bev_h-1)
-        voxel_coors_det[:,3] = voxel_coors_det[:,3].clip(min=0,max=self.bev_w-1)
+        voxel_coors_det[:, 0] = voxel_coors_det[:, 0].clip(min=0, max=self.bev_z-1)
+        voxel_coors_det[:, 1] = voxel_coors_det[:, 1].clip(min=0, max=self.bev_h-1)
+        voxel_coors_det[:, 2] = voxel_coors_det[:, 2].clip(min=0, max=self.bev_w-1)
 
-        det_label_binary = ((voxel_label_det>=1)&(voxel_label_det<=10))
+        det_label_binary = ((voxel_label_det>=1) & (voxel_label_det<=10))
         det_label = det_label_binary.long()
-        occupancy_det_label[0,voxel_coors_det[:,1],voxel_coors_det[:,2],voxel_coors_det[:,3]]=det_label
+        occupancy_det_label[0, voxel_coors_det[:, 0], voxel_coors_det[:, 1], voxel_coors_det[:, 2]] = det_label
         return occupancy_det_label
     
     def get_det_loss(self,voxel_label_det,occupancy_det_label,occupancy_det_pred):
@@ -446,7 +476,7 @@ class PanoSegOccHead(DETRHead):
                 occupancy_det_pred.new_tensor([avg_factor_det]))
         avg_factor_det = max(avg_factor_det, 1)
 
-        losses_det = self.lidar_det_loss(occupancy_det_pred,occupancy_det_label,avg_factor=avg_factor_det)
+        losses_det = self.lidar_det_loss(occupancy_det_pred, occupancy_det_label, avg_factor=avg_factor_det)
         return losses_det
     
     @force_fp32(apply_to=('preds_dicts'))
@@ -455,7 +485,7 @@ class PanoSegOccHead(DETRHead):
              gt_labels_list,
              pts_sem,
              preds_dicts,
-             dense_occupancy = None,
+             dense_occupancy=None,
              gt_bboxes_ignore=None,
              img_metas=None):
         """"Loss function.
@@ -485,9 +515,6 @@ class PanoSegOccHead(DETRHead):
         Returns:
             dict[str, Tensor]: A dictionary of loss components.
         """
-        assert gt_bboxes_ignore is None, \
-            f'{self.__class__.__name__} only supports ' \
-            f'for gt_bboxes_ignore setting to None.'
 
        # Extract the first three columns from pts_sem
         pts = pts_sem[:, :3]
@@ -497,12 +524,10 @@ class PanoSegOccHead(DETRHead):
 
         # If dense_occupancy is None, perform voxelization and label voxelization
         if dense_occupancy is None:
-            pts_coors, voxelized_data, voxel_coors = self.voxelize(pts, self.pc_range, self.voxel_lidar)
-            voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
+            _, voxel_coors, voxel_label = self.voxelize(pts, self.pc_range, self.voxel_lidar, pts_semantic_mask)
 
         # Perform voxelization and label voxelization for detection
-        pts_coors_det, voxelized_data_det, voxel_coors_det = self.voxelize(pts, self.pc_range, self.voxel_det)
-        voxel_label_det = self.label_voxelization(pts_semantic_mask, pts_coors_det, voxel_coors_det)
+        _, voxel_coors_det, voxel_label_det = self.voxelize(pts, self.pc_range, self.voxel_det, pts_semantic_mask)
         
         all_cls_scores = preds_dicts['all_cls_scores']
         all_bbox_preds = preds_dicts['all_bbox_preds']
@@ -514,31 +539,31 @@ class PanoSegOccHead(DETRHead):
         occupancy_pred = occupancy.squeeze(0)
         occupancy_det_pred = occupancy_det[0].squeeze(0)
 
-        cls_num,occ_z,occ_h,occ_w = occupancy_pred.shape
+        cls_num, occ_z, occ_h, occ_w = occupancy_pred.shape
         if dense_occupancy is None:
             occupancy_label = torch.full((1, occ_z, occ_h, occ_w), cls_num, device=occupancy_pred.device, dtype=torch.long)
         else:
-            occupancy_label = (torch.zeros(1,occ_z,occ_h,occ_w)).to(occupancy_pred.device).long()
+            occupancy_label = (torch.zeros(1, occ_z, occ_h, occ_w)).to(occupancy_pred.device).long()
        
-        occupancy_det_label = (torch.ones(1,self.bev_z,self.bev_h,self.bev_w)*2).to(occupancy_det_pred.device).long()
+        occupancy_det_label = (torch.ones(1, self.bev_z, self.bev_h, self.bev_w) * 2).to(occupancy_det_pred.device).long()
 
         if dense_occupancy is None:
-            voxel_coors[:,1] = voxel_coors[:,1].clip(min=0,max=occ_z-1)
-            voxel_coors[:,2] = voxel_coors[:,2].clip(min=0,max=occ_h-1)
-            voxel_coors[:,3] = voxel_coors[:,3].clip(min=0,max=occ_w-1)
-            occupancy_label[0,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]] = voxel_label
+            voxel_coors[:, 0] = voxel_coors[:, 0].clip(min=0, max=occ_z-1)
+            voxel_coors[:, 1] = voxel_coors[:, 1].clip(min=0, max=occ_h-1)
+            voxel_coors[:, 2] = voxel_coors[:, 2].clip(min=0, max=occ_w-1)
+            occupancy_label[0, voxel_coors[:, 0], voxel_coors[:, 1], voxel_coors[:, 2]] = voxel_label
         else:
             dense_occupancy = dense_occupancy.long().squeeze(0)
-            occupancy_label[0,dense_occupancy[:,0],dense_occupancy[:,1],dense_occupancy[:,2]]=dense_occupancy[:,3]
+            occupancy_label[0, dense_occupancy[:, 0], dense_occupancy[:, 1], dense_occupancy[:, 2]] = dense_occupancy[:, 3]
 
         occupancy_det_label = self.get_occupancy_det_label(voxel_coors_det, voxel_label_det, occupancy_det_label)
 
-        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0),occupancy_label)
+        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0), occupancy_label)
 
         occupancy_det_label = occupancy_det_label.reshape(-1)
         occupancy_label = occupancy_label.reshape(-1)
 
-        assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
+        # assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
         occupancy_pred = occupancy_pred.reshape(cls_num,-1).permute(1,0)
         occupancy_det_pred = occupancy_det_pred.reshape(2,-1).permute(1,0)
 
@@ -550,14 +575,10 @@ class PanoSegOccHead(DETRHead):
 
         all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]
         all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]
-        all_gt_bboxes_ignore_list = [
-            gt_bboxes_ignore for _ in range(num_dec_layers)
-        ]
 
         losses_cls, losses_bbox = multi_apply(
             self.loss_single, all_cls_scores, all_bbox_preds,
-            all_gt_bboxes_list, all_gt_labels_list,
-            all_gt_bboxes_ignore_list)
+            all_gt_bboxes_list, all_gt_labels_list)
 
         loss_dict = dict()
 
@@ -566,17 +587,17 @@ class PanoSegOccHead(DETRHead):
             num_total_pos = len(voxel_label)
         else:
             num_total_pos = len(dense_occupancy)
-        num_total_neg = len(occupancy_label)-num_total_pos
+        num_total_neg = len(occupancy_label) - num_total_pos
         avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
         if self.sync_cls_avg_factor:
             avg_factor = reduce_mean(
                 occupancy_pred.new_tensor([avg_factor]))
         avg_factor = max(avg_factor, 1)
 
-        losses_seg = self.lidar_seg_loss(occupancy_pred,occupancy_label,avg_factor=avg_factor)
+        losses_seg = self.lidar_seg_loss(occupancy_pred, occupancy_label, avg_factor=avg_factor)
 
         # Lidar det loss
-        losses_det = self.get_det_loss(voxel_label_det,occupancy_det_label,occupancy_det_pred)
+        losses_det = self.get_det_loss(voxel_label_det, occupancy_det_label, occupancy_det_pred)
 
         # loss of proposal generated from encode feature map.
         if enc_cls_scores is not None:
@@ -586,7 +607,7 @@ class PanoSegOccHead(DETRHead):
             ]
             enc_loss_cls, enc_losses_bbox = \
                 self.loss_single(enc_cls_scores, enc_bbox_preds,
-                                 gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)
+                                 gt_bboxes_list, binary_labels_list)
             loss_dict['enc_loss_cls'] = enc_loss_cls
             loss_dict['enc_loss_bbox'] = enc_losses_bbox
 
@@ -599,187 +620,35 @@ class PanoSegOccHead(DETRHead):
 
         # loss from other decoder layers
         num_dec_layer = 0
-        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1],
-                                           losses_bbox[:-1]):
+        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1], losses_bbox[:-1]):
             loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i
             loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i
             num_dec_layer += 1
 
         return loss_dict
     
-    @force_fp32(apply_to=('preds_dicts'))
-    def loss_new(self,
-             gt_bboxes_list,
-             gt_labels_list,
-             pts_sem,
-             preds_dicts,
-             dense_occupancy = None,
-             gt_bboxes_ignore=None,
-             img_metas=None):
-        """"Loss function.
-        Args:
-
-            gt_bboxes_list (list[Tensor]): Ground truth bboxes for each image
-                with shape (num_gts, 4) in [tl_x, tl_y, br_x, br_y] format.
-            gt_labels_list (list[Tensor]): Ground truth class indices for each
-                image with shape (num_gts, ).
-            preds_dicts:
-                all_cls_scores (Tensor): Classification score of all
-                    decoder layers, has shape
-                    [nb_dec, bs, num_query, cls_out_channels].
-                all_bbox_preds (Tensor): Sigmoid regression
-                    outputs of all decode layers. Each is a 4D-tensor with
-                    normalized coordinate format (cx, cy, w, h) and shape
-                    [nb_dec, bs, num_query, 4].
-                enc_cls_scores (Tensor): Classification scores of
-                    points on encode feature map , has shape
-                    (N, h*w, num_classes). Only be passed when as_two_stage is
-                    True, otherwise is None.
-                enc_bbox_preds (Tensor): Regression results of each points
-                    on the encode feature map, has shape (N, h*w, 4). Only be
-                    passed when as_two_stage is True, otherwise is None.
-            gt_bboxes_ignore (list[Tensor], optional): Bounding boxes
-                which can be ignored for each image. Default None.
-        Returns:
-            dict[str, Tensor]: A dictionary of loss components.
+    def voxelize(self, points, pc_range, voxel_size, pts_semantic_mask=None):
         """
-        assert gt_bboxes_ignore is None, \
-            f'{self.__class__.__name__} only supports ' \
-            f'for gt_bboxes_ignore setting to None.'
-
-        # GT voxel supervision
-        pts = pts_sem[:,:3]
-        pts_semantic_mask = pts_sem[:,3:4]
-
-        pts_numpy = pts.cpu().numpy()
-        pts_semantic_mask_numpy = pts_semantic_mask.cpu().numpy()
-        points_grid_ind = np.floor((np.clip(pts_numpy, self.pc_range[:3],self.pc_range[3:]) - self.pc_range[:3]) / self.voxel_lidar).astype(np.int)
-        label_voxel_pair = np.concatenate([points_grid_ind, pts_semantic_mask_numpy], axis=1)
-        label_voxel_pair = label_voxel_pair[np.lexsort((points_grid_ind[:, 0], points_grid_ind[:, 1], points_grid_ind[:, 2])), :]
-        label_voxel = torch.tensor(label_voxel_pair).to(pts.device).long()
-        if dense_occupancy is None:
-            pts_coors,voxelized_data,voxel_coors = self.voxelize(pts,self.pc_range,self.voxel_lidar)
-            voxel_label = self.label_voxelization(pts_semantic_mask, pts_coors, voxel_coors)
-
-        pts_coors_det,voxelized_data_det,voxel_coors_det = self.voxelize(pts,self.pc_range,self.voxel_det)
-        voxel_label_det = self.label_voxelization(pts_semantic_mask, pts_coors_det, voxel_coors_det)
-
-        all_cls_scores = preds_dicts['all_cls_scores']
-        all_bbox_preds = preds_dicts['all_bbox_preds']
-        enc_cls_scores = preds_dicts['enc_cls_scores']
-        enc_bbox_preds = preds_dicts['enc_bbox_preds']
-        occupancy = preds_dicts['occupancy']
-        occupancy_det = preds_dicts['occupancy_det']
-
-        occupancy_pred = occupancy.squeeze(0)
-        occupancy_det_pred = occupancy_det.squeeze(0)
-
-        cls_num,occ_z,occ_h,occ_w = occupancy_pred.shape
-        if dense_occupancy is None:
-            occupancy_label = (torch.ones(1,occ_z,occ_h,occ_w)*cls_num).to(occupancy_pred.device).long()
-        else:
-            occupancy_label = (torch.zeros(1,occ_z,occ_h,occ_w)).to(occupancy_pred.device).long()
-        occupancy_det_label = (torch.ones(1,self.bev_z,self.bev_h,self.bev_w)*2).to(occupancy_det_pred.device).long()
-
-        # Matrix operation acceleration
-        if dense_occupancy is None:
-            occupancy_label[0,label_voxel[:,2],label_voxel[:,1],label_voxel[:,0]] = label_voxel[:,3]
-        else:
-            dense_occupancy = dense_occupancy.long().squeeze(0)
-            occupancy_label[0,dense_occupancy[:,0],dense_occupancy[:,1],dense_occupancy[:,2]]=dense_occupancy[:,3]
-
-        voxel_coors_det[:,1] = voxel_coors_det[:,1].clip(min=0,max=self.bev_z-1)
-        voxel_coors_det[:,2] = voxel_coors_det[:,2].clip(min=0,max=self.bev_h-1)
-        voxel_coors_det[:,3] = voxel_coors_det[:,3].clip(min=0,max=self.bev_w-1)
-
-        det_label_binary = ((voxel_label_det>=1)&(voxel_label_det<=10))
-        det_label = det_label_binary.long()
-        occupancy_det_label[0,voxel_coors_det[:,1],voxel_coors_det[:,2],voxel_coors_det[:,3]]=det_label
-
-        losses_seg_aux = self.lidar_seg_aux_loss(occupancy_pred.unsqueeze(0),occupancy_label)
-
-        occupancy_det_label = occupancy_det_label.reshape(-1)
-        occupancy_label = occupancy_label.reshape(-1)
-
-
-        assert occupancy_label.max()<=cls_num and occupancy_label.min()>=0
-        occupancy_pred = occupancy_pred.reshape(cls_num,-1).permute(1,0)
-        occupancy_det_pred = occupancy_det_pred.reshape(2,-1).permute(1,0)
-
-        num_dec_layers = len(all_cls_scores)
-        device = gt_labels_list[0].device
-
-        gt_bboxes_list = [torch.cat((gt_bboxes.gravity_center, gt_bboxes.tensor[:, 3:]),
-            dim=1).to(device) for gt_bboxes in gt_bboxes_list]
-
-        all_gt_bboxes_list = [gt_bboxes_list for _ in range(num_dec_layers)]
-        all_gt_labels_list = [gt_labels_list for _ in range(num_dec_layers)]
-        all_gt_bboxes_ignore_list = [
-            gt_bboxes_ignore for _ in range(num_dec_layers)
-        ]
-
-        losses_cls, losses_bbox = multi_apply(
-            self.loss_single, all_cls_scores, all_bbox_preds,
-            all_gt_bboxes_list, all_gt_labels_list,
-            all_gt_bboxes_ignore_list)
-
-        loss_dict = dict()
-
-        # Lidar seg loss
-        if dense_occupancy is None:
-            num_total_pos = len(voxel_label)
-        else:
-            num_total_pos = len(dense_occupancy)
-        num_total_neg = len(occupancy_label)-num_total_pos
-        avg_factor = num_total_pos * 1.0 + num_total_neg * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor = reduce_mean(
-                occupancy_pred.new_tensor([avg_factor]))
-        avg_factor = max(avg_factor, 1)
-
-        losses_seg = self.lidar_seg_loss(occupancy_pred,occupancy_label,avg_factor=avg_factor)
-
-        # Lidar det loss
-        num_total_pos_det = len(voxel_label_det)
-
-
-        num_total_neg_det = len(occupancy_det_label)-num_total_pos_det
-        avg_factor_det = num_total_pos_det * 1.0 + num_total_neg_det * self.bg_weight
-        if self.sync_cls_avg_factor:
-            avg_factor_det = reduce_mean(
-                occupancy_det_pred.new_tensor([avg_factor_det]))
-        avg_factor_det = max(avg_factor_det, 1)
+        Input:
+            points [N, 3], (x, y, z)
+            point_cloud_range [6], [-51.2, -51.2, -5.0, 51.2, 51.2, 3.0], (-x, -y, -z, x, y, z)
+            voxelization_size [3], e.g. [0.256, 0.256, 0.125]
 
-        losses_det = self.lidar_det_loss(occupancy_det_pred,occupancy_det_label,avg_factor=avg_factor_det)
+        Output:
+            coors [N,4], (0, z, y, x)
+            unq_coors [M,4], (0, z, y, x)
 
-        # loss of proposal generated from encode feature map.
-        if enc_cls_scores is not None:
-            binary_labels_list = [
-                torch.zeros_like(gt_labels_list[i])
-                for i in range(len(all_gt_labels_list))
-            ]
-            enc_loss_cls, enc_losses_bbox = \
-                self.loss_single(enc_cls_scores, enc_bbox_preds,
-                                 gt_bboxes_list, binary_labels_list, gt_bboxes_ignore)
-            loss_dict['enc_loss_cls'] = enc_loss_cls
-            loss_dict['enc_loss_bbox'] = enc_losses_bbox
+        """
 
-        # loss from the last decoder layer
-        loss_dict['loss_cls'] = losses_cls[-1]
-        loss_dict['loss_bbox'] = losses_bbox[-1]
-        loss_dict['loss_seg'] = losses_seg
-        loss_dict['loss_det'] = losses_det
-        loss_dict['loss_seg_aux'] = losses_seg_aux
+        coors = torch.div(points[:, :3] - pc_range[None, :3], voxel_size[None, :], rounding_mode='floor').to(torch.int32)
 
-        # loss from other decoder layers
-        num_dec_layer = 0
-        for loss_cls_i, loss_bbox_i in zip(losses_cls[:-1],
-                                           losses_bbox[:-1]):
-            loss_dict[f'd{num_dec_layer}.loss_cls'] = loss_cls_i
-            loss_dict[f'd{num_dec_layer}.loss_bbox'] = loss_bbox_i
-            num_dec_layer += 1
+        unq_coors, unq_inv = custom_unique_n3(coors, return_inverse=True, return_counts=False, dim=0)
 
-        return loss_dict
+        if pts_semantic_mask is not None:
+            with torch.no_grad():
+                voxel_label_my, _ = mx_driving.scatter_max(pts_semantic_mask, unq_inv.to(torch.int32))
+            return coors[:, [2, 1, 0]].long(), unq_coors.long(), voxel_label_my.squeeze(-1).long()
+        return coors[:, [2, 1, 0]].long(), unq_coors.long()
 
     @force_fp32(apply_to=('preds_dicts'))
     def get_bboxes(self, preds_dicts, img_metas, rescale=False):
@@ -810,190 +679,58 @@ class PanoSegOccHead(DETRHead):
 
         return ret_list
 
-    def decode_lidar_seg(self,points,occupancy):
+    def decode_lidar_seg(self, points, occupancy):
 
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
+        pts_coors, _ = self.voxelize(points, self.pc_range, self.voxel_lidar)
 
         # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-
-        # valid_mask = (pts_coors[:,1].cpu().numpy()>=0) & (pts_coors[:,1].cpu().numpy()<=z_max) \
-        #     & (pts_coors[:,2].cpu().numpy()>=0) & (pts_coors[:,2].cpu().numpy()<=y_max) \
-        #     & (pts_coors[:,3].cpu().numpy()>=0) & (pts_coors[:,3].cpu().numpy()<=x_max) 
+        z_max = int((self.pc_range[5] - self.pc_range[2]) / self.voxel_lidar[2]) - 1
+        y_max = int((self.pc_range[4] - self.pc_range[1]) / self.voxel_lidar[1]) - 1
+        x_max = int((self.pc_range[3] - self.pc_range[0]) / self.voxel_lidar[0]) - 1
         
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_coors[:, 0] = pts_coors[:, 0].clip(min=0, max=z_max)
+        pts_coors[:, 1] = pts_coors[:, 1].clip(min=0, max=y_max)
+        pts_coors[:, 2] = pts_coors[:, 2].clip(min=0, max=x_max)
 
-        # pts_pred[valid_mask==False]=15
+        pts_pred = occupancy[:, :, pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
 
         return pts_pred
 
-    def voxelize(self, points,point_cloud_range,voxelization_size):
-        """
-        Input:
-            points
-
-        Output:
-            coors [N,4]
-            voxelized_data [M,3]
-            voxel_coors [M,4]
-
-        """
-        voxel_size = torch.tensor(voxelization_size, device=points.device)
-        pc_range = torch.tensor(point_cloud_range, device=points.device)
-        coors = torch.div(points[:, :3] - pc_range[None, :3], voxel_size[None, :], rounding_mode='floor').long()
-        coors = coors[:, [2, 1, 0]] # to zyx order
-
-        new_coors, unq_inv  = torch.unique(coors, return_inverse=True, return_counts=False, dim=0)
-
-        voxelized_data, voxel_coors = scatter_v2(points, coors, mode='avg', return_inv=False, new_coors=new_coors, unq_inv=unq_inv)
-
-        batch_idx_pts = torch.zeros(coors.size(0),1).to(device=points.device)
-        batch_idx_vox = torch.zeros(voxel_coors.size(0),1).to(device=points.device)
-
-        coors_batch = torch.cat([batch_idx_pts,coors],dim=1)
-        voxel_coors_batch = torch.cat([batch_idx_vox,voxel_coors],dim=1)
-
-        return coors_batch.long(),voxelized_data,voxel_coors_batch.long()
-
     def decode_lidar_seg_hr(self,points,occupancy):
 
         out_h = 512
         out_w = 512
         out_z = 160
 
-        self.voxel_lidar = [102.4/out_h,102.4/out_w,8/out_z]
+        self.voxel_lidar = [102.4/out_h, 102.4/out_w, 8/out_z]
 
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
+        pts_coors, _ = self.voxelize(points, self.pc_range, self.voxel_lidar)
 
         # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
+        z_max = int((self.pc_range[5] - self.pc_range[2]) / self.voxel_lidar[2]) - 1
+        y_max = int((self.pc_range[4] - self.pc_range[1]) / self.voxel_lidar[1]) - 1
+        x_max = int((self.pc_range[3] - self.pc_range[0]) / self.voxel_lidar[0]) - 1
+        pts_coors[:, 0] = pts_coors[:, 0].clip(min=0, max=z_max)
+        pts_coors[:, 1] = pts_coors[:, 1].clip(min=0, max=y_max)
+        pts_coors[:, 2] = pts_coors[:, 2].clip(min=0, max=x_max)
 
 
-        new_h = torch.linspace(-1, 1, out_h).view(1,out_h,1).expand(out_z,out_h,out_w)
-        new_w = torch.linspace(-1, 1, out_w).view(1,1,out_w).expand(out_z,out_h,out_w)
-        new_z = torch.linspace(-1, 1, out_z).view(out_z,1,1).expand(out_z,out_h,out_w)
+        new_h = torch.linspace(-1, 1, out_h).view(1, out_h, 1).expand(out_z, out_h, out_w)
+        new_w = torch.linspace(-1, 1, out_w).view(1, 1, out_w).expand(out_z, out_h, out_w)
+        new_z = torch.linspace(-1, 1, out_z).view(out_z, 1, 1).expand(out_z, out_h, out_w)
 
-        grid = torch.cat((new_w.unsqueeze(3),new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
+        grid = torch.cat((new_w.unsqueeze(3), new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
 
         grid = grid.unsqueeze(0).to(occupancy.device)
 
+        torch.npu.set_compile_mode(jit_compile=True)
         out_logit = F.grid_sample(occupancy, grid=grid)
+        torch.npu.set_compile_mode(jit_compile=False)
 
-        pts_pred = out_logit[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_pred = out_logit[:, :, pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
         return pts_pred
 
-    def decode_occupancy(self,points,occupancy):
-        out_h = 400
-        out_w = 400
-        out_z  = 64
-        self.voxel_lidar = [102.4/out_h,102.4/out_w,8/out_z]
-
-        pts_coors,voxelized_data,voxel_coors = self.voxelize(points,self.pc_range,self.voxel_lidar)
-
-
-        # clip out-ranged points
-        z_max = int((self.pc_range[5]-self.pc_range[2])/self.voxel_lidar[2])-1
-        y_max = int((self.pc_range[4]-self.pc_range[1])/self.voxel_lidar[1])-1
-        x_max = int((self.pc_range[3]-self.pc_range[0])/self.voxel_lidar[0])-1
-        pts_coors[:,1] = pts_coors[:,1].clip(min=0,max=z_max)
-        pts_coors[:,2] = pts_coors[:,2].clip(min=0,max=y_max)
-        pts_coors[:,3] = pts_coors[:,3].clip(min=0,max=x_max)
-
-
-        new_h = torch.linspace(-1, 1, out_h).view(1,out_h,1).expand(out_z,out_h,out_w)
-        new_w = torch.linspace(-1, 1, out_w).view(1,1,out_w).expand(out_z,out_h,out_w)
-        new_z = torch.linspace(-1, 1, out_z).view(out_z,1,1).expand(out_z,out_h,out_w)
-
-        grid = torch.cat((new_w.unsqueeze(3),new_h.unsqueeze(3), new_z.unsqueeze(3)), dim=-1)
-
-        grid = grid.unsqueeze(0).to(occupancy.device)
-
-        out_logit = F.grid_sample(occupancy, grid=grid)
-
-        # Occupancy Visualize
-        out_class = out_logit.sigmoid()>0.2
-        all_index = out_class.sum(dim=1).nonzero()
-
-        out_voxel = out_logit[:,:,all_index[:,1],all_index[:,2],all_index[:,3]]
-        out_voxel_scores = out_voxel.sigmoid()
-        out_voxel_confidence,out_voxel_labels = out_voxel_scores.max(dim=1)
-        output_occupancy = torch.cat((all_index.unsqueeze(0),out_voxel_labels.unsqueeze(-1)),dim=-1).cpu().numpy()[...,1:]
-
-        return output_occupancy
-
     def decode_lidar_seg_dense(self, dense, occupancy):
         dense  = dense.long()
-        pts_pred = occupancy[:,:,dense[0,:,0],dense[0,:,1],dense[0,:,2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
+        pts_pred = occupancy[:, :, dense[0, :, 0], dense[0, :, 1], dense[0, :, 2]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu().numpy()
         return pts_pred
-
-    @torch.no_grad()
-    def label_voxelization(self, pts_semantic_mask, pts_coors, voxel_coors):
-        mask = pts_semantic_mask
-        assert mask.size(0) == pts_coors.size(0)
-
-        pts_coors_cls = torch.cat([pts_coors, mask], dim=1) #[N, 5]
-        unq_coors_cls, unq_inv, unq_cnt = torch.unique(pts_coors_cls, return_inverse=True, return_counts=True, dim=0) #[N1, 5], [N], [N1]
-
-        unq_coors, unq_inv_2, _ = torch.unique(unq_coors_cls[:, :4], return_inverse=True, return_counts=True, dim=0) #[N2, 4], [N1], [N2,]
-        max_num, max_inds = torch_scatter.scatter_max(unq_cnt.float()[:,None], unq_inv_2, dim=0) #[N2, 1], [N2, 1]
-
-        cls_of_max_num = unq_coors_cls[:, -1][max_inds.reshape(-1)] #[N2,]
-        cls_of_max_num_N1 = cls_of_max_num[unq_inv_2] #[N1]
-        cls_of_max_num_at_pts = cls_of_max_num_N1[unq_inv] #[N]
-
-        assert cls_of_max_num_at_pts.size(0) == mask.size(0)
-
-        cls_no_change = cls_of_max_num_at_pts == mask[:,0] # fix memory bug when scale up
-        # cls_no_change = cls_of_max_num_at_pts == mask
-        assert cls_no_change.any()
-
-        max_pts_coors = pts_coors.max(0)[0]
-        max_voxel_coors = voxel_coors.max(0)[0]
-        assert (max_voxel_coors <= max_pts_coors).all()
-        bsz, num_win_z, num_win_y, num_win_x = \
-        int(max_pts_coors[0].item() + 1), int(max_pts_coors[1].item() + 1), int(max_pts_coors[2].item() + 1), int(max_pts_coors[3].item() + 1)
-
-        canvas = -pts_coors.new_ones((bsz, num_win_z, num_win_y, num_win_x))
-
-        canvas[pts_coors[:, 0], pts_coors[:, 1], pts_coors[:, 2], pts_coors[:, 3]] = \
-            torch.arange(pts_coors.size(0), dtype=pts_coors.dtype, device=pts_coors.device)
-
-        fetch_inds_of_points = canvas[voxel_coors[:, 0], voxel_coors[:, 1], voxel_coors[:, 2], voxel_coors[:, 3]]
-
-        assert (fetch_inds_of_points >= 0).all(), '-1 should not be in it.'
-
-        voxel_label = cls_of_max_num_at_pts[fetch_inds_of_points]
-
-        voxel_label = torch.clamp(voxel_label,min=0).long()
-
-        return voxel_label
-
-    @torch.no_grad()
-    def get_point_pred(self,occupancy,pts_coors,voxel_coors,voxel_label,pts_semantic_mask):
-
-        voxel_pred = occupancy[:,:,voxel_coors[:,1],voxel_coors[:,2],voxel_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-
-        voxel_gt = voxel_label.long().cpu()
-
-        accurate = voxel_pred==voxel_gt
-
-        acc = accurate.sum()/len(voxel_gt)
-
-        pts_pred = occupancy[:,:,pts_coors[:,1],pts_coors[:,2],pts_coors[:,3]].squeeze(0).softmax(dim=0).argmax(dim=0).cpu()
-        pts_gt  = pts_semantic_mask.long().squeeze(1).cpu()
-
-        pts_accurate = pts_pred==pts_gt
-        pts_acc = pts_accurate.sum()/len(pts_gt)
-
-        return pts_acc
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/__init__.py b/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
index 1012ef3..bf7f763 100644
--- a/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/detectors/__init__.py
@@ -1,3 +1,2 @@
 from .pano_occ import PanoOcc
-from .panoseg_occ import PanoSegOcc
-from .panoseg_occ_sparse import PanoSegOccSparse
\ No newline at end of file
+from .panoseg_occ import PanoSegOcc
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py b/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
index 46a8b99..4d8cc21 100644
--- a/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/detectors/pano_occ.py
@@ -128,10 +128,6 @@ class PanoOcc(MVXTwoStageDetector):
         losses = self.pts_bbox_head.loss(*loss_inputs, img_metas=img_metas)
         return losses
 
-    def forward_dummy(self, img):
-        dummy_metas = None
-        return self.forward_test(img=img, img_metas=[[dummy_metas]])
-
     def forward(self, return_loss=True, **kwargs):
         """Calls either forward_train or forward_test depending on whether
         return_loss=True.
diff --git a/projects/mmdet3d_plugin/bevformer/modules/__init__.py b/projects/mmdet3d_plugin/bevformer/modules/__init__.py
index 17ded68..f880296 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/__init__.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/__init__.py
@@ -7,12 +7,10 @@ from .decoder import DetectionTransformerDecoder
 from .occ_temporal_attention import OccTemporalAttention
 from .occ_spatial_attention import OccSpatialAttention
 from .occ_decoder import OccupancyDecoder
-from .occ_mlp_decoder import MLP_Decoder, SparseMLPDecoder
+from .occ_mlp_decoder import MLP_Decoder
 from .occ_temporal_encoder import OccTemporalEncoder
 from .transformer_occ import TransformerOcc
 from .occ_voxel_decoder import VoxelDecoder
 from .pano_transformer_occ import PanoOccTransformer
 from .panoseg_transformer_occ import PanoSegOccTransformer
-from .occ_voxel_seg_decoder import VoxelNaiveDecoder
-from .sparse_occ_decoder import SparseOccupancyDecoder
-from .sparse_occ_transformer import SparseOccupancyTransformer
\ No newline at end of file
+from .occ_voxel_seg_decoder import VoxelNaiveDecoder
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/modules/decoder.py b/projects/mmdet3d_plugin/bevformer/modules/decoder.py
index 33024f8..21f8b51 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/decoder.py
@@ -23,12 +23,7 @@ from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
 from mmcv.utils import (ConfigDict, build_from_cfg, deprecated_api_warning,
                         to_2tuple)
 
-from mmcv.utils import ext_loader
-from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
-    MultiScaleDeformableAttnFunction_fp16
-
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+from mx_driving import multi_scale_deformable_attn
 
 
 def inverse_sigmoid(x, eps=1e-5):
@@ -323,15 +318,8 @@ class CustomMSDeformableAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = multi_scale_deformable_attn(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py b/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
index 15058e4..e4caa64 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_decoder.py
@@ -4,6 +4,23 @@ from mmcv.cnn.bricks.registry import TRANSFORMER_LAYER_SEQUENCE
 import torch.nn.functional as F
 
 
+def interpolate_trilinear(x, scale_factor, mode, align_corners):
+    # assert mode == 'trilinear'
+    # assert align_corners == False
+    # bilinear + bilinear
+    scale_t, scale_h, scale_w = scale_factor
+    N, C, T, H, W = x.size(0), x.size(1), x.size(2), x.size(3), x.size(4)
+
+    x_fused_nc = x.reshape(N*C, T, H, W)
+    y_resize_hw = F.interpolate(x_fused_nc, scale_factor=(scale_h, scale_w), mode='bilinear')
+    new_shape_h, new_shape_w = y_resize_hw.shape[-2], y_resize_hw.shape[-1]
+    y_fused_hw = y_resize_hw.reshape(N, C, T, new_shape_h*new_shape_w)
+    y_resize_t = F.interpolate(y_fused_hw, scale_factor=(scale_t, 1), mode='bilinear')
+    new_shape_t = y_resize_t.shape[-2]
+    y = y_resize_t.reshape(N, C, new_shape_t, new_shape_h, new_shape_w)
+    return y
+
+
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class OccupancyDecoder(BaseModule):
 
@@ -66,6 +83,8 @@ class OccupancyDecoder(BaseModule):
 
         voxel_cls = self.semantic_cls(voxel_up1)
 
-        voxel_pred = F.interpolate(voxel_cls,scale_factor=(self.inter_up_rate[0],self.inter_up_rate[1],self.inter_up_rate[2]),mode=self.upsampling_method,align_corners=self.align_corners)
+        voxel_pred = interpolate_trilinear(voxel_cls, 
+                                           scale_factor=(self.inter_up_rate[0], self.inter_up_rate[1], self.inter_up_rate[2]), 
+                                           mode=self.upsampling_method, align_corners=self.align_corners)
 
         return voxel_pred, voxel_det
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py b/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
index 615e26b..952fdce 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_mlp_decoder.py
@@ -4,6 +4,24 @@ from mmcv.cnn.bricks.registry import TRANSFORMER_LAYER_SEQUENCE
 import torch.nn.functional as F
 import torch
 
+
+def interpolate_trilinear(x, scale_factor, mode, align_corners):
+    # assert mode == 'trilinear'
+    # assert align_corners == False
+    # bilinear + bilinear
+    scale_t, scale_h, scale_w = scale_factor
+    N, C, T, H, W = x.size(0), x.size(1), x.size(2), x.size(3), x.size(4)
+
+    x_fused_nc = x.reshape(N*C, T, H, W)
+    y_resize_hw = F.interpolate(x_fused_nc, scale_factor=(scale_h, scale_w), mode='bilinear')
+    new_shape_h, new_shape_w = y_resize_hw.shape[-2], y_resize_hw.shape[-1]
+    y_fused_hw = y_resize_hw.reshape(N, C, T, new_shape_h*new_shape_w)
+    y_resize_t = F.interpolate(y_fused_hw, scale_factor=(scale_t, 1), mode='bilinear')
+    new_shape_t = y_resize_t.shape[-2]
+    y = y_resize_t.reshape(N, C, new_shape_t, new_shape_h, new_shape_w)
+    return y
+
+
 @TRANSFORMER_LAYER_SEQUENCE.register_module()
 class MLP_Decoder(BaseModule):
 
@@ -32,7 +50,9 @@ class MLP_Decoder(BaseModule):
 
         voxel_point_cls = point_cls.view(1,inputs.shape[2],inputs.shape[3],inputs.shape[4],-1).permute(0,4,1,2,3)
 
-        voxel_logits = F.interpolate(voxel_point_cls,scale_factor=(self.inter_up_rate[0],self.inter_up_rate[1],self.inter_up_rate[2]),mode=self.upsampling_method,align_corners=self.align_corners)
+        voxel_logits = interpolate_trilinear(voxel_point_cls, 
+                                             scale_factor=(self.inter_up_rate[0], self.inter_up_rate[1], self.inter_up_rate[2]), 
+                                             mode=self.upsampling_method, align_corners=self.align_corners)
         
         return voxel_logits
 
diff --git a/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py b/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
index 8f62f3a..8236e93 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/occ_temporal_attention.py
@@ -7,6 +7,7 @@
 from projects.mmdet3d_plugin.models.utils.bricks import run_time
 from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32
 from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
+from mx_driving import multi_scale_deformable_attn
 import warnings
 import torch
 import torch.nn as nn
@@ -243,15 +244,8 @@ class OccTemporalAttention(BaseModule):
 
         sampling_locations = sampling_locations.contiguous()
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = multi_scale_deformable_attn(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
 
             output = multi_scale_deformable_attn_pytorch(
diff --git a/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py b/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
index be6c6ed..5604546 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/panoseg_transformer_occ.py
@@ -206,10 +206,12 @@ class PanoSegOccTransformer(BaseModule):
 
                 curr_grid_in_prev_frame = torch.stack(curr_grid_in_prev_frame_lst, dim=0)
 
+                torch.npu.set_compile_mode(jit_compile=True)
                 prev_bev_warp_to_curr_frame = nn.functional.grid_sample(
                     prev_bev[i].permute(0, 1, 4, 2, 3),  # [bs, dim, z, h, w]
                     curr_grid_in_prev_frame.permute(0, 3, 1, 2, 4),  # [bs, z, h, w, 3]
                     align_corners=False)
+                torch.npu.set_compile_mode(jit_compile=False)
                 prev_bev = prev_bev_warp_to_curr_frame.permute(0, 1, 3, 4, 2).unsqueeze(0) # add bs dim, [bs, dim, h, w, z]
 
             return prev_bev
diff --git a/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py b/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
index b53b66c..dfb7e1f 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/spatial_cross_attention.py
@@ -5,27 +5,27 @@
 #  Modified by Zhiqi Li
 # ---------------------------------------------
 
-from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
+import math
 import warnings
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-from mmcv.cnn import xavier_init, constant_init
-from mmcv.cnn.bricks.registry import (ATTENTION,
-                                      TRANSFORMER_LAYER,
-                                      TRANSFORMER_LAYER_SEQUENCE)
+from mmcv.cnn import constant_init, xavier_init
+from mmcv.cnn.bricks.registry import ATTENTION, TRANSFORMER_LAYER, TRANSFORMER_LAYER_SEQUENCE
 from mmcv.cnn.bricks.transformer import build_attention
-import math
-from mmcv.runner import force_fp32, auto_fp16
-
+from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
+from mmcv.runner import force_fp32
 from mmcv.runner.base_module import BaseModule, ModuleList, Sequential
-
-from mmcv.utils import ext_loader
-from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32, \
-    MultiScaleDeformableAttnFunction_fp16
 from projects.mmdet3d_plugin.models.utils.bricks import run_time
-ext_module = ext_loader.load_ext(
-    '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
+
+from mx_driving import multi_scale_deformable_attn
+
+
+indexes_global = None
+max_len_global = None
+bev_mask_id_global = -1
+count_global = None
 
 
 @ATTENTION.register_module()
@@ -135,10 +135,27 @@ class SpatialCrossAttention(BaseModule):
         # bevformer reference_points_cam shape: (num_cam,bs,h*w,num_points_in_pillar,2)
         D = reference_points_cam.size(3)
         indexes = []
-        for i, mask_per_img in enumerate(bev_mask):
-            index_query_per_img = mask_per_img[0].sum(-1).nonzero().squeeze(-1)
-            indexes.append(index_query_per_img)
-        max_len = max([len(each) for each in indexes])
+        global indexes_global, max_len_global, bev_mask_id_global, count_global
+        bev_mask_id = id(bev_mask)
+        if bev_mask_id == bev_mask_id_global:
+            indexes = indexes_global
+            max_len = max_len_global
+            count = count_global
+        else:
+            count = torch.any(bev_mask, 3)
+            bev_mask_ = count.squeeze()
+            for i, mask_per_img in enumerate(bev_mask_):
+                index_query_per_img = mask_per_img.nonzero().squeeze(-1)
+                indexes.append(index_query_per_img)
+
+            max_len = max([len(each) for each in indexes])
+            count = count.permute(1, 2, 0).sum(-1)
+            count = torch.clamp(count, min=1.0)
+            count = count[..., None]
+            count_global = count
+            indexes_global = indexes
+            max_len_global = max_len
+            bev_mask_id_global = bev_mask_id
 
         # each camera only interacts with its corresponding BEV queries. This step can  greatly save GPU memory.
         queries_rebatch = query.new_zeros(
@@ -146,9 +163,9 @@ class SpatialCrossAttention(BaseModule):
         reference_points_rebatch = reference_points_cam.new_zeros(
             [bs, self.num_cams, max_len, D, 2])
         
-        for j in range(bs):
-            for i, reference_points_per_img in enumerate(reference_points_cam):   
-                index_query_per_img = indexes[i]
+        for i, reference_points_per_img in enumerate(reference_points_cam):   
+            index_query_per_img = indexes[i]
+            for j in range(bs):
                 queries_rebatch[j, i, :len(index_query_per_img)] = query[j, index_query_per_img]
                 reference_points_rebatch[j, i, :len(index_query_per_img)] = reference_points_per_img[j, index_query_per_img]
 
@@ -159,17 +176,15 @@ class SpatialCrossAttention(BaseModule):
         value = value.permute(2, 0, 1, 3).reshape(
             bs * self.num_cams, l, self.embed_dims)
 
-        queries = self.deformable_attention(query=queries_rebatch.view(bs*self.num_cams, max_len, self.embed_dims), key=key, value=value,
-                                            reference_points=reference_points_rebatch.view(bs*self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
+        queries = self.deformable_attention(query=queries_rebatch.view(bs * self.num_cams, max_len, self.embed_dims), key=key, value=value,
+                                            reference_points=reference_points_rebatch.view(bs * self.num_cams, max_len, D, 2), spatial_shapes=spatial_shapes,
                                             level_start_index=level_start_index).view(bs, self.num_cams, max_len, self.embed_dims)
         for j in range(bs):
             for i, index_query_per_img in enumerate(indexes):
                 slots[j, index_query_per_img] += queries[j, i, :len(index_query_per_img)]
 
-        count = bev_mask.sum(-1) > 0
-        count = count.permute(1, 2, 0).sum(-1)
-        count = torch.clamp(count, min=1.0)
-        slots = slots / count[..., None]
+
+        slots = slots / count
         slots = self.output_proj(slots)
 
         return self.dropout(slots) + inp_residual
@@ -329,7 +344,7 @@ class MSDeformableAttention3D(BaseModule):
 
         bs, num_query, _ = query.shape
         bs, num_value, _ = value.shape
-        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
+        # assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value
 
         value = self.value_proj(value)
         if key_padding_mask is not None:
@@ -366,7 +381,7 @@ class MSDeformableAttention3D(BaseModule):
                 bs, num_query, num_heads, num_levels, num_all_points // num_Z_anchors, num_Z_anchors, xy)
             sampling_locations = reference_points + sampling_offsets
             bs, num_query, num_heads, num_levels, num_points, num_Z_anchors, xy = sampling_locations.shape
-            assert num_all_points == num_points * num_Z_anchors
+            # assert num_all_points == num_points * num_Z_anchors
 
             sampling_locations = sampling_locations.view(
                 bs, num_query, num_heads, num_levels, num_all_points, xy)
@@ -379,13 +394,8 @@ class MSDeformableAttention3D(BaseModule):
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
 
         if torch.cuda.is_available() and value.is_cuda:
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                                         sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
diff --git a/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py b/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
index 78fb9f5..61873c9 100644
--- a/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
+++ b/projects/mmdet3d_plugin/bevformer/modules/temporal_self_attention.py
@@ -7,6 +7,7 @@
 from projects.mmdet3d_plugin.models.utils.bricks import run_time
 from .multi_scale_deformable_attn_function import MultiScaleDeformableAttnFunction_fp32
 from mmcv.ops.multi_scale_deform_attn import multi_scale_deformable_attn_pytorch
+from mx_driving import multi_scale_deformable_attn
 import warnings
 import torch
 import torch.nn as nn
@@ -238,15 +239,8 @@ class TemporalSelfAttention(BaseModule):
                 f'Last dim of reference_points must be'
                 f' 2 or 4, but get {reference_points.shape[-1]} instead.')
         if torch.cuda.is_available() and value.is_cuda:
-
-            # using fp16 deformable attention is unstable because it performs many sum operations
-            if value.dtype == torch.float16:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            else:
-                MultiScaleDeformableAttnFunction = MultiScaleDeformableAttnFunction_fp32
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = multi_scale_deformable_attn(value, spatial_shapes, level_start_index, 
+                sampling_locations, attention_weights)
         else:
 
             output = multi_scale_deformable_attn_pytorch(
diff --git a/projects/mmdet3d_plugin/datasets/builder.py b/projects/mmdet3d_plugin/datasets/builder.py
index 0ad7a92..0625933 100644
--- a/projects/mmdet3d_plugin/datasets/builder.py
+++ b/projects/mmdet3d_plugin/datasets/builder.py
@@ -25,6 +25,7 @@ def build_dataloader(dataset,
                      seed=None,
                      shuffler_sampler=None,
                      nonshuffler_sampler=None,
+                     pin_memory=False,
                      **kwargs):
     """Build PyTorch DataLoader.
     In distributed training, each GPU/process has a dataloader.
@@ -86,7 +87,7 @@ def build_dataloader(dataset,
         sampler=sampler,
         num_workers=num_workers,
         collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),
-        pin_memory=False,
+        pin_memory=pin_memory,
         worker_init_fn=init_fn,
         **kwargs)
 
diff --git a/projects/mmdet3d_plugin/datasets/pipelines/compose.py b/projects/mmdet3d_plugin/datasets/pipelines/compose.py
index 08e46a8..6052444 100644
--- a/projects/mmdet3d_plugin/datasets/pipelines/compose.py
+++ b/projects/mmdet3d_plugin/datasets/pipelines/compose.py
@@ -3,6 +3,7 @@ import collections
 from mmcv.utils import build_from_cfg
 
 from mmdet.datasets.builder import PIPELINES
+from mmdet3d.datasets.builder import PIPELINES as PIPELINES_3d
 
 @PIPELINES.register_module()
 class CustomCompose:
@@ -16,7 +17,10 @@ class CustomCompose:
         self.transforms = []
         for transform in transforms:
             if isinstance(transform, dict):
-                transform = build_from_cfg(transform, PIPELINES)
+                if transform["type"] not in PIPELINES:
+                    transform = build_from_cfg(transform, PIPELINES_3d)
+                else:
+                    transform = build_from_cfg(transform, PIPELINES)
                 self.transforms.append(transform)
             elif callable(transform):
                 self.transforms.append(transform)
diff --git a/projects/mmdet3d_plugin/models/backbones/__init__.py b/projects/mmdet3d_plugin/models/backbones/__init__.py
index f86b114..cea72f5 100755
--- a/projects/mmdet3d_plugin/models/backbones/__init__.py
+++ b/projects/mmdet3d_plugin/models/backbones/__init__.py
@@ -1,5 +1,3 @@
 from .vovnet import VoVNet
-from .internv2_impl16 import InternV2Impl16
-from .sam_modeling import ImageEncoderViT
 
-__all__ = ['VoVNet', "InternV2Impl16", "ImageEncoderViT"]
\ No newline at end of file
+__all__ = ['VoVNet']
\ No newline at end of file
diff --git a/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py b/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py
index 50f3bd6..e69de29 100644
--- a/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py
+++ b/projects/mmdet3d_plugin/models/backbones/sam_modeling/__init__.py
@@ -1 +0,0 @@
-from .image_encoder import ImageEncoderViT
\ No newline at end of file
diff --git a/tools/dist_test.sh b/tools/dist_test.sh
index 3e2ec30..931aa0f 100755
--- a/tools/dist_test.sh
+++ b/tools/dist_test.sh
@@ -6,5 +6,5 @@ GPUS=$3
 PORT=${PORT:-29503}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
     $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4} --eval bbox
diff --git a/tools/dist_test_seg.sh b/tools/dist_test_seg.sh
index 7719313..0b4f6d2 100755
--- a/tools/dist_test_seg.sh
+++ b/tools/dist_test_seg.sh
@@ -6,5 +6,5 @@ GPUS=$3
 PORT=${PORT:-29503}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch --nproc_per_node=$GPUS --master_port=$PORT \
+torchrun --nproc_per_node=$GPUS --master_port=$PORT \
     $(dirname "$0")/test.py $CONFIG $CHECKPOINT --launcher pytorch ${@:4} --out 'seg_result.pkl'
diff --git a/tools/dist_train.sh b/tools/dist_train.sh
index cd9dd42..35e7be6 100755
--- a/tools/dist_train.sh
+++ b/tools/dist_train.sh
@@ -2,13 +2,14 @@
 #
 CONFIG=$1
 GPUS=$2
+WORK_DIR=$3
 NNODES=${NNODES:-1}
 NODE_RANK=${NODE_RANK:-0}
 PORT=${PORT:-29500}
 MASTER_ADDR=${MASTER_ADDR:-"127.0.0.1"}
 
 PYTHONPATH="$(dirname $0)/..":$PYTHONPATH \
-python -m torch.distributed.launch \
+torchrun \
     --nnodes=$NNODES \
     --node_rank=$NODE_RANK \
     --master_addr=$MASTER_ADDR \
@@ -17,4 +18,5 @@ python -m torch.distributed.launch \
     $(dirname "$0")/train.py \
     $CONFIG \
     --seed 0 \
-    --launcher pytorch ${@:3} --deterministic 2>&1 | tee output.log
\ No newline at end of file
+    --work-dir ${WORK_DIR} \
+    --launcher pytorch ${@:4} --deterministic
\ No newline at end of file
diff --git a/tools/test.py b/tools/test.py
index b7de8a6..ff93666 100755
--- a/tools/test.py
+++ b/tools/test.py
@@ -1,10 +1,15 @@
+# Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import argparse
 import mmcv
 import os
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import warnings
 from mmcv import Config, DictAction
 from mmcv.cnn import fuse_conv_bn
+from mmcv.device.npu import NPUDataParallel, NPUDistributedDataParallel
 from mmcv.parallel import MMDataParallel, MMDistributedDataParallel
 from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,
                          wrap_fp16_model)
@@ -20,6 +25,9 @@ import time
 import os.path as osp
 from tools.eval_metrics.lidar_seg import *
 
+torch.npu.config.allow_internal_format = False
+
+
 def parse_args():
     parser = argparse.ArgumentParser(
         description='MMDet test (and eval) a model')
@@ -225,11 +233,11 @@ def main():
         model.PALETTE = dataset.PALETTE
 
     if not distributed:
-        # assert False
-        model = MMDataParallel(model, device_ids=[0])
-        outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
+        assert False
+        # model = MMDataParallel(model, device_ids=[0])
+        # outputs = single_gpu_test(model, data_loader, args.show, args.show_dir)
     else:
-        model = MMDistributedDataParallel(
+        model = NPUDistributedDataParallel(
             model.cuda(),
             device_ids=[torch.cuda.current_device()],
             broadcast_buffers=False)
diff --git a/tools/train.py b/tools/train.py
index 390d37d..af6e918 100755
--- a/tools/train.py
+++ b/tools/train.py
@@ -6,6 +6,8 @@ import mmcv
 import os
 import time
 import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 import warnings
 from mmcv import Config, DictAction
 from mmcv.runner import get_dist_info, init_dist
@@ -22,6 +24,8 @@ from mmseg import __version__ as mmseg_version
 
 from mmcv.utils import TORCH_VERSION, digit_version
 
+torch.npu.config.allow_internal_format = False
+
 
 def parse_args():
     parser = argparse.ArgumentParser(description='Train a detector')
@@ -131,10 +135,6 @@ def main():
     # set cudnn_benchmark
     if cfg.get('cudnn_benchmark', False):
         torch.backends.cudnn.benchmark = True
-    # set tf32
-    if cfg.get('close_tf32', False):
-        torch.backends.cuda.matmul.allow_tf32 = False
-        torch.backends.cudnn.allow_tf32 = False
 
     # work_dir is determined in this priority: CLI > segment in file > filename
     if args.work_dir is not None:
