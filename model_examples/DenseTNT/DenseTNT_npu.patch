diff --git a/requirements.txt b/requirements.txt
index dbacf29..61e4efb 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,5 +1,4 @@
-torch>=1.6.0
 cython
 tqdm
 matplotlib
-scipy
\ No newline at end of file
+scipy
diff --git a/src/dataset_argoverse.py b/src/dataset_argoverse.py
index 456d615..cde5ae7 100644
--- a/src/dataset_argoverse.py
+++ b/src/dataset_argoverse.py
@@ -581,7 +581,7 @@ def argoverse2_get_instance(args: utils.Args, instance_dir):
             mapping['stage_one_label'] = stage_one_label
 
     # print(len(polyline_spans), len(vectors), map_start_polyline_idx, polyline_spans[map_start_polyline_idx])
-
+    vectors = utils.convert_to_float32(vectors)
     mapping.update(dict(
         matrix=np.array(vectors),
         labels=np.array(labels).reshape([args.future_frame_num, 2]),
diff --git a/src/modeling/lib.py b/src/modeling/lib.py
index 7f5e435..4f6e66a 100644
--- a/src/modeling/lib.py
+++ b/src/modeling/lib.py
@@ -32,7 +32,7 @@ class MLP(nn.Module):
         if out_features is None:
             out_features = hidden_size
         self.linear = nn.Linear(hidden_size, out_features)
-        self.layer_norm = LayerNorm(out_features)
+        self.layer_norm = torch.nn.LayerNorm(out_features)
 
     def forward(self, hidden_states):
         hidden_states = self.linear(hidden_states)
@@ -53,12 +53,11 @@ class GlobalGraph(nn.Module):
         self.num_attention_heads = num_attention_heads
         self.attention_head_size = hidden_size // num_attention_heads if attention_head_size is None else attention_head_size
         self.all_head_size = self.num_attention_heads * self.attention_head_size
+        self.sqrt_attention_head_size = math.sqrt(self.attention_head_size)
 
         self.num_qkv = 1
 
-        self.query = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
-        self.key = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
-        self.value = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
+        self.qkv = nn.Linear(hidden_size, 3 * self.all_head_size * self.num_qkv)
         if utils.args.attention_decay:
             self.attention_decay = nn.Parameter(torch.ones(1) * 0.5)
 
@@ -83,16 +82,15 @@ class GlobalGraph(nn.Module):
         return x.permute(0, 2, 1, 3)
 
     def forward(self, hidden_states, attention_mask=None, mapping=None, return_scores=False):
-        mixed_query_layer = self.query(hidden_states)
-        mixed_key_layer = nn.functional.linear(hidden_states, self.key.weight)
-        mixed_value_layer = self.value(hidden_states)
+        qkv = self.qkv(hidden_states)
+        query_layer, key_layer, value_layer = torch.chunk(qkv, 3, dim=-1)
 
-        query_layer = self.transpose_for_scores(mixed_query_layer)
-        key_layer = self.transpose_for_scores(mixed_key_layer)
-        value_layer = self.transpose_for_scores(mixed_value_layer)
+        query_layer = self.transpose_for_scores(query_layer)
+        key_layer = self.transpose_for_scores(key_layer)
+        value_layer = self.transpose_for_scores(value_layer)
 
         attention_scores = torch.matmul(
-            query_layer / math.sqrt(self.attention_head_size), key_layer.transpose(-1, -2))
+            query_layer / self.sqrt_attention_head_size, key_layer.transpose(-1, -2))
         # print(attention_scores.shape, attention_mask.shape)
         if attention_mask is not None:
             attention_scores = attention_scores + self.get_extended_attention_mask(attention_mask)
@@ -125,9 +123,14 @@ class CrossAttention(GlobalGraph):
         super(CrossAttention, self).__init__(hidden_size, attention_head_size, num_attention_heads)
         if query_hidden_size is not None:
             self.query = nn.Linear(query_hidden_size, self.all_head_size * self.num_qkv)
+        else:
+            self.query = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
         if key_hidden_size is not None:
             self.key = nn.Linear(key_hidden_size, self.all_head_size * self.num_qkv)
             self.value = nn.Linear(key_hidden_size, self.all_head_size * self.num_qkv)
+        else:
+            self.key = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
+            self.value = nn.Linear(hidden_size, self.all_head_size * self.num_qkv)
 
     def forward(self, hidden_states_query, hidden_states_key=None, attention_mask=None, mapping=None,
                 return_scores=False):
@@ -140,7 +143,7 @@ class CrossAttention(GlobalGraph):
         value_layer = self.transpose_for_scores(mixed_value_layer)
 
         attention_scores = torch.matmul(
-            query_layer / math.sqrt(self.attention_head_size), key_layer.transpose(-1, -2))
+            query_layer / self.sqrt_attention_head_size, key_layer.transpose(-1, -2))
         if attention_mask is not None:
             assert hidden_states_query.shape[1] == attention_mask.shape[1] \
                    and hidden_states_key.shape[1] == attention_mask.shape[2]
diff --git a/src/modeling/vectornet.py b/src/modeling/vectornet.py
index 502f936..794dd30 100644
--- a/src/modeling/vectornet.py
+++ b/src/modeling/vectornet.py
@@ -20,8 +20,8 @@ class NewSubGraph(nn.Module):
 
         self.layer_0 = MLP(hidden_size)
         self.layers = nn.ModuleList([GlobalGraph(hidden_size, num_attention_heads=2) for _ in range(depth)])
-        self.layers_2 = nn.ModuleList([LayerNorm(hidden_size) for _ in range(depth)])
-        self.layers_3 = nn.ModuleList([LayerNorm(hidden_size) for _ in range(depth)])
+        self.layers_2 = nn.ModuleList([torch.nn.LayerNorm(hidden_size) for _ in range(depth)])
+        self.layers_3 = nn.ModuleList([torch.nn.LayerNorm(hidden_size) for _ in range(depth)])
         self.layers_4 = nn.ModuleList([GlobalGraph(hidden_size) for _ in range(depth)])
         self.layer_0_again = MLP(hidden_size)
 
@@ -33,11 +33,18 @@ class NewSubGraph(nn.Module):
         max_vector_num = hidden_states.shape[1]
 
         attention_mask = torch.zeros([batch_size, max_vector_num, max_vector_num], device=device)
+        lengths_tensor = torch.tensor(lengths, device=device)
+        lengths_tensor = lengths_tensor.unsqueeze(1)
+        lengths_tensor = lengths_tensor.unsqueeze(1)
+        lengths_tensor = lengths_tensor.repeat(1, max_vector_num, max_vector_num)
+        range_tensor = torch.arange(max_vector_num, device=device)
+        range_tensor = range_tensor.unsqueeze(0)
+        range_tensor = range_tensor.unsqueeze(0)
+        range_tensor = range_tensor.repeat(batch_size, max_vector_num, 1)
+        mask = (range_tensor < lengths_tensor)
+        attention_mask.masked_fill_(mask, 1)
         hidden_states = self.layer_0(hidden_states)
         hidden_states = self.layer_0_again(hidden_states)
-        for i in range(batch_size):
-            assert lengths[i] > 0
-            attention_mask[i, :lengths[i], :lengths[i]].fill_(1)
 
         for layer_index, layer in enumerate(self.layers):
             temp = hidden_states
@@ -93,6 +100,7 @@ class VectorNet(nn.Module):
         :param polyline_spans: vectors of i_th element is matrix[polyline_spans[i]]
         :return: hidden states of all elements and hidden states of lanes
         """
+        preprocessed_matrix = [torch.tensor(mat, device=device) for mat in matrix]
         input_list_list = []
         # TODO(cyrushx): This is not used? Is it because input_list_list includes map data as well?
         # Yes, input_list_list includes map data, this will be used in the future release.
@@ -101,9 +109,11 @@ class VectorNet(nn.Module):
         for i in range(batch_size):
             input_list = []
             map_input_list = []
+            current_matrix = preprocessed_matrix[i]
             map_start_polyline_idx = mapping[i]['map_start_polyline_idx']
+            
             for j, polyline_span in enumerate(polyline_spans[i]):
-                tensor = torch.tensor(matrix[i][polyline_span], device=device)
+                tensor = current_matrix[polyline_span]
                 input_list.append(tensor)
                 if j >= map_start_polyline_idx:
                     map_input_list.append(tensor)
@@ -111,11 +121,10 @@ class VectorNet(nn.Module):
             input_list_list.append(input_list)
             map_input_list_list.append(map_input_list)
 
-        if True:
-            element_states_batch = []
-            for i in range(batch_size):
-                a, b = self.point_level_sub_graph(input_list_list[i])
-                element_states_batch.append(a)
+        element_states_batch = []
+        for i in range(batch_size):
+            a, b = self.point_level_sub_graph(input_list_list[i])
+            element_states_batch.append(a)
 
         if 'lane_scoring' in args.other_params:
             lane_states_batch = []
@@ -126,16 +135,12 @@ class VectorNet(nn.Module):
         # We follow laneGCN to fuse realtime traffic information from agent nodes to lane nodes.
         if 'laneGCN' in args.other_params:
             for i in range(batch_size):
-                map_start_polyline_idx = mapping[i]['map_start_polyline_idx']
-                agents = element_states_batch[i][:map_start_polyline_idx]
-                lanes = element_states_batch[i][map_start_polyline_idx:]
+                map_start = mapping[i]['map_start_polyline_idx']
+                agents = element_states_batch[i][:map_start]
+                lanes = element_states_batch[i][map_start:]
                 # Origin laneGCN contains three fusion layers. Here one fusion layer is enough.
-                if True:
-                    lanes = lanes + self.laneGCN_A2L(lanes.unsqueeze(0), torch.cat([lanes, agents[0:1]]).unsqueeze(0)).squeeze(0)
-                else:
-                    lanes = lanes + self.laneGCN_A2L(lanes.unsqueeze(0), agents.unsqueeze(0)).squeeze(0)
-                    lanes = lanes + self.laneGCN_L2L(lanes.unsqueeze(0)).squeeze(0)
-                    agents = agents + self.laneGCN_L2A(agents.unsqueeze(0), lanes.unsqueeze(0)).squeeze(0)
+                lanes = lanes + self.laneGCN_A2L(lanes.unsqueeze(0), 
+                                            torch.cat([lanes, agents[0:1]]).unsqueeze(0)).squeeze(0)
                 element_states_batch[i] = torch.cat([agents, lanes])
 
         return element_states_batch, lane_states_batch
diff --git a/src/run.py b/src/run.py
index 989fe1b..f4c92b6 100644
--- a/src/run.py
+++ b/src/run.py
@@ -14,6 +14,9 @@ from torch.utils.data import RandomSampler
 from torch.utils.data.distributed import DistributedSampler
 from tqdm import tqdm as tqdm_
 
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
 
 
@@ -209,6 +212,7 @@ def run_training_process(rank, world_size, args, queue):
         def setup(rank, world_size):
             os.environ['MASTER_ADDR'] = 'localhost'
             os.environ['MASTER_PORT'] = args.master_port
+            torch.npu.set_device(rank)
 
             # initialize the process group
             dist.init_process_group("nccl", rank=rank, world_size=world_size)
diff --git a/src/utils.py b/src/utils.py
index 4290911..6d8aa66 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -276,7 +276,6 @@ def init(args_: Args, logger_):
 
     if not args.do_eval and not args.debug and os.path.exists(args.output_dir):
         print('{} {} exists'.format(get_color_text('Warning!'), args.output_dir))
-        input()
 
     if args.do_eval:
         assert os.path.exists(args.output_dir)
@@ -373,6 +372,11 @@ def init(args_: Args, logger_):
 
     assert args.do_train or args.do_eval
 
+def convert_to_float32(item):
+    if isinstance(item, list):
+        return [convert_to_float32(sub_item) for sub_item in item]
+    else:
+        return np.float32(item)
 
 def add_eval_param(param):
     if param not in args.eval_params:
@@ -813,7 +817,13 @@ def merge_tensors(tensors: List[torch.Tensor], device, hidden_size=None) -> Tupl
 
 
 def de_merge_tensors(tensor: Tensor, lengths):
-    return [tensor[i, :lengths[i]] for i in range(len(lengths))]
+    lengths_tensor = torch.tensor(lengths, dtype=torch.long)
+    B = tensor.size(0)
+    N = tensor.size(1)
+    col_indices = torch.arange(N).expand(B, -1)
+    mask = col_indices < lengths_tensor.unsqueeze(1)
+    selected = tensor[mask]
+    return torch.split(selected, lengths, dim=0)
 
 
 def gather_tensors(tensor: torch.Tensor, indices: List[list]):
