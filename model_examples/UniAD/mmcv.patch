diff --git a/mmcv/ops/modulated_deform_conv.py b/mmcv/ops/modulated_deform_conv.py
index 8a348e83..dcb8c087 100644
--- a/mmcv/ops/modulated_deform_conv.py
+++ b/mmcv/ops/modulated_deform_conv.py
@@ -1,4 +1,5 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import math
 from typing import Optional, Tuple, Union
 
@@ -322,8 +323,9 @@ class ModulatedDeformConv2dPack(ModulatedDeformConv2d):
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore
         out = self.conv_offset(x)
-        o1, o2, mask = torch.chunk(out, 3, dim=1)
-        offset = torch.cat((o1, o2), dim=1)
+        len1 = ((out.shape[1] + 2) // 3) * 2
+        len2 = out.shape[1] - len1
+        offset, mask = torch.split(out, [len1, len2], dim=1)
         mask = torch.sigmoid(mask)
         return modulated_deform_conv2d(x, offset, mask, self.weight, self.bias,
                                        self.stride, self.padding,
@@ -422,4 +424,4 @@ if IS_MLU_AVAILABLE:
                 stride=self.stride,
                 padding=self.padding,
                 dilation=self.dilation,
-                mask=mask)
+                mask=mask)
\ No newline at end of file
diff --git a/mmcv/ops/multi_scale_deform_attn.py b/mmcv/ops/multi_scale_deform_attn.py
index 8c09cd2a..0107208f 100644
--- a/mmcv/ops/multi_scale_deform_attn.py
+++ b/mmcv/ops/multi_scale_deform_attn.py
@@ -1,4 +1,5 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import math
 import warnings
 from typing import Optional, no_type_check
@@ -15,6 +16,7 @@ from mmcv.cnn.bricks.registry import ATTENTION
 from mmcv.runner import BaseModule
 from mmcv.utils import IS_CUDA_AVAILABLE, IS_MLU_AVAILABLE, IS_NPU_AVAILABLE
 from ..utils import ext_loader
+import mx_driving.fused
 
 ext_module = ext_loader.load_ext(
     '_ext', ['ms_deform_attn_backward', 'ms_deform_attn_forward'])
@@ -363,9 +365,8 @@ class MultiScaleDeformableAttention(BaseModule):
         if ((IS_CUDA_AVAILABLE and value.is_cuda)
                 or (IS_MLU_AVAILABLE and value.is_mlu)
                 or (IS_NPU_AVAILABLE and value.device.type == 'npu')):
-            output = MultiScaleDeformableAttnFunction.apply(
-                value, spatial_shapes, level_start_index, sampling_locations,
-                attention_weights, self.im2col_step)
+            output = mx_driving.fused.multi_scale_deformable_attn(value, spatial_shapes, level_start_index,
+                                                                         sampling_locations, attention_weights)
         else:
             output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
@@ -376,4 +377,4 @@ class MultiScaleDeformableAttention(BaseModule):
             # (num_query, bs ,embed_dims)
             output = output.permute(1, 0, 2)
 
-        return self.dropout(output) + identity
+        return self.dropout(output) + identity
\ No newline at end of file
diff --git a/mmcv/parallel/distributed.py b/mmcv/parallel/distributed.py
index bf34cb59..f0dfecc9 100644
--- a/mmcv/parallel/distributed.py
+++ b/mmcv/parallel/distributed.py
@@ -156,8 +156,7 @@ class MMDistributedDataParallel(DistributedDataParallel):
         Returns:
             Any: Forward result of :attr:`module`.
         """
-        module_to_run = self._replicated_tensor_module if \
-            self._use_replicated_tensor_module else self.module
+        module_to_run = self.module
 
         if self.device_ids:
             inputs, kwargs = self.to_kwargs(  # type: ignore
diff --git a/mmcv/runner/dist_utils.py b/mmcv/runner/dist_utils.py
index c061b3c1..656cd069 100644
--- a/mmcv/runner/dist_utils.py
+++ b/mmcv/runner/dist_utils.py
@@ -36,7 +36,7 @@ def _is_free_port(port: int) -> bool:
 
 def init_dist(launcher: str, backend: str = 'nccl', **kwargs) -> None:
     if mp.get_start_method(allow_none=True) is None:
-        mp.set_start_method('spawn')
+        mp.set_start_method('fork')
     if launcher == 'pytorch':
         _init_dist_pytorch(backend, **kwargs)
     elif launcher == 'mpi':
diff --git a/mmcv/runner/hooks/optimizer.py b/mmcv/runner/hooks/optimizer.py
index 93015475..8bd1722f 100644
--- a/mmcv/runner/hooks/optimizer.py
+++ b/mmcv/runner/hooks/optimizer.py
@@ -1,4 +1,5 @@
 # Copyright (c) OpenMMLab. All rights reserved.
+# Copyright 2024 Huawei Technologies Co., Ltd
 import copy
 import logging
 from collections import defaultdict
@@ -52,11 +53,11 @@ class OptimizerHook(Hook):
         self.grad_clip = grad_clip
         self.detect_anomalous_params = detect_anomalous_params
 
-    def clip_grads(self, params):
+    def clip_grads(self, params, runner):
         params = list(
             filter(lambda p: p.requires_grad and p.grad is not None, params))
         if len(params) > 0:
-            return clip_grad.clip_grad_norm_(params, **self.grad_clip)
+            return runner.optimizer.clip_grad_norm_fused_(**self.grad_clip)
 
     def after_train_iter(self, runner):
         runner.optimizer.zero_grad()
@@ -65,7 +66,7 @@ class OptimizerHook(Hook):
         runner.outputs['loss'].backward()
 
         if self.grad_clip is not None:
-            grad_norm = self.clip_grads(runner.model.parameters())
+            grad_norm = self.clip_grads(runner.model.parameters(), runner)
             if grad_norm is not None:
                 # Add grad norm to the logger
                 runner.log_buffer.update({'grad_norm': float(grad_norm)},
@@ -182,7 +183,7 @@ class GradientCumulativeOptimizerHook(OptimizerHook):
                 or self.is_last_iter(runner)):
 
             if self.grad_clip is not None:
-                grad_norm = self.clip_grads(runner.model.parameters())
+                grad_norm = self.clip_grads(runner.model.parameters(), runner)
                 if grad_norm is not None:
                     # Add grad norm to the logger
                     runner.log_buffer.update({'grad_norm': float(grad_norm)},
@@ -291,7 +292,7 @@ if (TORCH_VERSION != 'parrots'
             self.loss_scaler.unscale_(runner.optimizer)
             # grad clip
             if self.grad_clip is not None:
-                grad_norm = self.clip_grads(runner.model.parameters())
+                grad_norm = self.clip_grads(runner.model.parameters(), runner)
                 if grad_norm is not None:
                     # Add grad norm to the logger
                     runner.log_buffer.update({'grad_norm': float(grad_norm)},
@@ -331,7 +332,7 @@ if (TORCH_VERSION != 'parrots'
                 self.loss_scaler.unscale_(runner.optimizer)
 
                 if self.grad_clip is not None:
-                    grad_norm = self.clip_grads(runner.model.parameters())
+                    grad_norm = self.clip_grads(runner.model.parameters(), runner)
                     if grad_norm is not None:
                         # Add grad norm to the logger
                         runner.log_buffer.update(
@@ -477,7 +478,7 @@ else:
                     if param.grad is not None:
                         param.grad.div_(self.loss_scaler.loss_scale)
                 if self.grad_clip is not None:
-                    grad_norm = self.clip_grads(fp32_weights)
+                    grad_norm = self.clip_grads(fp32_weights, runner)
                     if grad_norm is not None:
                         # Add grad norm to the logger
                         runner.log_buffer.update(
@@ -534,7 +535,7 @@ else:
                         if param.grad is not None:
                             param.grad.div_(self.loss_scaler.loss_scale)
                     if self.grad_clip is not None:
-                        grad_norm = self.clip_grads(fp32_weights)
+                        grad_norm = self.clip_grads(fp32_weights, runner)
                         if grad_norm is not None:
                             # Add grad norm to the logger
                             runner.log_buffer.update(
@@ -557,4 +558,4 @@ else:
 
                 # clear grads
                 runner.model.zero_grad()
-                runner.optimizer.zero_grad()
+                runner.optimizer.zero_grad()
\ No newline at end of file
diff --git a/requirements/runtime.txt b/requirements/runtime.txt
index 66e90d67..ac9275d1 100644
--- a/requirements/runtime.txt
+++ b/requirements/runtime.txt
@@ -1,7 +1,7 @@
 addict
-numpy
+numpy==1.22.0
 packaging
 Pillow
 pyyaml
 regex;sys_platform=='win32'
-yapf
+yapf
\ No newline at end of file
