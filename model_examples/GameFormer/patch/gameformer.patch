diff --git a/interaction_prediction/data_process.py b/interaction_prediction/data_process.py
index 87feff8..bc506ab 100644
--- a/interaction_prediction/data_process.py
+++ b/interaction_prediction/data_process.py
@@ -204,7 +204,7 @@ class DataProcess(object):
                         break             
 
             # scale the lane
-            vectorized_map[i] = cache_lane[np.linspace(0, added_points, num=300, endpoint=False, dtype=np.int)]
+            vectorized_map[i] = cache_lane[np.linspace(0, added_points, num=300, endpoint=False, dtype=np.int32)]
           
             # count
             added_lanes += 1
@@ -218,7 +218,7 @@ class DataProcess(object):
         for _, crosswalk in self.crosswalks.items():
             polygon = Polygon([(point.x, point.y) for point in crosswalk.polygon])
             polyline = polygon_completion(crosswalk.polygon)
-            polyline = polyline[np.linspace(0, polyline.shape[0], num=100, endpoint=False, dtype=np.int)]
+            polyline = polyline[np.linspace(0, polyline.shape[0], num=100, endpoint=False, dtype=np.int32)]
 
             if detection.intersects(polygon):
                 vectorized_crosswalks[added_cross_walks, :polyline.shape[0]] = polyline
diff --git a/interaction_prediction/train.py b/interaction_prediction/train.py
index c2d7f0d..5723bbf 100644
--- a/interaction_prediction/train.py
+++ b/interaction_prediction/train.py
@@ -1,5 +1,6 @@
 import torch
 import sys
+
 sys.path.append('..')
 import csv
 import argparse
@@ -13,18 +14,23 @@ import time
 from model.GameFormer import GameFormer
 from utils.inter_pred_utils import *
 
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+from torch_npu.optim import NpuFusedAdam
+
 
 # define model training epoch
 def training_epoch(train_data, model, optimizer, epoch):
     epoch_loss = []
     model.train()
-    current = 0
+    current, step = 0, 0
     start_time = time.time()
     size = len(train_data)
     ADE,FDE = [],[]
     ADEp,FDEp = [],[]
+    avg_train_times = []
 
-    for batch in train_data:
+    for step, batch in enumerate(train_data):
         # prepare data
         inputs = {
             'ego_state': batch[0].to(args.local_rank),
@@ -55,6 +61,9 @@ def training_epoch(train_data, model, optimizer, epoch):
         # compute metrics
         current += args.batch_size
         epoch_loss.append(loss.item())
+        avg_train_times.append((time.time() - start_time) / args.batch_size)
+        start_time = time.time()
+        avg_train_time = np.mean(avg_train_times[-10:])
 
         if dist.get_rank() == 0:
             logging.info(
@@ -62,10 +71,14 @@ def training_epoch(train_data, model, optimizer, epoch):
                 f"|Loss: {np.mean(epoch_loss):>.4f}|"+
                 f"Pred-1:ADE{np.mean(ADE):>.4f}-FDE{np.mean(FDE):>.4f}|"+
                 f"Pred-2:ADE{np.mean(ADEp):>.4f}-FDE{np.mean(FDEp):>.4f}|"+
-                f"{(time.time()-start_time)/current:>.4f}s/sample"
+                f"{avg_train_time:>.5f}s/sample"
                 )
+    if len(avg_train_times) != 0: 
+        model_performance = np.mean(avg_train_times[-50:]) * args.batch_size
+    else:
+        model_performance = None
     
-    return epoch_loss
+    return epoch_loss, model_performance
 
 # define model validation epoch
 def validation_epoch(valid_data, model, epoch):
@@ -133,16 +146,34 @@ def validation_epoch(valid_data, model, epoch):
         current += args.batch_size
         if dist.get_rank() == 0:
             logging.info(
-                f"\rTrain Progress: [{current:>6d}/{size*args.batch_size:>6d}]"+
+                f"\rValidation Progress: [{current:>6d}/{size*args.batch_size:>6d}]"+
                 f"|Loss: {np.mean(epoch_loss):>.4f}|"+
                 f"Pred-1:ADE{np.mean(ADE):>.4f}-FDE{np.mean(FDE):>.4f}|"+
                 f"Pred-2:ADE{np.mean(ADEp):>.4f}-FDE{np.mean(FDEp):>.4f}|"+
                 f"{(time.time()-start_time)/current:>.4f}s/sample"
-                )
+            )
+    
+    world_size = dist.get_world_size()
+    avg_ADE = torch.tensor(np.mean(ADE)).float().to(model.device) / world_size
+    avg_FDE = torch.tensor(np.mean(FDE)).float().to(model.device) / world_size
+    avg_ADEp = torch.tensor(np.mean(ADEp)).float().to(model.device) / world_size
+    avg_FDEp = torch.tensor(np.mean(FDEp)).float().to(model.device) / world_size
+    dist.all_reduce(avg_ADE, op=dist.ReduceOp.SUM)
+    dist.all_reduce(avg_FDE, op=dist.ReduceOp.SUM)
+    dist.all_reduce(avg_ADEp, op=dist.ReduceOp.SUM)
+    dist.all_reduce(avg_FDEp, op=dist.ReduceOp.SUM)
+    if dist.get_rank() == 0:
+        logging.info(
+            f"Gather Metrics from All Devices: "+
+            f"Pred-1:ADE{avg_ADE:>.4f}-FDE{avg_FDE:>.4f}|"+
+            f"Pred-2:ADE{avg_ADEp:>.4f}-FDE{avg_FDEp:>.4f}"
+        )
+    
         
     epoch_metrics = epoch_metrics.result()
-    
-    return epoch_metrics, epoch_loss
+    model_metric = avg_ADE
+
+    return epoch_metrics, epoch_loss, model_metric
 
 # Define model training process
 def main():
@@ -173,12 +204,11 @@ def main():
     model = DDP(model, device_ids=[local_rank], output_device=local_rank)
 
     # define optimizer and loss function
-    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
+    optimizer = NpuFusedAdam(model.parameters(), lr=args.learning_rate)
     scheduler = optim.lr_scheduler.MultiStepLR(
                                             optimizer, 
                                             milestones=[20, 22, 24, 26, 28], 
-                                            gamma=0.5,
-                                            verbose=True)
+                                            gamma=0.5)
     
     # load ckpts:
     curr_ep = 0
@@ -224,8 +254,8 @@ def main():
         train_data.sampler.set_epoch(epoch)
         valid_data.sampler.set_epoch(epoch)
 
-        train_loss = training_epoch(train_data, model, optimizer, epoch)
-        valid_metrics, val_loss = validation_epoch(valid_data, model, epoch)
+        train_loss, model_performance = training_epoch(train_data, model, optimizer, epoch)
+        valid_metrics, val_loss, model_metric = validation_epoch(valid_data, model, epoch)
 
         # save to training log
         log = {
@@ -235,6 +265,9 @@ def main():
             }
 
         log.update(valid_metrics)
+        if dist.get_rank() == 0:
+            logging.info(f"Model Performace (Seconds/Step): {model_performance}")
+            logging.info(f"Model Metric (ADE): {model_metric}")
 
         if dist.get_rank() == 0:
             # log & save
@@ -260,12 +293,12 @@ def main():
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(description='Interaction Prediction Training')
-    parser.add_argument("--local_rank", type=int)
+    parser.add_argument("--local-rank", type=int)
     # training
     parser.add_argument("--batch_size", type=int, help='training batch sizes', default=16)
     parser.add_argument("--training_epochs", type=int, help='training epochs', default=30)
     parser.add_argument("--learning_rate", type=float, help='training learning rates', default=1e-4)
-    parser.add_argument('--seed', type=int, help='fix random seed', default=3407)
+    parser.add_argument('--seed', type=int, help='fix random seed', default=2024)
     # data & loggings
     parser.add_argument('--name', type=str, help='log name (default: "Exp1")', default="Exp1_IP")
     parser.add_argument('--load_dir', type=str, help='name to load ckpts from log path (e.g. epochs_0.pth)', default='')
diff --git a/model/GameFormer.py b/model/GameFormer.py
index 17d6c8e..27c0790 100644
--- a/model/GameFormer.py
+++ b/model/GameFormer.py
@@ -1,6 +1,6 @@
 import torch
 from .modules import *
-
+from .npu_fused_modules import NpuFusedTransformerEncoderLayer
 
 class Encoder(nn.Module):
     def __init__(self, neighbors_to_predict, layers=6):
@@ -11,8 +11,8 @@ class Encoder(nn.Module):
         self.ego_encoder = AgentEncoder()
         self.lane_encoder = LaneEncoder()
         self.crosswalk_encoder = CrosswalkEncoder()
-        attention_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim*4,
-                                                     activation=F.gelu, dropout=dropout, batch_first=True)
+        attention_layer = NpuFusedTransformerEncoderLayer(d_model=dim, nhead=heads, dim_feedforward=dim*4,
+                                                          activation=F.gelu, dropout=dropout, batch_first=True)
         self.fusion_encoder = nn.TransformerEncoder(attention_layer, layers, enable_nested_tensor=False)
 
     def segment_map(self, map, map_encoding):
diff --git a/model/modules.py b/model/modules.py
index 300b8b6..ee44c92 100644
--- a/model/modules.py
+++ b/model/modules.py
@@ -2,7 +2,7 @@ import math
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
-
+from .npu_fused_modules import NpuFusedMultiheadAttention
 
 class PositionalEncoding(nn.Module):
     def __init__(self, max_len=100):
@@ -139,7 +139,7 @@ class SelfTransformer(nn.Module):
     def __init__(self):
         super(SelfTransformer, self).__init__()
         heads, dim, dropout = 8, 256, 0.1
-        self.self_attention = nn.MultiheadAttention(dim, heads, dropout, batch_first=True)
+        self.self_attention = NpuFusedMultiheadAttention(dim, heads, dropout, batch_first=True)
         self.norm_1 = nn.LayerNorm(dim)
         self.norm_2 = nn.LayerNorm(dim)
         self.ffn = nn.Sequential(nn.Linear(dim, dim*4), nn.GELU(), nn.Dropout(dropout), nn.Linear(dim*4, dim), nn.Dropout(dropout))
@@ -156,7 +156,7 @@ class CrossTransformer(nn.Module):
     def __init__(self):
         super(CrossTransformer, self).__init__()
         heads, dim, dropout = 8, 256, 0.1
-        self.cross_attention = nn.MultiheadAttention(dim, heads, dropout, batch_first=True)
+        self.cross_attention = NpuFusedMultiheadAttention(dim, heads, dropout, batch_first=True)
         self.norm_1 = nn.LayerNorm(dim)
         self.norm_2 = nn.LayerNorm(dim)
         self.ffn = nn.Sequential(nn.Linear(dim, dim*4), nn.GELU(), nn.Dropout(dropout), nn.Linear(dim*4, dim), nn.Dropout(dropout))
diff --git a/model/npu_fused_modules.py b/model/npu_fused_modules.py
new file mode 100644
index 0000000..4863060
--- /dev/null
+++ b/model/npu_fused_modules.py
@@ -0,0 +1,290 @@
+from typing import Optional, Tuple, List, Callable, Union
+import math
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch import Tensor
+import torch_npu
+
+
+class NpuFusedMultiheadAttention(nn.MultiheadAttention):
+    def forward(
+            self,
+            query: Tensor,
+            key: Tensor,
+            value: Tensor,
+            key_padding_mask: Optional[Tensor] = None,
+            need_weights: bool = True,
+            attn_mask: Optional[Tensor] = None,
+            average_attn_weights: bool = True,
+            is_causal : bool = False) -> Tuple[Tensor, Optional[Tensor]]:
+
+        if not self._qkv_same_embed_dim:
+            attn_output, attn_output_weights = fused_multi_head_attention_forward(
+                query, key, value, self.embed_dim, self.num_heads,
+                self.in_proj_weight, self.in_proj_bias,
+                self.bias_k, self.bias_v, self.add_zero_attn,
+                self.dropout, self.out_proj.weight, self.out_proj.bias,
+                training=self.training,
+                key_padding_mask=key_padding_mask, need_weights=need_weights,
+                attn_mask=attn_mask,
+                use_separate_proj_weight=True,
+                q_proj_weight=self.q_proj_weight, k_proj_weight=self.k_proj_weight,
+                v_proj_weight=self.v_proj_weight,
+                average_attn_weights=average_attn_weights,
+                is_causal=is_causal)
+        else:
+            attn_output, attn_output_weights = fused_multi_head_attention_forward(
+                query, key, value, self.embed_dim, self.num_heads,
+                self.in_proj_weight, self.in_proj_bias,
+                self.bias_k, self.bias_v, self.add_zero_attn,
+                self.dropout, self.out_proj.weight, self.out_proj.bias,
+                training=self.training,
+                key_padding_mask=key_padding_mask,
+                need_weights=need_weights,
+                attn_mask=attn_mask,
+                average_attn_weights=average_attn_weights,
+                is_causal=is_causal)
+            
+        return attn_output, attn_output_weights
+
+    
+def fused_multi_head_attention_forward(
+    query: Tensor,
+    key: Tensor,
+    value: Tensor,
+    embed_dim_to_check: int,
+    num_heads: int,
+    in_proj_weight: Optional[Tensor],
+    in_proj_bias: Optional[Tensor],
+    bias_k: Optional[Tensor],
+    bias_v: Optional[Tensor],
+    add_zero_attn: bool,
+    dropout_p: float,
+    out_proj_weight: Tensor,
+    out_proj_bias: Optional[Tensor],
+    training: bool = True,
+    key_padding_mask: Optional[Tensor] = None,
+    need_weights: bool = True,
+    attn_mask: Optional[Tensor] = None,
+    use_separate_proj_weight: bool = False,
+    q_proj_weight: Optional[Tensor] = None,
+    k_proj_weight: Optional[Tensor] = None,
+    v_proj_weight: Optional[Tensor] = None,
+    static_k: Optional[Tensor] = None,
+    static_v: Optional[Tensor] = None,
+    average_attn_weights: bool = True,
+    is_causal: bool = False,
+) -> Tuple[Tensor, Optional[Tensor]]:
+
+    from torch.overrides import handle_torch_function, has_torch_function
+    tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)
+    if has_torch_function(tens_ops):
+        return handle_torch_function(
+            fused_multi_head_attention_forward,
+            tens_ops,
+            query,
+            key,
+            value,
+            embed_dim_to_check,
+            num_heads,
+            in_proj_weight,
+            in_proj_bias,
+            bias_k,
+            bias_v,
+            add_zero_attn,
+            dropout_p,
+            out_proj_weight,
+            out_proj_bias,
+            training=training,
+            key_padding_mask=key_padding_mask,
+            need_weights=need_weights,
+            attn_mask=attn_mask,
+            is_causal=is_causal,
+            use_separate_proj_weight=use_separate_proj_weight,
+            q_proj_weight=q_proj_weight,
+            k_proj_weight=k_proj_weight,
+            v_proj_weight=v_proj_weight,
+            static_k=static_k,
+            static_v=static_v,
+            average_attn_weights=average_attn_weights,
+        )
+
+    # set up shape vars
+    bsz, tgt_len, embed_dim = query.shape
+    _, src_len, _ = key.shape
+
+    if is_causal and attn_mask is None:
+        raise RuntimeError(
+            "Need attn_mask if specifying the is_causal hint. "
+            "You may use the Transformer module method "
+            "`generate_square_subsequent_mask` to create this mask."
+        )
+
+    if isinstance(embed_dim, torch.Tensor):
+        # embed_dim can be a tensor when JIT tracing
+        head_dim = embed_dim.div(num_heads, rounding_mode='trunc')
+    else:
+        head_dim = embed_dim // num_heads
+
+    #
+    # compute in-projection
+    #
+    if not use_separate_proj_weight:
+        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
+    else:
+        if in_proj_bias is None:
+            b_q = b_k = b_v = None
+        else:
+            b_q, b_k, b_v = in_proj_bias.chunk(3)
+        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)
+
+    # prep attention mask
+
+    if attn_mask is not None:
+        # ensure attn_mask's dim is 3
+        if attn_mask.dim() == 2:
+            correct_2d_size = (tgt_len, src_len)
+            if attn_mask.shape != correct_2d_size:
+                raise RuntimeError(f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.")
+            attn_mask = attn_mask.unsqueeze(0)
+        elif attn_mask.dim() == 3:
+            correct_3d_size = (bsz * num_heads, tgt_len, src_len)
+            if attn_mask.shape != correct_3d_size:
+                raise RuntimeError(f"The shape of the 3D attn_mask is {attn_mask.shape}, but should be {correct_3d_size}.")
+        else:
+            raise RuntimeError(f"attn_mask's dimension {attn_mask.dim()} is not supported")
+
+    # add bias along batch dimension (currently second)
+    if bias_k is not None and bias_v is not None:
+        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
+        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
+        if attn_mask is not None:
+            attn_mask = torch._C._nn.pad(attn_mask, (0, 1))
+        if key_padding_mask is not None:
+            key_padding_mask = torch._C._nn.pad(key_padding_mask, (0, 1))
+
+    q = q.view(bsz, tgt_len, num_heads, head_dim)
+    k = k.view(bsz, src_len, num_heads, head_dim)
+    v = v.view(bsz, src_len, num_heads, head_dim)
+
+    # add zero attention along batch dimension (now first)
+    if add_zero_attn:
+        zero_attn_shape = (bsz * num_heads, 1, head_dim)
+        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
+        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
+        if attn_mask is not None:
+            attn_mask = torch._C._nn.pad(attn_mask, (0, 1))
+        if key_padding_mask is not None:
+            key_padding_mask = torch._C._nn.pad(key_padding_mask, (0, 1))
+
+    # merge key padding and attention masks
+    if key_padding_mask is not None:
+        key_padding_mask = key_padding_mask.view(bsz, 1, 1, src_len).   \
+            expand(-1, num_heads, -1, -1).reshape(bsz * num_heads, 1, src_len)
+        if attn_mask is None:
+            attn_mask = key_padding_mask
+        else:
+            attn_mask = attn_mask + key_padding_mask
+
+    # adjust dropout probability
+    if not training:
+        dropout_p = 0.0
+
+    if attn_mask is not None:
+        if attn_mask.size(0) == 1 and attn_mask.dim() == 3:
+            attn_mask = attn_mask.unsqueeze(0)
+        else:
+            attn_mask = attn_mask.view(bsz, num_heads, -1, src_len)
+
+        if attn_mask.shape[-2] == 1:
+            attn_mask = attn_mask.repeat([1, 1, tgt_len, 1])
+    
+    #使用NPU融合算子torch_npu.npu_fusion_attention
+    attn_output = torch_npu.npu_fusion_attention(q, k, v, head_num=num_heads, pse=None, atten_mask=attn_mask.bool(), 
+                                        input_layout="BSND", scale=1.0 / math.sqrt(q.shape[-1]), sparse_mode=1, 
+                                        keep_prob=1 - dropout_p)[0]
+
+    attn_output = attn_output.view(bsz, tgt_len, embed_dim)
+    attn_output = F.linear(attn_output, out_proj_weight, out_proj_bias)
+
+    return attn_output, None
+
+
+class NpuFusedTransformerEncoderLayer(nn.TransformerEncoderLayer):
+    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, 
+                dropout: float = 0.1, activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,
+                layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,
+                bias: bool = True, device=None, dtype=None) -> None:
+        super().__init__(d_model, nhead, dim_feedforward, dropout, activation, 
+                         layer_norm_eps, batch_first, norm_first, bias, device, dtype)
+        factory_kwargs = {'device': device, 'dtype': dtype}
+        self.self_attn = NpuFusedMultiheadAttention(d_model, nhead, dropout=dropout,
+                                            bias=bias, batch_first=batch_first,
+                                            **factory_kwargs)
+        
+    def forward(
+            self,
+            src: Tensor,
+            src_mask: Optional[Tensor] = None,
+            src_key_padding_mask: Optional[Tensor] = None,
+            is_causal: bool = False) -> Tensor:
+
+        x = src
+        if self.norm_first:
+            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)
+            x = x + self._ff_block(self.norm2(x))
+        else:
+            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))
+            x = self.norm2(x + self._ff_block(x))
+
+        return x
+    
+
+def _in_projection_packed(
+    q: Tensor,
+    k: Tensor,
+    v: Tensor,
+    w: Tensor,
+    b: Optional[Tensor] = None,
+) -> List[Tensor]:
+
+    E = q.size(-1)
+    if k is v:
+        if q is k:
+            # self-attention
+            proj = F.linear(q, w, b).split(E, -1)
+            return proj[0], proj[1], proj[2]
+        else:
+            # encoder-decoder attention
+            w_q, w_kv = w.split([E, E * 2])
+            if b is None:
+                b_q = b_kv = None
+            else:
+                b_q, b_kv = b.split([E, E * 2])
+            q_proj = F.linear(q, w_q, b_q)
+            kv_proj = F.linear(k, w_kv, b_kv).split(E, -1)
+            return (q_proj, kv_proj[0], kv_proj[1])
+    else:
+        w_q, w_k, w_v = w.chunk(3)
+        if b is None:
+            b_q = b_k = b_v = None
+        else:
+            b_q, b_k, b_v = b.chunk(3)
+        return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)
+    
+    
+def _in_projection(
+    q: Tensor,
+    k: Tensor,
+    v: Tensor,
+    w_q: Tensor,
+    w_k: Tensor,
+    w_v: Tensor,
+    b_q: Optional[Tensor] = None,
+    b_k: Optional[Tensor] = None,
+    b_v: Optional[Tensor] = None,
+) -> Tuple[Tensor, Tensor, Tensor]:
+    return F.linear(q, w_q, b_q), F.linear(k, w_k, b_k), F.linear(v, w_v, b_v)
+    
