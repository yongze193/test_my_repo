diff --git a/.gitignore b/.gitignore
index 71f7e7c..dd9b67b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -217,3 +217,6 @@ events.out.tfevents*

 # vim
 .vim
+
+# data
+data
\ No newline at end of file
diff --git a/bevdepth/datasets/nusc_det_dataset.py b/bevdepth/datasets/nusc_det_dataset.py
index 43e73d8..d4c795e 100644
--- a/bevdepth/datasets/nusc_det_dataset.py
+++ b/bevdepth/datasets/nusc_det_dataset.py
@@ -118,7 +118,7 @@ def depth_transform(cam_depth, resize, resize_dims, crop, flip, rotate):
     Returns:
         np array: [h/down_ratio, w/down_ratio, d]
     """
-
+    cam_depth = cam_depth.astype(np.float32)
     H, W = resize_dims
     cam_depth[:, :2] = cam_depth[:, :2] * resize
     cam_depth[:, 0] -= crop[0]
@@ -130,10 +130,10 @@ def depth_transform(cam_depth, resize, resize_dims, crop, flip, rotate):
     cam_depth[:, 1] -= H / 2.0

     h = rotate / 180 * np.pi
-    rot_matrix = [
+    rot_matrix = np.array([
         [np.cos(h), np.sin(h)],
         [-np.sin(h), np.cos(h)],
-    ]
+    ]).astype(np.float32)
     cam_depth[:, :2] = np.matmul(rot_matrix, cam_depth[:, :2].T).T

     cam_depth[:, 0] += W / 2.0
@@ -141,7 +141,7 @@ def depth_transform(cam_depth, resize, resize_dims, crop, flip, rotate):

     depth_coords = cam_depth[:, :2].astype(np.int16)

-    depth_map = np.zeros(resize_dims)
+    depth_map = np.zeros(resize_dims).astype(np.float32)
     valid_mask = ((depth_coords[:, 1] < resize_dims[0])
                   & (depth_coords[:, 0] < resize_dims[1])
                   & (depth_coords[:, 1] >= 0)
@@ -396,15 +396,15 @@ class NuscDetDataset(Dataset):
         sweep_sensor2sensor_mats = list()
         sweep_timestamps = list()
         sweep_lidar_depth = list()
-        if self.return_depth or self.use_fusion:
-            sweep_lidar_points = list()
-            for lidar_info in lidar_infos:
-                lidar_path = lidar_info['LIDAR_TOP']['filename']
-                lidar_points = np.fromfile(os.path.join(
-                    self.data_root, lidar_path),
-                                           dtype=np.float32,
-                                           count=-1).reshape(-1, 5)[..., :4]
-                sweep_lidar_points.append(lidar_points)
+        # if self.return_depth or self.use_fusion:
+        #     sweep_lidar_points = list()
+        #     for lidar_info in lidar_infos:
+        #         lidar_path = lidar_info['LIDAR_TOP']['filename']
+        #         lidar_points = np.fromfile(os.path.join(
+        #             self.data_root, lidar_path),
+        #                                    dtype=np.float32,
+        #                                    count=-1).reshape(-1, 5)[..., :4]
+        #         sweep_lidar_points.append(lidar_points)
         for cam in cams:
             imgs = list()
             sensor2ego_mats = list()
@@ -479,9 +479,16 @@ class NuscDetDataset(Dataset):
                 intrin_mat[:3, :3] = torch.Tensor(
                     cam_info[cam]['calibrated_sensor']['camera_intrinsic'])
                 if self.return_depth and (self.use_fusion or sweep_idx == 0):
-                    point_depth = self.get_lidar_depth(
-                        sweep_lidar_points[sweep_idx], img,
-                        lidar_infos[sweep_idx], cam_info[cam])
+                    # point_depth = self.get_lidar_depth(
+                    #     sweep_lidar_points[sweep_idx], img,
+                    #     lidar_infos[sweep_idx], cam_info[cam])
+
+                    # read depth gt from file
+                    file_name = os.path.split(cam_info[cam]['filename'])[-1]
+                    point_depth = np.fromfile(os.path.join(
+                        self.data_root, 'depth_gt', f'{file_name}.bin'),
+                        dtype=np.float32,
+                        count=-1).reshape(-1, 3)
                     point_depth_augmented = depth_transform(
                         point_depth, resize, self.ida_aug_conf['final_dim'],
                         crop, flip, rotate_ida)
diff --git a/bevdepth/exps/base_cli.py b/bevdepth/exps/base_cli.py
index cc887f7..aaabccd 100644
--- a/bevdepth/exps/base_cli.py
+++ b/bevdepth/exps/base_cli.py
@@ -32,16 +32,16 @@ def run_cli(model_class=BEVDepthLightningModel,
                                default=0,
                                help='seed for initializing training.')
     parent_parser.add_argument('--ckpt_path', type=str)
+    parent_parser.add_argument('--learning_rate', type=float)
     parser = BEVDepthLightningModel.add_model_specific_args(parent_parser)
     parser.set_defaults(profiler='simple',
                         deterministic=False,
-                        max_epochs=extra_trainer_config_args.get('epochs', 24),
+                        # max_epochs=extra_trainer_config_args.get('epochs', 24),
                         accelerator='ddp',
                         num_sanity_val_steps=0,
                         gradient_clip_val=5,
                         limit_val_batches=0,
                         enable_checkpointing=True,
-                        precision=16,
                         default_root_dir=os.path.join('./outputs/', exp_name))
     args = parser.parse_args()
     if args.seed is not None:
diff --git a/bevdepth/exps/nuscenes/base_exp.py b/bevdepth/exps/nuscenes/base_exp.py
index 6c4ffdb..fc5ae15 100644
--- a/bevdepth/exps/nuscenes/base_exp.py
+++ b/bevdepth/exps/nuscenes/base_exp.py
@@ -4,14 +4,16 @@ from functools import partial

 import mmcv
 import torch
+import torch_npu
 import torch.nn.functional as F
 import torch.nn.parallel
 import torch.utils.data
 import torch.utils.data.distributed
 import torchvision.models as models
 from pytorch_lightning.core import LightningModule
-from torch.cuda.amp.autocast_mode import autocast
+from torch_npu.npu.amp.autocast_mode import autocast
 from torch.optim.lr_scheduler import MultiStepLR
+from torch_npu.optim import NpuFusedAdamW

 from bevdepth.datasets.nusc_det_dataset import NuscDetDataset, collate_fn
 from bevdepth.evaluators.det_evaluators import DetNuscEvaluator
@@ -187,6 +189,7 @@ class BEVDepthLightningModel(LightningModule):

     def __init__(self,
                  gpus: int = 1,
+                 learning_rate = 2e-4 / 64,
                  data_root='data/nuScenes',
                  eval_interval=1,
                  batch_size_per_device=8,
@@ -203,7 +206,7 @@ class BEVDepthLightningModel(LightningModule):
         self.eval_interval = eval_interval
         self.batch_size_per_device = batch_size_per_device
         self.data_root = data_root
-        self.basic_lr_per_img = 2e-4 / 64
+        self.basic_lr_per_img = learning_rate
         self.class_names = class_names
         self.backbone_conf = backbone_conf
         self.head_conf = head_conf
@@ -308,7 +311,7 @@ class BEVDepthLightningModel(LightningModule):
         gt_depths = torch.where(
             (gt_depths < self.depth_channels + 1) & (gt_depths >= 0.0),
             gt_depths, torch.zeros_like(gt_depths))
-        gt_depths = F.one_hot(gt_depths.long(),
+        gt_depths = F.one_hot(gt_depths.int(),
                               num_classes=self.depth_channels + 1).view(
                                   -1, self.depth_channels + 1)[:, 1:]

@@ -373,7 +376,7 @@ class BEVDepthLightningModel(LightningModule):
     def configure_optimizers(self):
         lr = self.basic_lr_per_img * \
             self.batch_size_per_device * self.gpus
-        optimizer = torch.optim.AdamW(self.model.parameters(),
+        optimizer = NpuFusedAdamW(self.model.parameters(),
                                       lr=lr,
                                       weight_decay=1e-7)
         scheduler = MultiStepLR(optimizer, [19, 23])
@@ -397,7 +400,7 @@ class BEVDepthLightningModel(LightningModule):
         train_loader = torch.utils.data.DataLoader(
             train_dataset,
             batch_size=self.batch_size_per_device,
-            num_workers=4,
+            num_workers=8,
             drop_last=True,
             shuffle=False,
             collate_fn=partial(collate_fn,
diff --git a/bevdepth/exps/nuscenes/mv/bev_depth_lss_r50_256x704_128x128_24e_2key.py b/bevdepth/exps/nuscenes/mv/bev_depth_lss_r50_256x704_128x128_24e_2key.py
index de598ff..46b5997 100644
--- a/bevdepth/exps/nuscenes/mv/bev_depth_lss_r50_256x704_128x128_24e_2key.py
+++ b/bevdepth/exps/nuscenes/mv/bev_depth_lss_r50_256x704_128x128_24e_2key.py
@@ -22,11 +22,24 @@ bicycle 0.305   0.554   0.264   0.653   0.263   0.006
 traffic_cone    0.462   0.516   0.355   nan     nan     nan
 barrier 0.512   0.491   0.275   0.200   nan     nan
 """
+
+import os
+import psutil
+from functools import partial
+
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+from torch_npu.optim import NpuFusedAdamW
+from torch.optim.lr_scheduler import MultiStepLR
+
 from bevdepth.exps.base_cli import run_cli
 from bevdepth.exps.nuscenes.base_exp import \
     BEVDepthLightningModel as BaseBEVDepthLightningModel
 from bevdepth.models.base_bev_depth import BaseBEVDepth

+from bevdepth.datasets.nusc_det_dataset import NuscDetDataset, collate_fn
+

 class BEVDepthLightningModel(BaseBEVDepthLightningModel):

@@ -45,6 +58,15 @@ class BEVDepthLightningModel(BaseBEVDepthLightningModel):
                                   self.head_conf,
                                   is_train_depth=True)

+    def configure_optimizers(self):
+        lr = self.basic_lr_per_img * \
+            self.batch_size_per_device * self.gpus
+        optimizer = NpuFusedAdamW(self.model.parameters(),
+                                      lr=lr,
+                                      weight_decay=1e-3)
+        scheduler = MultiStepLR(optimizer, [19, 23])
+        return [[optimizer], [scheduler]]
+

 if __name__ == '__main__':
     run_cli(BEVDepthLightningModel,
diff --git a/bevdepth/layers/backbones/base_lss_fpn.py b/bevdepth/layers/backbones/base_lss_fpn.py
index c776334..a74926d 100644
--- a/bevdepth/layers/backbones/base_lss_fpn.py
+++ b/bevdepth/layers/backbones/base_lss_fpn.py
@@ -6,11 +6,10 @@ from mmdet3d.models import build_neck
 from mmdet.models import build_backbone
 from mmdet.models.backbones.resnet import BasicBlock
 from torch import nn
-from torch.cuda.amp.autocast_mode import autocast
+from torch_npu.npu.amp.autocast_mode import autocast

 try:
-    from bevdepth.ops.voxel_pooling_inference import voxel_pooling_inference
-    from bevdepth.ops.voxel_pooling_train import voxel_pooling_train
+    from mx_driving import npu_voxel_pooling_train
 except ImportError:
     print('Import VoxelPooling fail.')

@@ -417,10 +416,11 @@ class BaseLSSFPN(nn.Module):
                                                           1).expand(D, fH, fW)
         paddings = torch.ones_like(d_coords)

-        # D x H x W x 3
+        # D x H x W x 4
         frustum = torch.stack((x_coords, y_coords, d_coords, paddings), -1)
         return frustum

+
     def get_geometry(self, sensor2ego_mat, intrin_mat, ida_mat, bda_mat):
         """Transfer points from camera coord to ego coord.

@@ -438,24 +438,45 @@ class BaseLSSFPN(nn.Module):
         batch_size, num_cams, _, _ = sensor2ego_mat.shape

         # undo post-transformation
-        # B x N x D x H x W x 3
+        # D x H x W x 4
         points = self.frustum
-        ida_mat = ida_mat.view(batch_size, num_cams, 1, 1, 1, 4, 4)
-        points = ida_mat.inverse().matmul(points.unsqueeze(-1))
-        # cam_to_ego
+        D, H, W, M = points.shape
+
+        # reshape the points to [1, 1, D*H*W, 4]
+        points = points.view(1, 1, D * H * W, 4)
+
+        ida_mat = ida_mat.contiguous().inverse()
+        points_ida = torch.zeros((batch_size, num_cams, D * H * W, 4), device=points.device)
+        for i in range(M):
+            product = ida_mat[:, :, i, :].unsqueeze(2) * points
+            points_ida[:, :, :, i] = product.sum(dim=-1)
+        points = points_ida
+
+        # recover the depth information of coordinates.
         points = torch.cat(
-            (points[:, :, :, :, :, :2] * points[:, :, :, :, :, 2:3],
-             points[:, :, :, :, :, 2:]), 5)
+            (points[..., :2] * points[..., 2:3],
+             points[..., 2:]), 3)

+        # calculate the small matrix first, sensor2ego @ instin_mat
         combine = sensor2ego_mat.matmul(torch.inverse(intrin_mat))
-        points = combine.view(batch_size, num_cams, 1, 1, 1, 4,
-                              4).matmul(points)
+
+        # whether do bad or not, bda_mat shape [batch_size, 4, 4]
         if bda_mat is not None:
-            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1).view(
-                batch_size, num_cams, 1, 1, 1, 4, 4)
-            points = (bda_mat @ points).squeeze(-1)
+            bda_mat = bda_mat.unsqueeze(1).repeat(1, num_cams, 1, 1)
+            combine_bda = bda_mat.matmul(combine)
+
+            points_bda = torch.zeros((batch_size, num_cams, D *H *W, 4), device=points.device)
+            for i in range(M):
+                product = combine_bda[:, :, i, :].unsqueeze(2) * points
+                points_bda[:, :, :, i] = product.sum(dim=-1)
+            points = points_bda
         else:
-            points = points.squeeze(-1)
+            points_ida = torch.zeros((batch_size, num_cams, D *H *W, 4), device=points.device)
+            for i in range(M):
+                product = combine[:, :, i, :].unsqueeze(2) * points
+                points_ida[:, :, :, i] = product.sum(dim=-1)
+            points = points_ida
+        points = points.reshape(batch_size, num_cams, D, H, W, 4)
         return points[..., :3]

     def get_cam_feats(self, imgs):
@@ -541,14 +562,31 @@ class BaseLSSFPN(nn.Module):

             img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)

-            feature_map = voxel_pooling_train(geom_xyz,
+            feature_map = npu_voxel_pooling_train(geom_xyz,
                                               img_feat_with_depth.contiguous(),
                                               self.voxel_num.cuda())
         else:
-            feature_map = voxel_pooling_inference(
-                geom_xyz, depth, depth_feature[:, self.depth_channels:(
-                    self.depth_channels + self.output_channels)].contiguous(),
-                self.voxel_num.cuda())
+            img_feat_with_depth = depth.unsqueeze(
+                1) * depth_feature[:, self.depth_channels:(
+                    self.depth_channels + self.output_channels)].unsqueeze(2)
+
+            img_feat_with_depth = self._forward_voxel_net(img_feat_with_depth)
+
+            img_feat_with_depth = img_feat_with_depth.reshape(
+                batch_size,
+                num_cams,
+                img_feat_with_depth.shape[1],
+                img_feat_with_depth.shape[2],
+                img_feat_with_depth.shape[3],
+                img_feat_with_depth.shape[4],
+            )
+
+            img_feat_with_depth = img_feat_with_depth.permute(0, 1, 3, 4, 5, 2)
+
+            feature_map = npu_voxel_pooling_train(geom_xyz,
+                                              img_feat_with_depth.contiguous(),
+                                              self.voxel_num.cuda())
+
         if is_return_depth:
             # final_depth has to be fp32, otherwise the depth
             # loss will colapse during the traing process.
diff --git a/bevdepth/layers/backbones/bevstereo_lss_fpn.py b/bevdepth/layers/backbones/bevstereo_lss_fpn.py
index 6cb4532..f5d998d 100644
--- a/bevdepth/layers/backbones/bevstereo_lss_fpn.py
+++ b/bevdepth/layers/backbones/bevstereo_lss_fpn.py
@@ -14,8 +14,7 @@ from bevdepth.layers.backbones.base_lss_fpn import (ASPP, BaseLSSFPN, Mlp,
                                                     SELayer)

 try:
-    from bevdepth.ops.voxel_pooling_inference import voxel_pooling_inference
-    from bevdepth.ops.voxel_pooling_train import voxel_pooling_train
+    from mx_driving import npu_voxel_pooling_train
 except ImportError:
     print('Import VoxelPooling fail.')

diff --git a/bevdepth/layers/backbones/fusion_lss_fpn.py b/bevdepth/layers/backbones/fusion_lss_fpn.py
index 78c07de..5cc82eb 100644
--- a/bevdepth/layers/backbones/fusion_lss_fpn.py
+++ b/bevdepth/layers/backbones/fusion_lss_fpn.py
@@ -4,8 +4,7 @@ import torch.nn as nn
 from mmdet.models.backbones.resnet import BasicBlock

 try:
-    from bevdepth.ops.voxel_pooling_inference import voxel_pooling_inference
-    from bevdepth.ops.voxel_pooling_train import voxel_pooling_train
+    from mx_driving import npu_voxel_pooling_train
 except ImportError:
     print('Import VoxelPooling fail.')

diff --git a/bevdepth/layers/backbones/matrixvt.py b/bevdepth/layers/backbones/matrixvt.py
index 6192bb2..a5261ad 100644
--- a/bevdepth/layers/backbones/matrixvt.py
+++ b/bevdepth/layers/backbones/matrixvt.py
@@ -1,7 +1,20 @@
+# Copyright 2024 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 # Copyright (c) Megvii Inc. All rights reserved.
 import torch
 from torch import nn
-from torch.cuda.amp import autocast
+from torch_npu.npu.amp.autocast_mode import autocast

 from bevdepth.layers.backbones.base_lss_fpn import BaseLSSFPN

@@ -272,14 +285,18 @@ class MatrixVT(BaseLSSFPN):
         geom_uni = self.bev_anchors[None].repeat([B, 1, 1, 1])  # B,128,128,2
         B, L, L, _ = geom_uni.shape

-        circle_map = geom_uni.new_zeros((B, D, L * L))
+        circle_map = geom_uni.new_zeros((B, D, L * L), device='cpu')
+
+        ray_map = geom_uni.new_zeros((B, Nc * W, L * L), device='cpu')
+        geom_idx_cpu = geom_idx.cpu()

-        ray_map = geom_uni.new_zeros((B, Nc * W, L * L))
         for b in range(B):
             for dir in range(Nc * W):
-                ray_map[b, dir, geom_idx[b, dir]] += 1
+                ray_map[b, dir, geom_idx_cpu[b, dir]] += 1
             for d in range(D):
-                circle_map[b, d, geom_idx[b, :, d]] += 1
+                circle_map[b, d, geom_idx_cpu[b, :, d]] += 1
+        circle_map = circle_map.to(geom_uni.device)
+        ray_map = ray_map.to(geom_uni.device)
         null_point = int((bev_size / 2) * (bev_size + 1))
         circle_map[..., null_point] = 0
         ray_map[..., null_point] = 0
diff --git a/bevdepth/layers/heads/bev_depth_head.py b/bevdepth/layers/heads/bev_depth_head.py
index 2cf77d0..72883e6 100644
--- a/bevdepth/layers/heads/bev_depth_head.py
+++ b/bevdepth/layers/heads/bev_depth_head.py
@@ -8,7 +8,7 @@ from mmdet3d.models.dense_heads.centerpoint_head import CenterHead, circle_nms
 from mmdet3d.models.utils import clip_sigmoid
 from mmdet.core import reduce_mean
 from mmdet.models import build_backbone
-from torch.cuda.amp import autocast
+from torch_npu.npu.amp.autocast_mode import autocast

 __all__ = ['BEVDepthHead']

@@ -212,32 +212,35 @@ class BEVDepthHead(CenterHead):
                 # 0 is background for each task, so we need to add 1 here.
                 task_class.append(gt_labels_3d[m] + 1 - flag2)
             task_boxes.append(
-                torch.cat(task_box, axis=0).to(gt_bboxes_3d.device))
+                torch.cat(task_box, axis=0))
             task_classes.append(
-                torch.cat(task_class).long().to(gt_bboxes_3d.device))
+                torch.cat(task_class).int())
             flag2 += len(mask)
         draw_gaussian = draw_heatmap_gaussian
         heatmaps, anno_boxes, inds, masks = [], [], [], []
+        feature_map_size = feature_map_size.cpu().numpy().tolist()

         for idx, task_head in enumerate(self.task_heads):
             heatmap = gt_bboxes_3d.new_zeros(
                 (len(self.class_names[idx]), feature_map_size[1],
                  feature_map_size[0]),
-                device='cuda')
+                device='cpu')

             anno_box = gt_bboxes_3d.new_zeros(
                 (max_objs, len(self.train_cfg['code_weights'])),
                 dtype=torch.float32,
-                device='cuda')
+                device='cpu')

             ind = gt_labels_3d.new_zeros((max_objs),
-                                         dtype=torch.int64,
-                                         device='cuda')
+                                         dtype=torch.int32,
+                                         device='cpu')
             mask = gt_bboxes_3d.new_zeros((max_objs),
                                           dtype=torch.uint8,
-                                          device='cuda')
+                                          device='cpu')

             num_objs = min(task_boxes[idx].shape[0], max_objs)
+            task_classes[idx] = task_classes[idx].cpu()
+            task_boxes[idx] = task_boxes[idx].cpu()

             for k in range(num_objs):
                 cls_id = task_classes[idx][k] - 1
@@ -269,7 +272,7 @@ class BEVDepthHead(CenterHead):

                     center = torch.tensor([coor_x, coor_y],
                                           dtype=torch.float32,
-                                          device='cuda')
+                                          device='cpu')
                     center_int = center.to(torch.int32)

                     # throw out not in range objects to avoid out of array
@@ -297,7 +300,7 @@ class BEVDepthHead(CenterHead):
                         box_dim = box_dim.log()
                     if len(task_boxes[idx][k]) > 7:
                         anno_box[new_idx] = torch.cat([
-                            center - torch.tensor([x, y], device='cuda'),
+                            center - torch.tensor([x, y], device='cpu'),
                             z.unsqueeze(0),
                             box_dim,
                             torch.sin(rot).unsqueeze(0),
@@ -307,16 +310,16 @@ class BEVDepthHead(CenterHead):
                         ])
                     else:
                         anno_box[new_idx] = torch.cat([
-                            center - torch.tensor([x, y], device='cuda'),
+                            center - torch.tensor([x, y], device='cpu'),
                             z.unsqueeze(0), box_dim,
                             torch.sin(rot).unsqueeze(0),
                             torch.cos(rot).unsqueeze(0)
                         ])

-            heatmaps.append(heatmap)
-            anno_boxes.append(anno_box)
-            masks.append(mask)
-            inds.append(ind)
+            heatmaps.append(heatmap.cuda())
+            anno_boxes.append(anno_box.cuda())
+            masks.append(mask.cuda())
+            inds.append(ind.cuda())
         return heatmaps, anno_boxes, inds, masks

     def loss(self, targets, preds_dicts, **kwargs):
@@ -436,7 +439,7 @@ class BEVDepthHead(CenterHead):
                         boxes.detach().cpu().numpy(),
                         self.test_cfg['min_radius'][task_id],
                         post_max_size=self.test_cfg['post_max_size']),
-                                        dtype=torch.long,
+                                        dtype=torch.int,
                                         device=boxes.device)

                     boxes3d = boxes3d[keep]
@@ -459,7 +462,7 @@ class BEVDepthHead(CenterHead):
                             self.test_cfg['thresh_scale'][task_id],
                             post_max_size=self.test_cfg['post_max_size'],
                         ),
-                        dtype=torch.long,
+                        dtype=torch.int,
                         device=boxes.device,
                     )

diff --git a/requirements.txt b/requirements.txt
index 26546cd..92222b1 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,12 +1,14 @@
-numba
-numpy
+numba==0.58.1
+numpy<=1.23.5
 nuscenes-devkit
 opencv-python-headless
 pandas
-pytorch-lightning==1.6.0
+pytorch-lightning==1.6.5
 scikit-image
 scipy
 setuptools==59.5.0
 tensorboardX
-torch==1.9.0
-torchvision==0.10.0
+torchvision==0.12.0
+mmdet==2.28.0
+mmsegmentation==0.30.0
+protobuf==3.20.1
diff --git a/scripts/gen_depth_gt.py b/scripts/gen_depth_gt.py
new file mode 100644
index 0000000..4855ff2
--- /dev/null
+++ b/scripts/gen_depth_gt.py
@@ -0,0 +1,115 @@
+import os
+from multiprocessing import Pool
+
+import mmcv
+import numpy as np
+from nuscenes.utils.data_classes import LidarPointCloud
+from nuscenes.utils.geometry_utils import view_points
+from pyquaternion import Quaternion
+
+
+# https://github.com/nutonomy/nuscenes-devkit/blob/master/python-sdk/nuscenes/nuscenes.py#L834
+def map_pointcloud_to_image(
+    pc,
+    im,
+    lidar_calibrated_sensor,
+    lidar_ego_pose,
+    cam_calibrated_sensor,
+    cam_ego_pose,
+    min_dist: float = 0.0,
+):
+
+    # Points live in the point sensor frame. So they need to be
+    # transformed via global to the image plane.
+    # First step: transform the pointcloud to the ego vehicle
+    # frame for the timestamp of the sweep.
+
+    pc = LidarPointCloud(pc.T)
+    pc.rotate(Quaternion(lidar_calibrated_sensor['rotation']).rotation_matrix)
+    pc.translate(np.array(lidar_calibrated_sensor['translation']))
+
+    # Second step: transform from ego to the global frame.
+    pc.rotate(Quaternion(lidar_ego_pose['rotation']).rotation_matrix)
+    pc.translate(np.array(lidar_ego_pose['translation']))
+
+    # Third step: transform from global into the ego vehicle
+    # frame for the timestamp of the image.
+    pc.translate(-np.array(cam_ego_pose['translation']))
+    pc.rotate(Quaternion(cam_ego_pose['rotation']).rotation_matrix.T)
+
+    # Fourth step: transform from ego into the camera.
+    pc.translate(-np.array(cam_calibrated_sensor['translation']))
+    pc.rotate(Quaternion(cam_calibrated_sensor['rotation']).rotation_matrix.T)
+
+    # Fifth step: actually take a "picture" of the point cloud.
+    # Grab the depths (camera frame z axis points away from the camera).
+    depths = pc.points[2, :]
+    coloring = depths
+
+    # Take the actual picture (matrix multiplication with camera-matrix
+    # + renormalization).
+    points = view_points(pc.points[:3, :],
+                         np.array(cam_calibrated_sensor['camera_intrinsic']),
+                         normalize=True)
+
+    # Remove points that are either outside or behind the camera.
+    # Leave a margin of 1 pixel for aesthetic reasons. Also make
+    # sure points are at least 1m in front of the camera to avoid
+    # seeing the lidar points on the camera casing for non-keyframes
+    # which are slightly out of sync.
+    mask = np.ones(depths.shape[0], dtype=bool)
+    mask = np.logical_and(mask, depths > min_dist)
+    mask = np.logical_and(mask, points[0, :] > 1)
+    mask = np.logical_and(mask, points[0, :] < im.shape[1] - 1)
+    mask = np.logical_and(mask, points[1, :] > 1)
+    mask = np.logical_and(mask, points[1, :] < im.shape[0] - 1)
+    points = points[:, mask]
+    coloring = coloring[mask]
+
+    return points, coloring
+
+
+data_root = 'data/nuScenes'
+info_path = 'data/nuScenes/nuscenes_infos_train.pkl'
+# data3d_nusc = NuscMVDetData()
+
+lidar_key = 'LIDAR_TOP'
+cam_keys = [
+    'CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_RIGHT',
+    'CAM_BACK', 'CAM_BACK_LEFT'
+]
+
+
+def worker(info):
+    lidar_path = info['lidar_infos'][lidar_key]['filename']
+    points = np.fromfile(os.path.join(data_root, lidar_path),
+                         dtype=np.float32,
+                         count=-1).reshape(-1, 5)[..., :4]
+    lidar_calibrated_sensor = info['lidar_infos'][lidar_key][
+        'calibrated_sensor']
+    lidar_ego_pose = info['lidar_infos'][lidar_key]['ego_pose']
+    for i, cam_key in enumerate(cam_keys):
+        cam_calibrated_sensor = info['cam_infos'][cam_key]['calibrated_sensor']
+        cam_ego_pose = info['cam_infos'][cam_key]['ego_pose']
+        img = mmcv.imread(
+            os.path.join(data_root, info['cam_infos'][cam_key]['filename']))
+        pts_img, depth = map_pointcloud_to_image(
+            points.copy(), img, lidar_calibrated_sensor.copy(),
+            lidar_ego_pose.copy(), cam_calibrated_sensor, cam_ego_pose)
+        file_name = os.path.split(info['cam_infos'][cam_key]['filename'])[-1]
+        np.concatenate([pts_img[:2, :].T, depth[:, None]],
+                       axis=1).astype(np.float32).flatten().tofile(
+                           os.path.join(data_root, 'depth_gt',
+                                        f'{file_name}.bin'))
+    # plt.savefig(f"{sample_idx}")
+
+
+if __name__ == '__main__':
+    po = Pool(24)
+    mmcv.mkdir_or_exist(os.path.join(data_root, 'depth_gt'))
+    infos = mmcv.load(info_path)
+    # import ipdb; ipdb.set_trace()
+    for info in infos:
+        po.apply_async(func=worker, args=(info, ))
+    po.close()
+    po.join()
\ No newline at end of file
diff --git a/setup.py b/setup.py
index e8070cc..5324d32 100644
--- a/setup.py
+++ b/setup.py
@@ -19,19 +19,9 @@ def make_cuda_ext(name,
     define_macros = []
     extra_compile_args = {'cxx': [] + extra_args}

-    if torch.cuda.is_available() or os.getenv('FORCE_CUDA', '0') == '1':
-        define_macros += [('WITH_CUDA', None)]
-        extension = CUDAExtension
-        extra_compile_args['nvcc'] = extra_args + [
-            '-D__CUDA_NO_HALF_OPERATORS__',
-            '-D__CUDA_NO_HALF_CONVERSIONS__',
-            '-D__CUDA_NO_HALF2_OPERATORS__',
-        ]
-        sources += sources_cuda
-    else:
-        print('Compiling {} without CUDA'.format(name))
-        extension = CppExtension
-        # raise EnvironmentError('CUDA is required to compile MMDetection!')
+    print('Compiling {} without CUDA'.format(name))
+    extension = CppExtension
+    # raise EnvironmentError('CUDA is required to compile MMDetection!')

     return extension(
         name='{}.{}'.format(module, name),
@@ -57,19 +47,6 @@ setup(
         'Operating System :: OS Independent',
     ],
     install_requires=[],
-    ext_modules=[
-        make_cuda_ext(
-            name='voxel_pooling_train_ext',
-            module='bevdepth.ops.voxel_pooling_train',
-            sources=['src/voxel_pooling_train_forward.cpp'],
-            sources_cuda=['src/voxel_pooling_train_forward_cuda.cu'],
-        ),
-        make_cuda_ext(
-            name='voxel_pooling_inference_ext',
-            module='bevdepth.ops.voxel_pooling_inference',
-            sources=['src/voxel_pooling_inference_forward.cpp'],
-            sources_cuda=['src/voxel_pooling_inference_forward_cuda.cu'],
-        ),
-    ],
+
     cmdclass={'build_ext': BuildExtension},
 )
