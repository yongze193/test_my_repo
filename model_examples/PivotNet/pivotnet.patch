diff --git a/configs/pivotnet_nuscenes_swint.py b/configs/pivotnet_nuscenes_swint.py
index 00b6f43..e494808 100644
--- a/configs/pivotnet_nuscenes_swint.py
+++ b/configs/pivotnet_nuscenes_swint.py
@@ -3,6 +3,7 @@ import torch
 import numpy as np
 import torch.nn as nn
 from torch.optim import AdamW
+from torch_npu.optim import NpuFusedAdamW
 from torchvision.transforms import Compose
 from torch.optim.lr_scheduler import MultiStepLR
 from torch.utils.data.distributed import DistributedSampler
@@ -12,6 +13,12 @@ from mapmaster.engine.experiment import BaseExp
 from mapmaster.dataset.nuscenes_pivotnet import NuScenesMapDataset
 from mapmaster.dataset.transform import Resize, Normalize, ToTensor_Pivot
 from mapmaster.utils.misc import get_param_groups, is_distributed
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+
 
 
 class EXPConfig:
@@ -21,8 +28,8 @@ class EXPConfig:
 
     map_conf = dict(
         dataset_name="nuscenes",
-        nusc_root="/data/dataset/public/nuScenes",
-        anno_root="/data/dataset/public/nuScenes/customer/pivot-bezier",
+        nusc_root="data/nuscenes",
+        anno_root="data/nuscenes/customer/pivot-bezier",
         split_dir="assets/splits/nuscenes",
         num_classes=3,
         ego_size=(60, 30),
@@ -279,7 +286,7 @@ class Exp(BaseExp):
 
     def _configure_optimizer(self):
         optimizer_setup = self.exp_config.optimizer_setup
-        optimizer = AdamW(get_param_groups(self.model, optimizer_setup))
+        optimizer = NpuFusedAdamW(get_param_groups(self.model, optimizer_setup))
         return optimizer
 
     def _configure_lr_scheduler(self):
diff --git a/mapmaster/engine/core.py b/mapmaster/engine/core.py
index 4ecc69f..4128b3a 100644
--- a/mapmaster/engine/core.py
+++ b/mapmaster/engine/core.py
@@ -10,8 +10,14 @@ from mapmaster.engine.callbacks import CheckPointLoader, CheckPointSaver, ClearM
 from mapmaster.engine.callbacks import TensorBoardMonitor, TextMonitor, ClipGrad
 from mapmaster.utils.env import collect_env_info, get_root_dir
 from mapmaster.utils.misc import setup_logger, sanitize_filename, PyDecorator, all_gather_object
+import torch
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
 
 
+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+
 __all__ = ["BaseCli", "BeMapNetCli"]
 
 
diff --git a/mapmaster/engine/executor.py b/mapmaster/engine/executor.py
index a7bcf4c..501986c 100644
--- a/mapmaster/engine/executor.py
+++ b/mapmaster/engine/executor.py
@@ -1,17 +1,22 @@
-import os
-import torch
-from tqdm import tqdm
-from typing import Sequence
-from mapmaster.engine.experiment import BaseExp
-from mapmaster.utils.misc import get_rank, synchronize
-
-
-__all__ = ["Callback", "BaseExecutor", "Trainer", "BeMapNetEvaluator"]
-
-
-class Callback:
-
-    # callback enabled rank list
+import os
+import torch
+from tqdm import tqdm
+from typing import Sequence
+from mapmaster.engine.experiment import BaseExp
+from mapmaster.utils.misc import get_rank, synchronize
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+
+torch.npu.config.allow_internal_format = False
+torch.npu.set_compile_mode(jit_compile=False)
+
+__all__ = ["Callback", "BaseExecutor", "Trainer", "BeMapNetEvaluator"]
+
+
+class Callback:
+
+    # callback enabled rank list
     # None means callback is always enabled
     enabled_rank = None
 
diff --git a/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py b/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
index 28f9ea1..87dbcf7 100644
--- a/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
+++ b/mapmaster/models/bev_decoder/deform_transformer/deform_transformer.py
@@ -155,7 +155,8 @@ class DeformTransformer(nn.Module):
 
         cams = []
         for cam in extrinsic:
-            cam_coords = torch.linalg.inv(cam) @ coords_flatten.T  # (4, N)
+            # cam_coords = torch.linalg.inv(cam) @ coords_flatten.T  # (4, N)
+            cam_coords = torch.matmul(torch.inverse(cam), coords_flatten.T)
             cam_coords = cam_coords[:3, :]  # (3, N) -- x, y, z
             cams.append(cam_coords)
         cams = torch.stack(cams, dim=0)  # (6, 3, N) Coordinates in Camera Frame
diff --git a/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py b/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
index c091ed6..487e6f8 100644
--- a/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
+++ b/mapmaster/models/bev_decoder/deform_transformer/ops/modules/ms_deform_attn.py
@@ -12,20 +12,20 @@ from __future__ import division
 
 import warnings
 import math
-
-import torch
-from torch import nn
-import torch.nn.functional as F
-from torch.nn.init import xavier_uniform_, constant_
-
-from ..functions import MSDeformAttnFunction
-
-
-def _is_power_of_2(n):
-    if (not isinstance(n, int)) or (n < 0):
-        raise ValueError("invalid input for _is_power_of_2: {} (type: {})".format(n, type(n)))
-    return (n & (n - 1) == 0) and n != 0
-
+
+import torch
+from torch import nn
+import torch.nn.functional as F
+from torch.nn.init import xavier_uniform_, constant_
+
+import torch, torch_npu
+from mx_driving import npu_multi_scale_deformable_attn_function
+
+def _is_power_of_2(n):
+    if (not isinstance(n, int)) or (n < 0):
+        raise ValueError("invalid input for _is_power_of_2: {} (type: {})".format(n, type(n)))
+    return (n & (n - 1) == 0) and n != 0
+
 
 class MSDeformAttn(nn.Module):
     def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4):
@@ -122,19 +122,19 @@ class MSDeformAttn(nn.Module):
         elif reference_points.shape[-1] == 4:
             sampling_locations = (
                 reference_points[:, :, None, :, None, :2]
-                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
-            )
-        else:
-            raise ValueError(
-                "Last dim of reference_points must be 2 or 4, but get {} instead.".format(reference_points.shape[-1])
-            )
-        output = MSDeformAttnFunction.apply(
-            value,
-            input_spatial_shapes,
-            input_level_start_index,
-            sampling_locations,
-            attention_weights,
-            self.im2col_step,
-        )
-        output = self.output_proj(output)
-        return output
+                + sampling_offsets / self.n_points * reference_points[:, :, None, :, None, 2:] * 0.5
+            )
+        else:
+            raise ValueError(
+                "Last dim of reference_points must be 2 or 4, but get {} instead.".format(reference_points.shape[-1])
+            )
+
+        output = npu_multi_scale_deformable_attn_function(
+            value,
+            input_spatial_shapes,
+            input_level_start_index,
+            sampling_locations,
+            attention_weights,
+        )
+        output = self.output_proj(output)
+        return output
diff --git a/mapmaster/models/output_head/line_matching.py b/mapmaster/models/output_head/line_matching.py
index c690ffb..8c47f85 100644
--- a/mapmaster/models/output_head/line_matching.py
+++ b/mapmaster/models/output_head/line_matching.py
@@ -1,33 +1,67 @@
 import numpy as np
+import torch
+
+# def seq_matching_dist_parallel(cost, gt_lens, coe_endpts=0):
+#     # Time complexity: O(m*n)
+#     bs, m, n = cost.shape
+#     assert m <= n
+#     min_cost = np.ones((bs, m, n)) * np.inf
+#     mem_sort_value = np.ones((bs, m, n)) * np.inf  # v[i][j] = np.min(min_cost[i][:j+1])
+
+#     # initialization
+#     for j in range(0, n):
+#         if j == 0:
+#             min_cost[:, 0, j] = cost[:, 0, j]
+#         mem_sort_value[:, 0, j] = min_cost[:, 0, 0]
+
+#     for i in range(1, m):
+#         for j in range(i, n):
+#             min_cost[:, i, j] = mem_sort_value[:, i-1, j-1] + cost[:, i, j]
+#             indexes = (min_cost[:, i, j] < mem_sort_value[:, i, j-1])
+#             indexes_inv = np.array(1-indexes, dtype=np.bool)
+#             mem_sort_value[indexes, i, j] = min_cost[indexes, i, j]
+#             mem_sort_value[indexes_inv, i, j] = mem_sort_value[indexes_inv, i, j-1]
+
+#     indexes = []
+#     for i, ll in enumerate(gt_lens):
+#         indexes.append([i, ll-1, n-1])
+#     indexes = np.array(indexes)
+#     xs, ys, zs = indexes[:, 0], indexes[:, 1], indexes[:, 2]
+#     res_cost = min_cost[xs, ys, zs] + (cost[xs, 0, 0] + cost[xs, ys, zs]) * coe_endpts
+#     return  res_cost / (indexes[:, 1]+1+coe_endpts*2)
 
 def seq_matching_dist_parallel(cost, gt_lens, coe_endpts=0):
+    if cost.device.type != 'npu':
+        cost = cost.to('npu')
+    if gt_lens.device.type != 'npu':
+        gt_lens = gt_lens.to('npu')
+
     # Time complexity: O(m*n)
     bs, m, n = cost.shape
     assert m <= n
-    min_cost = np.ones((bs, m, n)) * np.inf  
-    mem_sort_value = np.ones((bs, m, n)) * np.inf  # v[i][j] = np.min(min_cost[i][:j+1])
 
-    # initialization
-    for j in range(0, n):
-        if j == 0:
-            min_cost[:, 0, j] = cost[:, 0, j] 
-        mem_sort_value[:, 0, j] = min_cost[:, 0, 0]
-        
+    min_cost = torch.full((bs, m, n), float('inf'), device=cost.device)
+    mem_sort_value = torch.full((bs, m, n), float('inf'), device=cost.device)
+
+    # Initialization
+    min_cost[:, 0, 0] = cost[:, 0, 0]
+    mem_sort_value[:, 0, :] = min_cost[:, 0, 0].unsqueeze(-1)
+
+    # Vectorized computation
     for i in range(1, m):
-        for j in range(i, n):
-            min_cost[:, i, j] = mem_sort_value[:, i-1, j-1] + cost[:, i, j]
-            indexes = (min_cost[:, i, j] < mem_sort_value[:, i, j-1])
-            indexes_inv = np.array(1-indexes, dtype=np.bool)
-            mem_sort_value[indexes, i, j] = min_cost[indexes, i, j]
-            mem_sort_value[indexes_inv, i, j] = mem_sort_value[indexes_inv, i, j-1]
-
-    indexes = []
-    for i, ll in enumerate(gt_lens):
-        indexes.append([i, ll-1, n-1])
-    indexes = np.array(indexes)
-    xs, ys, zs = indexes[:, 0], indexes[:, 1], indexes[:, 2]
-    res_cost = min_cost[xs, ys, zs] + (cost[xs, 0, 0] + cost[xs, ys, zs]) * coe_endpts
-    return  res_cost / (indexes[:, 1]+1+coe_endpts*2)
+        # 计算 min_cost[:, i, i:]
+        min_cost[:, i, i:] = mem_sort_value[:, i-1, i-1:-1] + cost[:, i, i:]
+        mem_sort_value[:, i, i:], _ = torch.cummin(min_cost[:, i, i:], dim=-1)
+
+    batch_idx = torch.arange(bs, device=cost.device)
+    ys = gt_lens.to(torch.long) - 1
+    zs = torch.full((bs,), n-1, device=cost.device, dtype=torch.long)
+
+    res_min_cost = min_cost[batch_idx, ys, zs]
+    endpoint_cost = (cost[batch_idx, 0, 0] + cost[batch_idx, ys, zs]) * coe_endpts
+    denominator = gt_lens.to(res_min_cost.dtype) + coe_endpts * 2
+
+    return (res_min_cost + endpoint_cost) / denominator
 
 def pivot_dynamic_matching(cost: np.array):
     # Time complexity: O(m*n)
diff --git a/mapmaster/models/output_head/pivot_post_processor.py b/mapmaster/models/output_head/pivot_post_processor.py
index cc49dcc..42e8646 100644
--- a/mapmaster/models/output_head/pivot_post_processor.py
+++ b/mapmaster/models/output_head/pivot_post_processor.py
@@ -56,15 +56,24 @@ class HungarianMatcher(nn.Module):
                 gt_pts_mask = torch.zeros(gt_num, n_pt, dtype=torch.double, device=gt_pts.device)
                 gt_lens = torch.tensor([ll for ll in targets[0]["valid_len"][cid]]) # n_gt
                 gt_lens = gt_lens.unsqueeze(-1).repeat(1, dt_num).flatten()
-                for i, ll in enumerate(targets[0]["valid_len"][cid]):
-                    gt_pts_mask[i][:ll] = 1
+                # for i, ll in enumerate(targets[0]["valid_len"][cid]):
+                #     gt_pts_mask[i][:ll] = 1
+
+                valid_lens = torch.tensor(targets[0]["valid_len"][cid], device=gt_pts.device, dtype=torch.long)
+                row_indices = torch.arange(n_pt, device=gt_pts.device).expand(len(valid_lens), -1)
+                gt_pts_mask = (row_indices < valid_lens.unsqueeze(1)).to(dtype=torch.float32)
+
                 gt_pts_mask = gt_pts_mask.unsqueeze(1).unsqueeze(-1).repeat(1, dt_num, 1, n_pt).flatten(0, 1)   
                 cost_mat_seqmatching = torch.cdist(gt_pts, dt_pts, p=1) * gt_pts_mask                # [n_gt*n_dt, n_pts, n_pts]
+                # cost_mat_seqmatching = seq_matching_dist_parallel(
+                #     cost_mat_seqmatching.detach().cpu().numpy(),
+                #     gt_lens,
+                #     self.coe_endpts).reshape(gt_num, dt_num).transpose(1, 0)  #[n_gt, n_dt]
                 cost_mat_seqmatching = seq_matching_dist_parallel(
-                    cost_mat_seqmatching.detach().cpu().numpy(), 
+                    cost_mat_seqmatching,
                     gt_lens, 
                     self.coe_endpts).reshape(gt_num, dt_num).transpose(1, 0)  #[n_gt, n_dt]
-                cost_mat_seqmatching = torch.from_numpy(cost_mat_seqmatching).to(cost_mat_mask.device)
+                # cost_mat_seqmatching = torch.from_numpy(cost_mat_seqmatching).to(cost_mat_mask.device)
                 
                 # 4. sum mat
                 sizes = [len(tgt["obj_labels"][cid]) for tgt in targets]
@@ -154,6 +163,8 @@ class SetCriterion(nn.Module):
                     _, matched_pt_idx = pivot_dynamic_matching(cost_mat.detach().cpu().numpy())
                     matched_pt_idx = torch.tensor(matched_pt_idx)
                     # match pts loss
+                    # npu adapt, no damage to precision
+                    tgt_pts = torch.tensor(tgt_pts, dtype=torch.float32)
                     loss_match = w * F.l1_loss(src_pts[matched_pt_idx], tgt_pts, reduction="none").sum(dim=-1)   # [n_gt_pt, 2] -> [n_gt_dt]
                     loss_match = (loss_match * weight_pt).sum() / weight_pt.sum()
                     loss_pts += loss_match / num_instances
@@ -191,14 +202,20 @@ class SetCriterion(nn.Module):
             inter_loss = F.l1_loss(collinear_src_pts, inter_tgt, reduction="sum")
         return inter_loss
     
+    # @staticmethod
+    # def interpolate(start_pt, end_pt, inter_num):
+    #     res = torch.zeros((inter_num, 2), dtype=start_pt.dtype, device=start_pt.device)
+    #     num_len = inter_num + 1  # segment num.
+    #     for i in range(1, num_len):
+    #         ratio = i / num_len
+    #         res[i-1] = (1 - ratio) * start_pt + ratio * end_pt
+    #     return res
+
     @staticmethod
     def interpolate(start_pt, end_pt, inter_num):
-        res = torch.zeros((inter_num, 2), dtype=start_pt.dtype, device=start_pt.device)
-        num_len = inter_num + 1  # segment num.
-        for i in range(1, num_len):
-            ratio = i / num_len
-            res[i-1] = (1 - ratio) * start_pt + ratio * end_pt
-        return res
+        ratios = torch.arange(1, inter_num + 1, dtype=start_pt.dtype, device=start_pt.device) / (inter_num + 1)
+        ratios = ratios.view(-1, 1)
+        return (1 - ratios) * start_pt + ratios * end_pt
 
     def criterion_instance_labels(self, outputs, targets, matching_indices):
         loss_labels = 0
diff --git a/requirement.txt b/requirement.txt
index 7120c16..8e55b13 100644
--- a/requirement.txt
+++ b/requirement.txt
@@ -1,12 +1,14 @@
-clearml
-loguru
-Ninja
-numba
-opencv-contrib-python
-pandas
-scikit-image
-tabulate
-tensorboardX
-Pillow==9.4.0
-numpy==1.23.5
-visvalingamwyatt=0.2.0
\ No newline at end of file
+torchvision==0.16.0
+decorator
+clearml
+loguru
+Ninja
+numba
+opencv-contrib-python
+pandas
+scikit-image
+tabulate
+tensorboardX
+Pillow==9.4.0
+numpy==1.23.5
+visvalingamwyatt==0.2.0
\ No newline at end of file
diff --git a/run.sh b/run.sh
index ee8c865..6a7ddb8 100644
--- a/run.sh
+++ b/run.sh
@@ -1,6 +1,8 @@
 #!/usr/bin/env bash
 
-export PYTHONPATH=$(pwd)
+export PYTHONPATH=$PYTHONPATH:$(pwd)
+export CPU_AFFINITY_CONF=1
+export TASK_QUEUE_ENABLE=2
 
 case "$1" in
     "train")
diff --git a/tools/evaluation/cd.py b/tools/evaluation/cd.py
index 9edf001..1e67cd3 100644
--- a/tools/evaluation/cd.py
+++ b/tools/evaluation/cd.py
@@ -1,17 +1,18 @@
 import torch
-
-def chamfer_distance(source_pc, target_pc, threshold, cum=False, bidirectional=True):
-    torch.backends.cuda.matmul.allow_tf32 = False
-    torch.backends.cudnn.allow_tf32 = False
-    # dist = torch.cdist(source_pc.float(), target_pc.float())
-    # dist = torch.cdist(source_pc.float(), target_pc.float(), compute_mode='donot_use_mm_for_euclid_dist')
-    dist = torch.cdist(source_pc.type(torch.float64), target_pc.type(torch.float64))
-    dist1, _ = torch.min(dist, 2)
-    dist2, _ = torch.min(dist, 1)
-    if cum:
-        len1 = dist1.shape[-1]
-        len2 = dist2.shape[-1]
-        dist1 = dist1.sum(-1)
+
+def chamfer_distance(source_pc, target_pc, threshold, cum=False, bidirectional=True):
+    torch.backends.cuda.matmul.allow_tf32 = False
+    torch.backends.cudnn.allow_tf32 = False
+    # dist = torch.cdist(source_pc.float(), target_pc.float())
+    # dist = torch.cdist(source_pc.float(), target_pc.float(), compute_mode='donot_use_mm_for_euclid_dist')
+    # npu adapt, no damage to precision
+    dist = torch.cdist(source_pc.type(torch.float32), target_pc.type(torch.float32))
+    dist1, _ = torch.min(dist, 2)
+    dist2, _ = torch.min(dist, 1)
+    if cum:
+        len1 = dist1.shape[-1]
+        len2 = dist2.shape[-1]
+        dist1 = dist1.sum(-1)
         dist2 = dist2.sum(-1)
         return dist1, dist2, len1, len2
     dist1 = dist1.mean(-1)
diff --git a/tools/evaluation/eval.py b/tools/evaluation/eval.py
index d85a976..5c79f41 100644
--- a/tools/evaluation/eval.py
+++ b/tools/evaluation/eval.py
@@ -1,19 +1,20 @@
 import os
 import sys
 import torch
-import numpy as np
-import pickle as pkl
-from tqdm import tqdm
-from tabulate import tabulate
-from torch.utils.data import Dataset, DataLoader
-from ap import instance_mask_ap as get_batch_ap
-
-
-class BeMapNetResultForNuScenes(Dataset):
-    def __init__(self, gt_dir, dt_dir, val_txt):
-        self.gt_dir, self.dt_dir = gt_dir, dt_dir
-        self.tokens = [fname.strip().split('.')[0] for fname in open(val_txt).readlines()]        
-        self.max_line_count = 100
+import numpy as np
+import pickle as pkl
+from tqdm import tqdm
+from tabulate import tabulate
+from torch.utils.data import Dataset, DataLoader
+from ap import instance_mask_ap as get_batch_ap
+import torch_npu
+from torch_npu.contrib import transfer_to_npu
+
+class BeMapNetResultForNuScenes(Dataset):
+    def __init__(self, gt_dir, dt_dir, val_txt):
+        self.gt_dir, self.dt_dir = gt_dir, dt_dir
+        self.tokens = [fname.strip().split('.')[0] for fname in open(val_txt).readlines()]
+        self.max_line_count = 100
 
     def __getitem__(self, idx):
         token = self.tokens[idx]
